\documentclass[english]{article}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amscd}
\usepackage[latin2]{inputenc}
\usepackage{t1enc}
\usepackage[mathscr]{eucal}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{pict2e}
\usepackage{epic}
\usepackage{hyperref}
\usepackage{mathtools}
\numberwithin{equation}{section}
\usepackage[margin=2.9cm]{geometry}
\usepackage{epstopdf}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\newcommand{\ovl}{\overline}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}

\DeclarePairedDelimiter\paren{(}{)}           % (parentheses)
\DeclarePairedDelimiter\ang{\langle}{\rangle} % <angle brackets>
\DeclarePairedDelimiter\abs{\lvert}{\rvert}   % |absolute value|
\DeclarePairedDelimiter\norm{\lVert}{\rVert}  % ||norm||
\DeclarePairedDelimiter\bkt{[}{]}             % [brackets]
\DeclarePairedDelimiter\set{\{}{\}}           % {braces}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.

\begin{document}
	
	\title{Writeup}
	
	\author{Yannik Pitcan \\
		Ram Akella
	}
	\maketitle
	
	\section*{Research Objective}
	Deep reinforcement learning methods have not been studied for the preventive maintenance problem, not withstanding claims made by firms such as General Electric (GE) \cite{GE}. The goal of the current study is to explore the use of such methods for the preventative maintenance problem.  The first step to achieving this goal is to understand how well such deep reinforcement learning methods can be leveraged to implicitly predict maintenance metrics such as Remaining Useful Life (RUL) to achieve close to optimal repair policies. The second step is to investigate how to combine two models: one, observations about the system process, and two, prior knowledge of the system. In particular, this study seeks to understand how to mix the observations with the prior knowledge to get a better understanding of the given system. The first model is data-based empirical knowledge and the second model is either prior knowledge or expertise-based adaptation of empirical knowledge. The study also seeks to understand how to combine these models with  multiple predictor distributions. Currently, the first step has been realized using deep reinforcement learning and classical POMDP implementations for the CMAPPS dataset. The following paper describes the deep reinforcement learning and POMDP implementation used. Two methods for combining real-time observation with prior knowledge were taken in this research. The first method is fitting a Hidden Markov Model (HMM) to the provided data to create a POMDP, and the created POMDP was solved using model-based approaches. The other method uses POMDP-RL techniques \textit{citation} using deep variational autoencoders.  Future efforts beyond what is presented in this paper will focus on the second step of the proposed research.
	
	\subsection*{Dynamical System Consideration}
	Past work on RL for preventive maintenance have shown promise, but suffered from a few flaws. One flaw was the lack of distinguishment between covariates and sensor readings. For example, past work attempted to understand the RUL distribution via fitting a Weibull distribution. But, the fitted parameters ($\lambda$ and $k$) were dependent on input covariates. This dependence shines light on the second flaw, the use of summary values such as RUL to represent the full distribution, when a set of samples $(x_t,y_t)$ from said distribution may have been preferable. The presented research differs from past work in that it models the data as a dynamic system with latent states and observations and avoids encapsulating the knowledge of the distributions with a simple summary statistic.
	
	\subsection*{POMDP vs MDP}
	The use of a Partially Observable Markov Decision Process (POMDP) goes in tandem with distinguishing between covariates and sensor readings. In previous research, when the preventive maintenance problem was treated as an \textit{spell out MDP, what is it, provide reference}  MDP, it ignored the fact that there may be hidden states (for example, system health) that impacted sensor observations.
	
	\section*{Using a Modified EM Framework}
	The primary question this research seeks to answer is, how we can determine these hidden states?
	
	Let $S=\{1,2,\ldots,X\}$ be the space of underlying states and $O$ be the space of observations. A HMM $\theta=(\pi_0,P,B)$ is parameterized by $\pi_0$, $P$, and $B$, which represent the initial state matrix, state transition matrix, and emission matrix, respectively. Let $\pi_0=[P(s_0=i)]_{i=1}^X$, $P_{ij}=P(s_{t+1}=j|s_t=i)$ and $B_{ij}(\eta)=P_{\eta}(y_t=j|s_t=i)$.
	
	For estimation, we can use an EM \textit{what is an EM framework?  providd reference}framework, treating the hidden states as the latent states in a Hidden Markov Model (HMM) with our measurements as the observations. The latent states represent the health of the machine, where $1$ is failing condition and $M$ is perfect condition.
	
	Let $\mathcal{Y} = (Y^{(1)},\ldots,Y^{(D)})$ be the set of $D$ observation sequences, where each $Y^{(i)}=(y_1^{(i)},\ldots,y_{T}^{(i)})$, and assume that each observation is an IID draw.

	The technique used was to repeat the following steps until convergence ($\theta = (\pi_0,P,B)$):
	\begin{itemize}
		\item Compute $Q(\theta,\theta^s) = \sum_{x\in \mathcal{S}} \log [P(\mathcal{O},x;\theta)]P(x|\mathcal{O};\theta^s)$ for the $E$-step
		\item Set $\theta^{s+1} = \arg \max_{\theta} Q(\theta, \theta^s)$ for the $M$-step.
	\end{itemize}
	
	Let $\theta^s = (\pi_0^s,P^s,B^s)$. 
	
	We can rewrite the auxiliary log-likelihood as
	\begin{align*}
	Q(\theta^{s},\theta) &= \E\left[ \sum_{k=1}^N \sum_{i=1}^X I(x_k=i) \log B_{iy_k}(\eta) + \sum_{k=1}^N \sum_{i=1}^X \sum_{j=1}^X I(x_{k-1}=i,x_k=j) \log P_{ij} | y_{1:N},\theta^{(n)}\right] + const(\theta^{s}) \\
	&= \sum_{k=1}^N \sum_{i=1}^X \pi_{k|N}(i)\log B_{iy_k}(\eta) + \sum_{k=1}^N \sum_{i=1}^X \sum_{j=1}^X \pi_{k|N}(i,j)\log P_{ij} + const(\theta^s)
	\end{align*}
	
	Of note is that $\pi_{k|N}(i)=P(x_k=i|y_{1:N},\theta^s)$ and $\pi_{k|N}(i,j)=P(x_k=i,x_{k+1}=j|y_{1:N},\theta^s)$ are computed in the $E$ step of the EM framework. The derivation of these probabilities is presented in the Forward-Backward Filtering subsection.
	
	\subsection*{Continuous Emissions}
	
	For this work, assume $B_{ij}(\eta) = P_{\eta}(y_t=j|s_t=i) = f(j)$, where $f\sim N(C(i),R(i))$. Therefore, $\eta=(C(1),\ldots,C(X),R(1),\ldots,R(X))$. The parameters $\eta$ and $P$ are estimated in the $M$ step.
	
	Then $\pi_{k|N}(i)\log B_{iy_k}(\eta)$ becomes
	$$
	\sum_{k=1}^N \sum_{i=1}^X \pi_{k|N}(i)\paren*{-\frac{\log R(i)}{2}-\frac{(y_k-C(i))^2}{2R(i)}}=-\frac{1}{2}\sum_{k=1}^N \sum_{i=1}^X \pi_{k|N}(i)\paren*{\log R(i)+\frac{(y_k-C(i))^2}{R(i)}}.
	$$
		
	$$P_{ij}=\frac{\sum_{k=1}^N \pi_{k|N}(i,j)}{\sum_{k=1}^N \pi_{k|N}(i)}$$ is the estimate after maximizing $Q(\theta^{s},\theta)$ with respect to the constraint that $P$ is stochastic.
	
	And subsequently, the $M$ step updates for $C$ and $R$ yield
	$C(i) = \frac{\sum_{k=1}^N \pi_{k|N}(i)y_k}{\sum_{k=1}^N \pi_{k|N}(i)}$ and $R(i)=$ (still computing this...).
	
	\subsection*{Forward-Backward Filtering}
	
	It is not obvious how to compute $\pi_{k|N}$ and $\pi_{k|N}(i,j)$ at a first glance. To compute them, a forward-backward filtering algorithm is applied. In this algorithm, two passes are made through the data. The first pass goes forward in time while the second pass goes backward in time.
	
	$$\pi_{k|N}(i)=P(x_k=i|y_{1:N})=P(x_k=i|y_{1:k},y_{k+1:N})\propto P(y_{k+1:N}|x_k=i)P(x_k=i|y_{1:k})=\alpha_{1:k}(i)\beta_{k+1:N}(i)$$ where $\alpha_{1:k}(i) = P(x_k=i|y_{1:k})$ and $\beta_{k+1:N}(i)=P(y_{k+1:N}|x_k=i)$. In shorthand notation, we say $$\pi_{k|N}\propto \alpha_{1:k}\beta_{k+1:N}.$$
	
	$\alpha$ and $\beta$ the forward and backward ``messages", respectively.
	
	Similarly, $$\pi_{k|N}(i,j)\propto \alpha_{1:k}(i)P_{ij}B_{jy_{k+1}}\beta_{k+1:N}(j).$$
	
	To compute the forward and backward messages, this is done recursively. For the forward update, we have
	\begin{align*}
	\alpha_{1:k+1} &=P(x_{k+1}|y_{1:k+1}) \\
	&=P(x_{k+1}|y_{1:k},y_{k+1}) \\
	&\propto P(y_{k+1}|x_{k+1},y_{1:k})P(x_{k+1}|y_{1:k}) \\
	&\propto P(y_{k+1}|x_{k+1}) P(x_{k+1}|y_{1:k}) 
	\end{align*}
	
	And \begin{align*}
	P(x_{k+1}|y_{1:k+1}) &\propto P(y_{k+1}|x_{k+1})\sum_{x_k} P(x_{k+1}|x_k,y_{1:k})P(x_k|y_{1:k}) \\
	&= P(y_{k+1}|x_{k+1})\sum_{x_k} P(x_{k+1}|x_k)P(x_k|y_{1:k}) \\
	&= P(y_{k+1}|x_{k+1})\sum_{x_k} P(x_{k+1}|x_k)\alpha_{1:k}.
	\end{align*}
	
	And, for the backward update:
	\begin{align*}
	\beta_{k+1:N} &=P(y_{k+1:N}|x_k) \\
	&=\sum_{x_{k+1}}P(y_{k+1:N}|x_k,x_{k+1})P(x_{k+1}|x_k) \\
	&= \sum_{x_{k+1}}P(y_{k+1:N}|x_{k+1})P(x_{k+1}|x_k) \\
	&= \sum_{x_{k+1}}P(y_{k+1},y_{k+2:N}|x_{k+1})P(x_{k+1}|x_k) \\
	&= \sum_{x_{k+1}}P(y_{k+1}|x_{k+1})P(y_{k+2:N}|x_{k+1})P(x_{k+1}|x_k) = \sum_{x_{k+1}} P(y_{k+1}|x_{k+1})\beta_{k+2:N}P(x_{k+1}|x_k)
	\end{align*}
	
	Assume we find the observation at the $k$th step is $j$, i.e. $y_k=j$. Then $O_{k} = diag(B_{*j})$, meaning it is a matrix with diagonal entries given by $P(y_k=j|x_k=i)$. In matrix form, the above recursions take the form of:
	$$\alpha_{1:k+1} \propto O_{k+1}P^{\top} \alpha_{1:k}$$
	and
	$$\beta_{k+1:N}=PO_{k+1}\beta_{k+2:N}.$$
	
%	In our case, we have continuous observations, which we assume are distributed Gaussian. Our model is $$y_k = C(x_k) + v_k, \quad v_k\sim N_d(0,R(x_k)))$$ i.i.d where $d$ is the dimension of the observation vectors and $\eta=(C(1),\ldots,C(X),R(1),\ldots,R(X)).$
%	
%	$B_{iy_k}=p(y_k|s_t=i)$ as a density instead of a probability mass and $y_k \sim N_d(C(i),R(i))$.
%	
%	Then $\pi_{k|N}(i)\log B_{iy_k}(\eta)$ becomes
%	$$
%	\sum_{k=1}^N \sum_{i=1}^X \pi_{k|N}(i)\paren*{-\frac{\log R(i)}{2}-\frac{(y_k-C(i))^2}{2R(i)}}=-\frac{1}{2}\sum_{k=1}^N \sum_{i=1}^X \pi_{k|N}(i)\paren*{\log R(i)+\frac{(y_k-C(i))^2}{R(i)}}.
%	$$
%	
%	We optimize $Q$ with the constraints that $\sum_{j} P_{ij}=1$, $\pi_0=(1,0,\ldots,0)$, and $\pi_0 P^t = (0,\ldots,0,1)$. For this, we can use a Lagrangian.
%	
%	Solving this Lagrangian, we get
%	$C(i) = \frac{\sum_{k=1}^N \pi_{k|N}(i)y_k}{\sum_{k=1}^N \pi_{k|N}(i)}$ and $R(i)=$ (still computing this...).
	
	\subsection*{Adapting for Fixed States}
	As mentioned before, our state sequences must be constrained to those state sequences that begin with a ``perfect" state and end at a ``failing" state. The difference between past work and the current work is that both starting and ending states are constrained in the presented research.
	
	There are several ways to carry out this constraint; this work investigates two methods. The first method involves modifying the $M$ step calculation for the expected log-likelihood to add an extra constraint in the Lagrangian. The second method incorporates the constraint in the forward-backward recursion. Ultimately, the second method was chosen for future study, as it was more amenable to a closed form solution.
	
	Since the final state will be a failing state, $$\alpha_{1:N}=P(x_N|y_{1:N})=(0,0,0,1),$$ the vector of all zeroes except for a $1$ in the final position. This is the final $\alpha$ calculated in the forward recursions traditionally, but assuming $O$ and $P$ are invertible,  we see that $$\alpha_{1:k+1}O_{k+1}^{-1}(P^{\top})^{-1} \propto \alpha_{1:k}.$$ Thus we can work backwards from $\alpha_{1:N}$ to determine the rest of the $\alpha$ terms.
	
	Every time we do the $E$ step in our $EM$ algorithm, we must compute $\alpha_{1:j}$ for $j\in \{1,\ldots,N\}$. To that extent, note that $\alpha_{1:1}=(1,0,0,0)$ and $\alpha_{1:N}=(0,0,0,1)$, as shown previously. Then $$\alpha_{1:k}\propto \prod_{i=2}^k (O_{i}P^{\top})\alpha_{1:1}$$ in the forward direction. In the backward direction, $$\alpha_{1:k}\propto \prod_{i=N}^{k} (P^{\top})^{-1} O_{i}^{-1}\alpha_{1:N}.$$
	
	To account for constraints on the final state \textit{and} the start state, we redefine $$\alpha_{1:k} = \frac{c_1\prod_{i=N}^{k} (P^{\top})^{-1} O_{i}^{-1}\alpha_{1:N} + c_2\prod_{i=2}^k (O_{i}P^{\top})\alpha_{1:1}}{2},$$ for $1<k<N$, where $c_1$ and $c_2$ are normalizing constants for the corresponding terms in the numerator.
	
	The backward message updates do not change. These averaged $\alpha$ value,s along with the $\beta$ values, are used to compute $\pi_{k|N}(i)$
	
	And in the $M$ step, we skip parameter updates for $P_{ij}$ when $i<j$ and keep these values fixed at $0$. This idea is inspired by Roweis \cite{hmm}.
	
%	And notice what we are doing is a constrained EM approach. Past results (Tettleton) show that within constrained parameter spaces like the one we have, we still see convergence to local optima.
	
%	\par\noindent\rule{\textwidth}{0.4pt}
	
	In summary, an altered method of forward-backward filtering is used to control the first and last states. This method has not been taken in previous studies, and empirically the results are promising when when using this method.
	
	The estimation of HMM parameters is done for each system trajectory, and then the average of these parameters over all trajectories is calculated. For example, this method estimates the transition matrix for each sample path for a given machine and then takes the average of all these transition matrices to obtain a state transition matrix for that machine.

	The following Tables show the state transition and emission matrices. For emissions, two matrices are used -- one representing the means at each state (Figure 2) and one with variances (Figure 3).
%	In our setting, since $P(s,\mathcal{O})=P(\mathcal{O})P(s|\mathcal{O})$, we can write
%	$$
%	\arg \max_{\theta} \sum_{s\in \mathcal{S}} \log [P(\mathcal{O},s;\theta)] P(s|\mathcal{O};\theta^{s}) = \arg \max_{\theta} \sum_{s\in \mathcal{S}} \log [P(\mathcal{O},s;\theta)] P(s,\mathcal{O};\theta^{s})=\arg \max_{\theta} \hat{Q}(\theta,\theta^s).
%	$$
%	
%	We also see that
%	$$P(s,\mathcal{O};\theta) = \prod_{d=1}^D \paren*{\pi_{s_1^{(d)}}B_{s_1^{(d)}}(o_1^{(d)}) \prod_{t=2}^T A_{s_{t-1}^{(d)}}B_{s_t^{(d)}}(o_t^{(d)})}$$
%	
%	$$
%	\log P(s,\mathcal{O};\theta)=\sum_{d=1}^D \bkt*{\log \pi_{s_1^{(d)}} + \sum_{t=2}^T \log A_{s_{t-1}^{(d)}s_t^{(d)}} + \sum_{t=1}^T \log B_{s_t^{(d)}}(o_t^{(d)})}
%	$$
%	
%	And then, plugging this into $\hat{Q}(\theta,\theta^s)$, we get
%	
%	\begin{align*}
%	\hat{Q}(\theta,\theta^s) = \sum_{s\in \mathcal{S}}\sum_{d=1}^{D} \log \pi_{s_1^{(d)}}P(s,\mathcal{O};\theta^s) &+ \sum_{s\in \mathcal{S}}\sum_{d=1}^{D}\sum_{t=2}^T \log A_{z_{t-1}^{(d)}z_t^{(d)}} P(s,\mathcal{O};\theta^s)P(s,\mathcal{O};\theta^s) \\
%	&+ \sum_{s\in \mathcal{S}}\sum_{d=1}^{D}\sum_{t=1}^T \log B_{z_t^{(d)}}(o_t^{(d)})P(s,\mathcal{O};\theta^s)
%	\end{align*}
%	
%	Our update steps after maximizing using Lagrangians are
%	$$\pi_i^{(s+1)} = \frac{1}{d}\sum_{d=1}^D P(s_1^{(d)}=i|O^{(d)};\theta^s)$$
%	
%	$$A_{ij}^{s+1} = \frac{\sum_{d=1}^D \sum_{t=2}^T P(s_{t-1}^{(d)}=i,s_t^{(d)}=j|O^{(d)};\theta^s)}{\sum_{d=1}^D \sum_{t=2}^T P(s_{t-1}^{(d)}=i|O^{(d)};\theta^s)}$$
%	
%	$$B_i^{(s+1)}(j) = \frac{\sum_{d=1}^D \sum_{t=1}^T P(s_{t}^{(d)}=i|O^{(d)};\theta^s)I(o_t^{(d)}=j)}{\sum_{d=1}^D \sum_{t=1}^T P(s_{t}^{(d)}=i|O^{(d)};\theta^s)}$$
%	
	
%	\begin{align*}
%	\hat{Q}(\theta,\theta^s) = \sum_{s\in \mathcal{S}}\sum_{d=1}^{D} \log \pi_{s_1^{(d)}}P(s,\mathcal{O};\theta^s) &+ \sum_{s\in \mathcal{S}}\sum_{d=1}^{D}\sum_{t=2}^T \log A_{z_{t-1}^{(d)}z_t^{(d)}} P(s,\mathcal{O};\theta^s)P(s,\mathcal{O};\theta^s) \\
%	&+ \sum_{s\in \mathcal{S}}\sum_{d=1}^{D}\sum_{t=1}^T \log B_{z_t^{(d)}}(o_t^{(d)})P(s,\mathcal{O};\theta^s)
%	\end{align*}
	
%	 The transition matrix we get from implementing this customized EM method on our CMAPSS data is 
%	$$\left(
%	\begin{array}{cccc}
%		1 & 0 & 0 & 0 \\
%		0.129 & 0.871 & 0 & 0 \\
%		0 & 0.284 & 0.716 & 0 \\
%		0 & 0 & 0.378 & 0.622 \\
%	\end{array}
%	\right)$$
	
	\begin{figure}
		\centering
		\includegraphics[width=0.5\linewidth]{../../../Pictures/transmat}
		\caption[State transition matrix]{}
		\label{fig:transmat}
	\end{figure}
	
		\begin{figure}
		\centering
		\includegraphics[width=0.5\linewidth]{../../../Pictures/emission_means}
		\caption[Emission matrix]{}
		\label{fig:emissionmeans}
	\end{figure}

	\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{../../../Pictures/covars}
	\caption[Emission matrix covariances]{}
	\label{fig:emissioncovars}
	\end{figure}
	
%	\subsection*{Other methods to consider}
%	Letting $Q(\theta,\theta')$ be the objective function we want to optimize, we originally have $$Q(\theta,\theta')=\sum_{s\in S} \log P(O,s|\theta)P(O,s|\theta')$$ where $S$ represents the set of all possible state sequences. But as stated before, we now have an extra constraint that our start and end states are $M$ and $1$. The route we take is to consider an extra variable $R$ in the formulation of $Q(\theta,\theta')$, which represents a restriction to some subset of sequences. For our setting, we'll use $r\in R$ such that $r_0=M$ and $r_T=1$. This becomes:
%	
%	\begin{align*}
%	P(O,s|\theta,R) &= P(O|s,\theta,R)P(s|\theta,R) \\
%	&= P(O|s,\theta,R)P(s|\theta)P(s|R) \\
%	&= \prod_{t=1}^T P(o_t|s_t)P(s_t|s_{t-1})P(s_t|r_t) \\
%	&= \prod_{t=1}^T B_{s_t,o_t} A_{s_{t-1},s_t} P(s_t|r_t)
%	\end{align*}
%	
%	The crux here is that $P(s_1|r_1)$ is either zero or one (and so is  $P(s_T|r_T)$ ). If $s_1=M$, then $P(s_1|r_1)=1$ and zero otherwise. Similarly, if $s_T=M$, then $P(s_T|r_T)=1$ and zero otherwise.
%	
%	This assumes conditional independence of the parameters $\theta$ and the restricted sequence $R$.
%	
%	Alternatively, we do not assume independence of $\theta$ and $R$. So instead, we have the first line from above and we would like to find $\theta$ such that this is optimal.
%	
%	$$p(O|R,\theta) = \sum_{s\in S} p(O,s|R,\theta) = \sum_{s\in S} p(O|s,R,\theta) p(s|R,\theta)$$
%	
%	Then we can expand the expected log-likelihood expression to
%	\begin{align*}
%	Q(\hat{\theta},\theta) &= \sum_{s\in S} p(O|s,R,\theta)p(s|R,\theta) \log(p(O|s,R,\hat{\theta})) \\
%	&+ \sum_{s\in S} p(O|s,R,\theta)p(s|R,\theta) \log (P(s|R,\hat{\theta}))
%	\end{align*}
%	
	With the hidden states estimated, we can introduce the POMDP model.
	
	\subsection*{Variational Autoencoders}
	The prior two approaches involved treating the preventive maintenance problem as a HMM. Conversely, deep learning techniques such as variational autoencoders can be used to tackle this problem. The intuition behind the VAE approach is the desire to maximize the sum of log marginal likelihood terms $\sum_{n=1}^N \log p_{\theta}(o^{(n)})$ for observations $(o^{(n)})_{n=1}^N$ where $p_{\theta}(o)=\int p_{\theta}(o|s)p_{\theta}(s)\,ds.$ This is intractable in practice, so instead the sum of ELBOs is maximized, where each ELBO term is a lower bound of the log marginal likelihood.
	$$ELBO(\theta,\phi,o)=\E_{q_{\phi}(s|o)}[\log \frac{p_{\theta}(o|s)p_{\theta}(s)}{q_{\phi}(s|o)}]$$ for a family of encoders $q_{\phi}(s|o)$ parameterised by $\phi$. One can see the analogy between this and the EM approach discussed earlier. This study uses a surrogate expression, ELBO, instead of the log marginal distribution. 
	
	\subsection*{Deep Variational RL}
	Lastly, estimation and control are incorporated by using deep variational reinforcement learning. Fundamentally, instead of learning a generative model that maximizes just the ELBO functions, the newly developed method also optimizes an expected return. The proposed method was implemented on CMAPSS data using Tensorflow \cite{tensor}.  Tensorflow was chosen over other machine learning libraries because of the need to restrict the state sequences from model estimates. Tensorflow, although more difficult to prototype in, leads itself to customization far easier than most other tools available.
	 
	\section*{Detailed Discussion of Two Research Pathways}
	Currently, the models in place treat the preventive maintenance problem as an MDP, focusing solely on measurement observations $(y_t)$ while ignoring the potential hidden states $(x_t)$. This leads to the first research path.
	
	\subsection*{Path 1: Single Source}
	The first research path proposed is the use of single source prior knowledge.  This path takes the form of:
	
	\begin{itemize}
		\item \textbf{Estimation:} Estimating an underlying state and then use model-based approaches to solve for optimal repair strategy to reduce total maintenance cost.
		\item \textbf{Control:} Determining optimal actions without knowledge of underlying states (model-free reinforcement-learning approach).
		\item \textbf{Combining Estimation and Control:} Understanding the benefits of using a deep-RL approach that combines the two.
	\end{itemize}
	
	
	To test this research approach, a machine running until failure was simulated. The underlying states $(1,2,3,4)$ represent the health level of the machine, with $1$ indicating very-low/failing and $4$ very-high/perfect condition. A level of $2$ denotes low but not failing and $3$ moderately high health.
	
	The state transitions are given by 
	$$
	\begin{bmatrix}
	p_{11} & 0 & 0 & 0 \\
	p_{21} & p_{22} & 0 & 0 \\
	0 & p_{32} & p_{33} & 0 \\
	0 & 0 & p_{43} & p_{44}
	\end{bmatrix}
	$$
	where $p_{ij}$ denotes the probability of the system health moving from level $i$ to $j$. The above system is for the case where no action is taken. This model assumes gradual degradation.
	
	The actions here are either to repair $a=1$ or do nothing $a=0$.
	
	When a repair is done, then
	$$
	\begin{bmatrix}
	0 & p'_{12} & p'_{13} & p'_{14} \\
	0 & 0 & p'_{23} & p'_{24} \\
	0 & 0 & 0 & p'_{34} \\
	0 & 0 & 0 & p'_{44}
	\end{bmatrix}
	$$
	
	is the transition matrix, where $p'$ are the probabilities of when a repair is executed. 
	
	The observations depend on whether or not the systems is repaired. Furthermore, the observation space is continuous with the observations distributed by pdfs $f_0$ and $f_1$ for actions ``no-repair" and ``repair", respectively.
	
	If no repair is executed, then define $O(o|s,a=0) \sim f_0(s)$. Else, if there is a repair, then $O(o|s,a=1) \sim f_1(s)$.
	
	Using discrete observation buckets, a matrix is given of form:
	$$    O(0) = 
	\begin{bmatrix}
	No Repair & 1 & 2 & 3 & 4 \\
	1 & o_{11} & o_{12} & o_{13} & o_{14} \\
	2 & o_{21} & o_{22} & o_{23} & o_{24} \\
	3 & o_{31} & o_{32} & o_{33} & o_{34} \\
	4 & o_{41} & o_{42} & o_{43} & o_{44} \\
	\end{bmatrix}
	$$
	
	and
	
	$$    O(1) = 
	\begin{bmatrix}
	Repair & 1 & 2 & 3 & 4 \\
	1 & o'_{11} & o'_{12} & o'_{13} & o'_{14} \\
	2 & o'_{21} & o'_{22} & o'_{23} & o'_{24} \\
	3 & o'_{31} & o'_{32} & o'_{33} & o'_{34} \\
	4 & o'_{41} & o'_{42} & o'_{43} & o'_{44} \\
	\end{bmatrix}
	$$
	
	Lastly, the reward function takes the following form:
	$$\sum_{t=1}^T \sum_{s=1}^4 [c_1(4-s_t)I(s_t=j) + c_2(s_t)I_M(a_t=1|s_t=j)]]$$
	where $I$ is an indicator function. $c_1(4-s_t)$ is the cost of maintenance and $c_2(s_t)$ is the cost of being in a degraded state.
	
	Another way of writing this is as follows:
	
	$$
	R(0) = 
	\begin{bmatrix}
	No Repair & Reward \\
	1 & r_1 \\
	2 & r_2 \\
	3 & r_3 \\
	4 & r_4 \\
	\end{bmatrix}
	$$
	
	$$
	R(1) = 
	\begin{bmatrix}
	Repair & Reward \\
	1 & r'_1 \\
	2 & r'_2 \\
	3 & r'_3 \\
	4 & r'_4 \\
	\end{bmatrix}
	$$
	 
	 \begin{table}[h!]
	 	\begin{center}
	 		\caption{Reward table.}
	 		\label{tab:table1}
	 		\begin{tabular}{l|c|r} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
	 			\textbf{State} & \textbf{Repair} & \textbf{Not Repair}\\
	 			%$\alpha$ & $\beta$ & $\gamma$ \\
	 			\hline
	 			1 & 100 & -50\\
	 			2 & 50 & 0\\
	 			3 & 0 & 50\\
	 			4 & -50 & 100
	 		\end{tabular}
	 	\end{center}
	 \end{table}
	 
	 For example, the reward for repairing in state $2$ is $50$.
	
	The overarching goal is to use reinforcement learning methods with POMDPs to give an optimal strategy for repairing the system that minimizes the total repair costs and losses due to non-repair.
	
	Two methods can be used to realize this approach to the given research problsm:
	\begin{enumerate}
		\item Using an EM-based approach for estimation and then determining an optimal control policy using any POMDP solver. For example, using a VAE (variational autoencoder) for time series. This approach doesn't incorporate control, instead maximizing a likelihood function and outputting a particle filter model.
		\item A DVRL (deep variational reinforcement learning) approach, which combines both estimation and control. This approach optimizes an objective cost function that depends on model parameters.
	\end{enumerate}
	
	Both of these approaches also incorporate knowledge of the start and end states being in perfect and poor condition respectively, which is discussed in the following section.
	
	\section*{Current Status 1}
	
	So far, the authors have studied four different POMDP based techniques for modeling the preventive maintenance problem. We implemented value-iteration, Monte-Carlo Tree Search (POMCP), ADRQN, and DVRL on the CMAPSS dataset. After realizing these implementations, we are currently analyzing the performance of deep-RL methods such as DVRL and ADRQN versus model-based methods. We have also incorporated the constraints on the start and end states discussed in this paper via a method similar to boosting, where we propagate the errors between our true start and end states and our estimates and re-running DVRL until we reach convergence. The next steps are to analyze how two sources of information (prior knowledge with observations) should be combined to determine optimal repair strategies.
	
	\subsection*{Results from Separating Estimation and Control}
	After doing HMM estimation and POMDP value iteration, we developed the utility vector seen in Figure 4.
	
		\begin{figure}
		\centering
		\includegraphics[width=0.25\linewidth]{../../../Pictures/utilityvector}
		\caption[Utility vector]{}
		\label{fig:utilityvec}
	\end{figure}
	
	In practice, there is often some existing knowledge about the system process before seeing any real-time measurements. This knowledge leads to the second research path.
	
	\subsection*{Path 2: Combining Multiple Sources}
	
	Here, this study asks how to combine prior knowledge about the data generating process when determining the optimal repair strategy that minimizes costs. For example, if given a controlled system generating data, this alters the observed machine measurements as opposed to an uncontrolled system. This alteration is because if the machine is deteriorating, a repair and restoration the system back to full health is executed such that the measurements seen will be different than if machine fails.
	
	The alternative route would be to identify enough of the distribution, based on impact to the objective function, and disambiguate. 
	\\
	\\
	For now, assume the data generating process takes on one of the following forms:
	\begin{itemize}
		\item No prior distribution -- data is generated via bootstrapping. For example, with the C-MAPPS data, data is resampled from the trajectories.
		\item One prior distribution:
		\begin{itemize}
			\item Can this prior distribution be trusted, or should it be ignored and just bootstrap a distribution; or, should the distributions just be combined?
		\end{itemize}
		\item If there are two prior distributions, which one generates our data, or how do they interact with each other? For example, they can be combined a ``weighted average" (partial update) or fully via a Bayesian update.
	\end{itemize}
	
	These three possible ways of combining the two models are under investigation:
	\begin{enumerate}
		\item One generating model at each timestep
			\begin{itemize}
				\item In this setting, only one of $P_1$ or $P_2$ generates data.
			\end{itemize}
		\item Mixture model
		\item Hierarchical Bayes
	\end{enumerate}
	
	Analogously to the single source model, there is another layer of complexity when studying estimation versus control here. The analysis either:
	\begin{itemize}
		\item Estimates the underlying model and then comes up with a control policy using POMDP-R.
		\item Performs control and estimation at once using a method such as DVRL again.
	\end{itemize}
	
	This section gives one a glimpse of some of the open problems the authors are addressing.
	
	\subsection*{Sequential Testing}
	Thomas Kolarik's paper ``Using SPRT as stopping condition" 
	\cite{url} introduces Sequential Probability Ratio Testing (SPRT) for different families of hypotheses.
	
	When there are two families of composite hypotheses, i.e.,
	$$H_0: f\in \{g_{\theta}:\theta\in \Theta\} \textrm{ against } H_A:f\in \{h_{\gamma}:\gamma\in \Gamma\}$$, we consider a sequential test where we compute $$L_n = \frac{\sup_{\gamma\in \Gamma} \prod_{i=1}^n h_{\gamma}(X_i)}{\sup_{\theta\in \Theta} \prod_{i=1}^n g_{\theta}(X_i)}.$$ If $L_n$ crosses $e^A$ or $e^{-B}$ for constants $A,B>0$, then we stop sampling. If $L_n>e^A$, then we reject the null.
	
	This is then used to infer which distribution is generating the data.  The current research component is understanding how to use this with particle filters. By using a variational autoencoder to learn an underlying model, the output becomes a particle filter. Currently, there has not been any published research on how to use SPRT with particle filters.
	
	\subsection*{Example: Mixture Model Setting}
	What this study proposes to solve is initially the following:
	if the data is generated from $\epsilon_t P_0 + (1-\epsilon_t) P_1$, where $P_0$ and $P_1$ are the two priors, does $\epsilon_t\to 0$ or to a constant over time? Maybe $P_0$ is one prior generated from bootstrapping and $P_1$ is the second prior provided or based on data that incorporates domain knowledge.
	
	In this setting, the $z_t=\{x_t,y_t\}$ pairs can be thought of as coming from data sources $P_0$ and $P_1$.
	
	Define $p(z_t|P_0)$ and $p(z_t|P_1)$ to be the probabilities of observing $z_t$ given $P_0$ and $P_1$ respectively at time $t$. Then $$p(P_j|z_t)=\frac{\epsilon_t p(z_t|P_0) I(j=0) + (1-\epsilon_t) p(z_t|P_1) I(j=1)}{\epsilon_t p(z_t|P_0) + (1-\epsilon_t) p(z_t|P_1)}.$$
	
	But now, this study seeks to understand
	$$p(\{P_{j_1},\ldots,P_{j_T}\}|\{z_1,\ldots,z_T\})$$ where each $P_{jt}$ is either $P_0$ or $P_1$.
	
	The simplistic approach to estimate $\epsilon$ would be to do a maximization of the above quantity over all assignments of $\{P_{j1},\ldots,P_{jT}\}$ to the set $\{P_0,P_1\}^T$. This method, however, is computationally intensive and furthermore does not account for time-dependence in the data.
	
	Therefore, the authors propose modeling this as a multiarmed bandit problem, where $P_0$ and $P_1$ are the two arms. The goal here is to then use concentration inequalities to understand good approximate repair policies. These will be approximate because there is a dependency between actions taken at different timesteps and we are looking at finite-horizon solutions.
	
	%We propose a method which uses RL to optimize an objective function which implicitly tells us the proportion of time data comes from $P_0$ vs $P_1$. At time $t$, after we observe $z_1,\ldots,z_t$, we can do one of three actions. Either we choose $P_0$ or $P_1$ as our hypothesis of the prior, or we continue exploration, which incurs a cost $C$. If we choose incorrectly between $P_0$ and $P_1$, we incur a cost $L$ and if we were correct, we don't incur any cost.
	%
	%This is now a three action POMDP where the observations are the $z_i$'s and the underlying states are $P_0$ and $P_1$.
	%
	%Our observation distributions are given by the pdfs $O_s(z)$, which can be one of $O_0(z)$ or $O_1(z)$, depending on whether the underlying data comes from $P_0$ or $P_1$ at that time.
	%
	%State transitions, however, require a little more thought. For now, the transitions between our two states is just a 2x2 matrix
	%
	%$$
	%\begin{bmatrix}
	%p_{00} & p_{01} \\ 
	%p_{10} & p_{11} 
	%\end{bmatrix}
	%$$
	%
	%For our cost function, define this again to be $R_s(a)=C*I(a=2)+L*I(a<2,s\neq a)$ where $s\in \{0,1\}$ represents the underlying distribution and $a\in \{0,1,2\}$ represents choosing $P_0$, $P_1$, or neither. So if $a=1$, we chose $P_1$. One way of dealing with the time-dependent structure here is by incorporating a discount factor $\gamma$ that weighs the most recent reward more.
	
	%In this setting, we cannot use a model-based approach since we don't have knowledge about our state transitions. But with a model-free approach such as deep variational reinforcement learning \url{https://arxiv.org/pdf/1806.02426.pdf}, we can infer the proportion of time data comes from each generating process. One potential route to take here is to use a generalized sequential probability ratio test for separate families of hypotheses \url{http://stat.columbia.edu/~jcliu/paper/GSPRT_SQA3.pdf} to better understand how the distributions are being combined. Another possible approach is to use a hierarchical Bayesian model to relate $P(z_t|P_0)$ and $P(z_t|P_1)$ and update the previously mentioned weighted mixture model.
	
	\section*{Current Status 2}
	
	For this research path, the first priority is studying how we can combine particle filters (outputs of VAE) with SPRT. An open problems that will be tackled after this combination is resolved are how to get guarantees on learning multiple-source distributions using concentration-type inequalities.
	%
	%\section*{Appendix: End Of Life Analysis Background}
	%The goal of the preliminary analysis is to create a Bayesian probabilistic survival model, using incomplete data sets in a general way.  The goal of this section is to define the model and methodology used and to outline options and identify risk areas.
	%
	%Let T be the continuous nonnegative random variable representation the end of life time of a machine in some population.  Let f(t) be the pdf of T and let the distribution function be
	%\begin{equation}
	%F(t) = P( t \le t) = \int_0^t f(u) du
	%\end{equation}
	%The probability of a machine functioning until time t is given by the function
	%\begin{equation}
	%S(t) = 1 - F(t)=P(T>t)
	%\end{equation}
	%where S(0)=1 and $S(\infty)=\lim_{t \rightarrow \infty} S(t)=0$.  The hazard function h(t) is defined as the instantaneous rate of failure at time t
	%\begin{equation}
	%h(t)=\frac{f(t)}{S(t)}
	%\end{equation}
	%The interpretation is that $h(t)\Delta t$ is the approximate probability of failure in $(t, t+\Delta t)$.  It is easy to show that
	%\begin{equation}
	%h(t)=-\frac{d}{dt}log(S(t))
	%\end{equation}
	%Integrating both sides and exponentiating we get
	%\begin{equation}
	%S(t)=exp(-\int_0^t h(u) du)
	%\end{equation}
	%The hazard function H(t) and the survivor function S(t) are related by
	%\begin{equation}
	%S(t)=exp(-H(t))
	%\end{equation}
	%and the hazard function h(t) has the properties
	%\begin{equation}
	%h(t) \ge 0 \quad  \text{and} \quad  \int_0^\infty h(t)dt = \infty
	%\end{equation}
	%Finally the survival pdf is given by
	%\begin{equation}
	%f(t)=h(t)exp(-\int_0^t h(u) du)
	%\end{equation}
	%\subsection*{The Weibull distribution}
	%The Weibull distribution is one of the most commonly used distributions to model survival pdf, and is the distribution we will use to model survival pdf
	%\begin{equation}
	%f(t)=\begin{cases}
	%\alpha \gamma t^{\alpha -1}exp(-\gamma t^{\alpha}) \quad t>0, \alpha >0, \gamma>0\\
	%0 \quad \text{otherwise}
	%\end{cases}
	%\end{equation}
	%The Weibull distribution is denoted by $\mathcal{W}(\alpha, \gamma)$ where $\alpha$ and $\gamma$ are the parameters of the distribution.  For this distribution the hazard distribution h(t) is monotonically increasing when $\alpha >1$ and monotonically decreasing when $0 \alpha < 1$.  It follows that
	%the survivor function is 
	%\begin{equation} \label{eq:W}
	%S(t)=exp(-\gamma t^\alpha)
	%\end{equation}
	%and the hazard function is
	%\begin{equation}
	%h(t)=\gamma \alpha t^{\alpha -1}
	%\end{equation}
	%and the cumulative hazard function H(t) is given by
	%\begin{equation}
	%H(t)=\gamma t ^\alpha
	%\end{equation}
	%\subsection*{Proportional Hazard Model}
	%The hazard function depends on both time and a set of covariates.  The proportional hazards model separates these components by by specifying that the hazard at time t, for a machine whose covariate vector is $\mathbf{x}$ is given by the linear relatioship
	%\begin{equation}
	%h(t|\mathbf{x})=h_0(t) exp(\mathbf{x}' \mathbf{\beta})
	%\end{equation}
	%where $\mathbf{\beta}$ is a vector of regression coefficients.
	%\subsection*{Censoring}
	%We expect the end of life sensor data is right censored, that is, the survival times are only known for a portion of machines under study.  The likelihood function for righ censored data will be constructed as follows.  Suppose there are n machines and associated with the $i^{th}$ individual is a survival time $t_i$ and a fixed censoring time $c_i$.  The $t_o$ are assumed to be independent and identically distributed with density f(t) and survival function S(t).  The exact survival time for machine i, $t_i$, will be observed only if $t_i \le c_i$,  The data can be represented as the n pairs of random variables $(y_i, \nu_i)$ where
	%\begin{equation}
	%y_i=\min(t_i, c_i)
	%\end{equation}
	%and
	%\begin{equation}
	%\nu_i=\begin{cases}
	%1 \quad \text{if} \quad t_i \le c_i,\\
	%0 \quad \text{if} \quad t_i > c_i
	%\end{cases}
	%\end{equation}
	%Then the likelihood function for ($\mathbf{\beta}, h_0(.)$) for a set of right censored data is given by
	%\begin{equation}
	%\begin{split}
	%L(\mathbf{\beta}, h_0(t) | D) \propto \prod_{i=1}^n [h_o(y_i) exp(\eta_i)]^{\nu_i} (S_0(y_i)^{exp(\eta_i)})\\
	%\prod_{i=1}^n [h_o(y_i) exp(\eta_i)]^{\nu_i} exp \left \{ 
	%-\sum_{i=1}^n exp(\eta_i)H_0(y_i) \right \}
	%\end{split}
	%\end{equation}
	%where $D=(n,\mathbf{y}, X, \mathbf{\nu})$, $\mathbf{y}=(y_1, y_2,...,y_n)'$ and $\mathbf{\nu}=(\nu_1, \nu_2, ...,\nu_n)$, $\eta_i=\mathbf{x}_i' \mathbf{\beta}$ is the linear predictor of machine i, and
	%\begin{equation}
	%S_0(t)=exp(-\int_0^t h_0(u)du)=exp(-H_0(t))
	%\end{equation}
	%is the baseline survivor function.
	
	\begin{thebibliography}{1}
		\bibitem{GE} Predix Platform, online: https://www.ge.com/digital/iiot-platform/machine-learning-analytics, last access 10-2-2019.		
		
		\bibitem{tensor} Abadi, Martín, et al. ``Tensorflow: Large-scale machine learning on heterogeneous distributed systems." \textit{arXiv preprint arXiv:1603.04467} (2016). 

		\bibitem{hmm}S. Roweis. Constrained Hidden Markov Models. Advances in neural information processing systems, 2000.
		
		\bibitem{url} T. Kolarik, ``Use SPRT as stopping condition," online: http://stat.columbia.edu/~jcliu/paper/GSPRT\_SQA3.pdf
		
	\end{thebibliography}
	
\end{document}