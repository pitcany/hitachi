\documentclass[english]{article}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amscd}
\usepackage[latin2]{inputenc}
\usepackage{t1enc}
\usepackage[mathscr]{eucal}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{pict2e}
\usepackage{epic}
\usepackage{hyperref}
\usepackage{mathtools}
\numberwithin{equation}{section}
\usepackage[margin=2.9cm]{geometry}
\usepackage{epstopdf}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\newcommand{\ovl}{\overline}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}

\DeclarePairedDelimiter\paren{(}{)}           % (parentheses)
\DeclarePairedDelimiter\ang{\langle}{\rangle} % <angle brackets>
\DeclarePairedDelimiter\abs{\lvert}{\rvert}   % |absolute value|
\DeclarePairedDelimiter\norm{\lVert}{\rVert}  % ||norm||
\DeclarePairedDelimiter\bkt{[}{]}             % [brackets]
\DeclarePairedDelimiter\set{\{}{\}}           % {braces}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.

\begin{document}
	
	\title{Writeup}
	
	\author{Yannik Pitcan \\
		Ram Akella
	}
	\maketitle
	
	\section*{Research Objective}
	Deep reinforcement learning methods have not been studied for the preventive maintenance problem, not withstanding claims made by firms such as GE. The first goal is to understand how well such deep reinforcement learning methods can help us implicitly predict quantities such as RUL (remaining useful life) and achieve close to optimal repair policies. The second goal is to investigate how we can combine observations about the system process with prior knowledge and for multiple predictor distributions. In particular, we want to understand how to mix the two models to get a better understanding of our system. One is data-based empirical knowledge and the second is either prior knowledge or expertise-based adaptation of empirical knowledge. This will be combined with real-time observations. Currently, the first task is completed with deep reinforcement learning and classical POMDP implementations for the CMAPPS dataset. The focus from now onwards is the second task.
	
	\subsection*{Dynamical System Consideration}
	Past work on RL for preventive maintenance had promise, but suffered from a few flaws. One of them was the lack of distinguishment between covariates and sensor readings. For example, past work attempted to understand the RUL distribution via fitting a Weibull distribution. But, the fitted parameters ($\lambda$ and $k$) were dependent on input covariates. This takes us to our second point that summary values such as RUL (remaining useful life) were used to represent the full distribution, when a set of samples $(x_t,y_t)$ from said distribution may have been preferable. Our current work differs from past work in that it models the data as a dynamical system with latent states and observations and avoids encapsulating knowledge of the distributions by a summary statistic.
	\subsection*{POMDP vs MDP}
	The use of a Partially Observable Markov Decision Process goes in tandem with distinguishing between covariates and sensor readings. In the past work when the preventive maintenance problem was treated as an MDP, it ignored the fact that there may be hidden states (for example, system health) that impacted sensor observations.
	
	\section*{Outline}
	We demonstrate two methods. One will be fitting an HMM to the provided data and then we have a POMDP. After that, we can solve the POMDP using model-based approaches.
	
	The other approach involves using POMDP-RL techniques using deep variational autoencoders
	.
	\subsection*{Measuring Hidden States}
	The question then becomes: "How we can determine these hidden states?
	
	Let $S=\{1,2,\ldots,M\}$ be the space of underlying states and the space of observations be $O=\{1,2,\ldots,N\}$. An HMM $\theta=(\pi,A,B)$ is parameterized by $\pi$, $A$, and $B$ representing the initial state matrix, state transition matrix, and emission matrix respectively. Let $\pi_i=P(s_1=i)$, $A_{ij}=P(s_{t+1}=j|s_t=i)$ and $B_i(j)=P(o_t=j|s_t=i)$.
	
	For estimation, we can use an EM framework, treating the hidden states as the latent states in a Hidden Markov Model with our measurements as the observations. The latent states represent the health of the machine, where $1$ is failing condition and $M$ is perfect condition.
	
	Let $\mathcal{O} = (O^{(1)},\ldots,O^{(D)})$ be the set of $D$ observation sequences, where each $O^{(i)}=(o_1^{(i)},\ldots,o_{T}^{(i)})$ and assume each observation is an iid draw.

	The technique we use is to repeat the following steps until convergence ($\theta = (\pi,A,B)$):
	\begin{itemize}
		\item Compute $Q(\theta,\theta^s) = \sum_{s\in \mathcal{S}} \log [P(\mathcal{O},s;\theta)]P(s|\mathcal{O};\theta^s).$
		\item Set $\theta^{s+1} = \arg \max_{\theta} Q(\theta, \theta^s)$
	\end{itemize}
	
	Let $\theta^s = (\pi^s,A^s,B^s)$. In our setting, since $P(s,\mathcal{O})=P(\mathcal{O})P(s|\mathcal{O})$, we can write
	$$
	\arg \max_{\theta} \sum_{s\in \mathcal{S}} \log [P(\mathcal{O},s;\theta)] P(s|\mathcal{O};\theta^{s}) = \arg \max_{\theta} \sum_{s\in \mathcal{S}} \log [P(\mathcal{O},s;\theta)] P(s,\mathcal{O};\theta^{s})=\arg \max_{\theta} \hat{Q}(\theta,\theta^s).
	$$
	
	We also see that
	$$P(s,\mathcal{Y};\theta) = \prod_{d=1}^D \paren*{\pi_{s_1^{(d)}}B_{y_1^{(d)}}(y_1^{(d)}) \prod_{t=2}^T A_{s_{t-1}^{(d)}}B_{s_t^{(d)}}(y_t^{(d)})}$$
	
	$$
	\log P(s,\mathcal{O};\theta)=\sum_{d=1}^D \bkt*{\log \pi_{s_1^{(d)}} + \sum_{t=2}^T \log A_{z_{t-1}^{(d)}z_t^{(d)}} + \sum_{t=1}^T \log B_{z_t^{(d)}}(o_t^{(d)})}
	$$
	
	And then, plugging this into $\hat{Q}(\theta,\theta^s)$, we get
	
	\begin{align*}
	\hat{Q}(\theta,\theta^s) = \sum_{s\in \mathcal{S}}\sum_{d=1}^{D} \log \pi_{s_1^{(d)}}P(s,\mathcal{O};\theta^s) &+ \sum_{s\in \mathcal{S}}\sum_{d=1}^{D}\sum_{t=2}^T \log A_{z_{t-1}^{(d)}z_t^{(d)}} P(s,\mathcal{O};\theta^s)P(s,\mathcal{O};\theta^s) \\
	&+ \sum_{s\in \mathcal{S}}\sum_{d=1}^{D}\sum_{t=1}^T \log B_{z_t^{(d)}}(o_t^{(d)})P(s,\mathcal{O};\theta^s)
	\end{align*}
	
	\subsection*{Adapting for the initial and end states being fixed}
	In our setting, the problem is trickier because we must constrain our set of state sequences to those that begin with a "perfect" state and end at a "failing" state. We propose to optimize $\hat{Q}(\theta,\theta^s)$ with not only constraints that $\pi$, $A$, and $B$ are valid probability distributions, but also extra constraints on $A^{(T)}$ and $\pi_1$.
	
	\subsection*{Other methods to consider}
	Letting $Q(\theta,\theta')$ be the objective function we want to optimize, we originally have $$Q(\theta,\theta')=\sum_{s\in S} \log P(O,s|\theta)P(O,s|\theta')$$ where $S$ represents the set of all possible state sequences. But as stated before, we now have an extra constraint that our start and end states are $M$ and $1$. The route we take is to consider an extra variable $R$ in the formulation of $Q(\theta,\theta')$, which represents a restriction to some subset of sequences. For our setting, we'll use $r\in R$ such that $r_0=M$ and $r_T=1$. This becomes:
	
	\begin{align*}
	P(O,s|\theta,R) &= P(O|s,\theta,R)P(s|\theta,R) \\
	&= P(O|s,\theta,R)P(s|\theta)P(s|R) \\
	&= \prod_{t=1}^T P(o_t|s_t)P(s_t|s_{t-1})P(s_t|r_t) \\
	&= \prod_{t=1}^T B_{s_t,o_t} A_{s_{t-1},s_t} P(s_t|r_t)
	\end{align*}
	
	The crux here is that $P(s_1|r_1)$ is either zero or one (and so is  $P(s_T|r_T)$ ). If $s_1=M$, then $P(s_1|r_1)=1$ and zero otherwise. Similarly, if $s_T=M$, then $P(s_T|r_T)=1$ and zero otherwise.
	
	This assumes conditional independence of the parameters $\theta$ and the restricted sequence $R$.
	
	Alternatively, we do not assume independence of $\theta$ and $R$. So instead, we have the first line from above and we would like to find $\theta$ such that this is optimal.
	
	$$p(O|R,\theta) = \sum_{s\in S} p(O,s|R,\theta) = \sum_{s\in S} p(O|s,R,\theta) p(s|R,\theta)$$
	
	Then we can expand the expected log-likelihood expression to
	\begin{align*}
	Q(\hat{\theta},\theta) &= \sum_{s\in S} p(O|s,R,\theta)p(s|R,\theta) \log(p(O|s,R,\hat{\theta})) \\
	&+ \sum_{s\in S} p(O|s,R,\theta)p(s|R,\theta) \log (P(s|R,\hat{\theta}))
	\end{align*}
	
	Now that we have hidden states estimated, we can introduce the POMDP model.
	
	\subsection*{Variational Autoencoders}
	The prior two approaches involved treating our preventive maintenance problem as a hidden Markov Model. We can use deep learning techniques such as variational autoencoders instead to tackle this problem. The intuition behind the VAE approach is we want to maximize the sum of log marginal likelihood terms $\sum_{n=1}^N \log p_{\theta}(o^{(n)})$ for observations $(o^{(n)})_{n=1}^N$ where $p_{\theta}(o)=\int p_{\theta}(o|s)p_{\theta}(s)\,ds.$ This is intractable in practice so instead we maximize the sum of ELBOs where each ELBO term is a lower bound of the log marginal likelihood.
	$$ELBO(\theta,\phi,o)=\E_{q_{\phi}(s|o)}[\log \frac{p_{\theta}(o|s)p_{\theta}(s)}{q_{\phi}(s|o)}]$$ for a family of encoders $q_{\phi}(s|o)$ parameterised by $\phi$. One can see the analogy between this and the EM approach discussed earlier. We're using a surrogate expression, ELBO, instead of the log marginal distribution.
	 
	\section*{Detailed Discussion of Two Research Pathways}
	Currently, the models in place just treat the preventive maintenance problem as an MDP, focusing solely on measurement observations $(y_t)$ while ignoring the potential hidden states $(x_t)$. This leads us to discuss the first research direction.
	\subsection*{Path 1: Single Source}
	This may be split into two parts:
	
	\begin{itemize}
		\item \textbf{Estimation:} System identification -- estimate an underlying state and then use model-based approaches to solve for optimal repair strategy to reduce total maintenance cost.
		\item \textbf{Control:} Determining optimal actions without knowledge of underlying states (model-free reinforcement-learning approach)
		\item \textbf{Combining Estimation and Control:} Understanding the benefits of using a deep-RL approach that combines the two.
	\end{itemize}
	
	
	As before, we create a simulation for a machine running until it reaches failure. The underlying states $(1,2,3,4)$ represent the health level of the machine, with $1$ indicating very-low/failing and $4$ very-high/perfect condition. A level of $2$ denotes low but not failing and $3$ moderately high health.
	
	Our state transitions are given by 
	$$
	\begin{bmatrix}
	p_{11} & 0 & 0 & 0 \\
	p_{21} & p_{22} & 0 & 0 \\
	0 & p_{32} & p_{33} & 0 \\
	0 & 0 & p_{43} & p_{44}
	\end{bmatrix}
	$$
	where $p_{ij}$ denotes the probability of the system health moving from level $i$ to $j$. The above system is for the case where no action is taken. This model assumes gradual degradation.
	
	The actions here are either to repair $a=1$ or do nothing $a=0$.
	
	When a repair is done, then
	$$
	\begin{bmatrix}
	0 & p'_{12} & p'_{13} & p'_{14} \\
	0 & 0 & p'_{23} & p'_{24} \\
	0 & 0 & 0 & p'_{34} \\
	0 & 0 & 0 & p'_{44}
	\end{bmatrix}
	$$
	
	is our transition matrix, where we use $p'$ to denote these are probabilities when we do a repair.
	
	Our observations depend on whether we repair the system or not. Furthermore, the observation space is continuous with the observations distributed by pdfs $f_0$ and $f_1$ for actions 'no-repair' and 'repair' respectively.
	
	If we don't repair the system, then define $O(o|s,a=0) \sim f_0(s)$. Else, if we do repair, then $O(o|s,a=1) \sim f_1(s)$.
	
	If we use discrete observation buckets, then we have a matrix form
	$$    O(0) = 
	\begin{bmatrix}
	No Repair & 1 & 2 & 3 & 4 \\
	1 & o_{11} & o_{12} & o_{13} & o_{14} \\
	2 & o_{21} & o_{22} & o_{23} & o_{24} \\
	3 & o_{31} & o_{32} & o_{33} & o_{34} \\
	4 & o_{41} & o_{42} & o_{43} & o_{44} \\
	\end{bmatrix}
	$$
	
	and
	
	$$    O(1) = 
	\begin{bmatrix}
	Repair & 1 & 2 & 3 & 4 \\
	1 & o'_{11} & o'_{12} & o'_{13} & o'_{14} \\
	2 & o'_{21} & o'_{22} & o'_{23} & o'_{24} \\
	3 & o'_{31} & o'_{32} & o'_{33} & o'_{34} \\
	4 & o'_{41} & o'_{42} & o'_{43} & o'_{44} \\
	\end{bmatrix}
	$$
	
	Lastly, our reward function takes the following form:
	$$\sum_{t=1}^T \sum_{s=1}^4 [c_1(4-s_t)I(s_t=j) + c_2(s_t)I_M(a_t=1|s_t=j)]]$$
	where $I$ is an indicator function. $c_1(4-s_t)$ is the cost of maintenance and $c_2(s_t)$ is the cost of being in a degraded state.
	
	Another way of writing this is as follows
	
	$$
	R(0) = 
	\begin{bmatrix}
	No Repair & Reward \\
	1 & r_1 \\
	2 & r_2 \\
	3 & r_3 \\
	4 & r_4 \\
	\end{bmatrix}
	$$
	
	$$
	R(1) = 
	\begin{bmatrix}
	Repair & Reward \\
	1 & r'_1 \\
	2 & r'_2 \\
	3 & r'_3 \\
	4 & r'_4 \\
	\end{bmatrix}
	$$
	
	The overarching goal is to use reinforcement learning methods with POMDPs to give an optimal strategy for repairing the system that minimizes the total repair costs and losses due to non-repair.
	
	The two routes are as follows:
	\begin{itemize}
		\item 1. EM based approach for estimation and then determining an optimal control policy using any POMDP solver. For example, we could use a VAE (variational autoencoder) for time series. This doesn't incorporate control, maximizing a likelihood function, and outputs a particle filter model.
		\item 2. DVRL (deep variational reinforcement learning), which combines both estimation and control. This is because it's optimizing an objective cost function that also depends on model parameters.
	\end{itemize}
	
	We also incorporate knowledge of the start and end states being in perfect and poor condition respectively, which we discuss in the next section.
	
	\section*{Current Status 1}
	
	So far, we have studied four different POMDP based techniques for modeling the preventive maintenance problem. We implemented value-iteration, Monte-Carlo Tree Search (POMCP), ADRQN, and DVRL on the CMAPSS dataset. After finishing up these implementations, we are currently analyzing how well deep-RL methods such as DVRL and ADRQN performed versus model-based methods. We also incorporate constraints on the start and end states by a method similar to boosting, where we propagate the errors between our true start and end states and our estimates, re-running DVRL until we reach convergence. The next steps are analyzing how two sources of information (prior knowledge with observations) should be combined to determine optimal repair strategies.
	
	In practice, we often have some existing knowledge about the system process before seeing measurements. This leads us to the second research path.
	
	\subsection*{Path 2: Combining Multiple Sources}
	
	Here, we ask how to combine prior knowledge about the data generating process when determining the optimal repair strategy that minimizes costs. For example, if we have a controlled system generating data, this alters the observed machine measurements as opposed to an uncontrolled system. This is because if the machine is deteriorating, we repair and restore the system back to full health so our measurements seen will be different than if we let the machine fail.
	
	The alternative route would be to identify enough of the distribution based on impact on the objective function and disambiguate. 
	\\
	\\
	For now, let us assume the data generating process takes on one of the following forms:
	\begin{itemize}
		\item No prior distribution -- our data is generated via bootstrapping. For example, with the C-MAPPS data, we could resample from the trajectories.
		\item One prior
		\begin{itemize}
			\item Do we trust this prior distribution? Or should we ignore that and just bootstrap? Or just combine both?
		\end{itemize}
		\item We have two prior distributions and we would like to understand which one generates our data, or how they interact with each other. For example, we can combine two priors via a "weighted average" (partial update) or fully via a Bayesian update.
	\end{itemize}
	
	These three are the possible ways of combining the two models, which we are investigating:
	\begin{itemize}
		\item One generating model at each timestep
		\item Mixture model
		\item Hierarchical Bayes
	\end{itemize}
	
	Analogously to the single source model, we have another layer of complexity when studying estimation versus control here. Either we could
	\begin{itemize}
		\item Estimate underlying model and then come up with a control policy using POMDP-RL.
		\item Perform control and estimation at once using a method such as DVRL again.
	\end{itemize}
	
	This section gives one a glimpse of some of the open problems we are working on.
	
	\subsection*{Sequential Testing}
	The paper \url{http://stat.columbia.edu/~jcliu/paper/GSPRT_SQA3.pdf} introduces Sequential Probability Ratio Testing (SPRT) for different families of hypotheses.
	
	When we have two families of composite hypotheses, i.e.,
	$$H_0: f\in \{g_{\theta}:\theta\in \Theta\} \textrm{ against } H_A:f\in \{h_{\gamma}:\gamma\in \Gamma\}$$, we consider a sequential test where we compute $$L_n = \frac{\sup_{\gamma\in \Gamma} \prod_{i=1}^n h_{\gamma}(X_i)}{\sup_{\theta\in \Theta} \prod_{i=1}^n g_{\theta}(X_i)}.$$ If $L_n$ crosses $e^A$ or $e^{-B}$ for constants $A,B>0$, then we stop sampling. If $L_n>e^A$, then we reject the null.
	
	We could use this to infer which distribution is generating our data, but the research component is understanding how to use this with particle filters. If we use a variational autoencoder to learn an underlying model, the output is a particle filter. Currently there hasn't been work on how to use SPRT with particle filters.
	\subsection*{Example: Mixture Model Setting}
	What we would like to solve is initially the following:
	If our data is generated from $\epsilon_t P_0 + (1-\epsilon_t) P_1$, where $P_0$ and $P_1$ are the two priors, does $\epsilon_t\to 0$ or to a constant over time? Maybe $P_0$ is one prior generated from bootstrapping and $P_1$ is the second prior provided or based on data that incorporates domain knowledge.
	
	In this setting, we can think of our $z_t=\{x_t,y_t\}$ pairs coming from data sources $P_0$ and $P_1$.
	
	Define $p(z_t|P_0)$ and $p(z_t|P_1)$ to be the probabilities of observing $z_t$ given $P_0$ and $P_1$ respectively at time $t$. Then $$p(P_j|z_t)=\frac{\epsilon_t p(z_t|P_0) I(j=0) + (1-\epsilon_t) p(z_t|P_1) I(j=1)}{\epsilon_t p(z_t|P_0) + (1-\epsilon_t) p(z_t|P_1)}.$$
	
	But now, we want to understand
	$$p(\{P_{j_1},\ldots,P_{j_T}\}|\{z_1,\ldots,z_T\})$$ where each $P_{jt}$ is either $P_0$ or $P_1$.
	
	The simplistic approach to estimate $\epsilon$ would be to do a maximization of the above quantity over all assignments of $\{P_{j1},\ldots,P_{jT}\}$ to the set $\{P_0,P_1\}^T$. This method, however, is computationally intensive and furthermore does not account for time-dependence in our data.
	
	We propose modeling this as a multiarmed bandit problem, where $P_0$ and $P_1$ are the two arms. The goal here is to then use concentration inequalities to understand good approximate repair policies. These will be approximate because there is a dependency between actions taken at different timesteps and we are looking at finite-horizon solutions.
	
	%We propose a method which uses RL to optimize an objective function which implicitly tells us the proportion of time data comes from $P_0$ vs $P_1$. At time $t$, after we observe $z_1,\ldots,z_t$, we can do one of three actions. Either we choose $P_0$ or $P_1$ as our hypothesis of the prior, or we continue exploration, which incurs a cost $C$. If we choose incorrectly between $P_0$ and $P_1$, we incur a cost $L$ and if we were correct, we don't incur any cost.
	%
	%This is now a three action POMDP where the observations are the $z_i$'s and the underlying states are $P_0$ and $P_1$.
	%
	%Our observation distributions are given by the pdfs $O_s(z)$, which can be one of $O_0(z)$ or $O_1(z)$, depending on whether the underlying data comes from $P_0$ or $P_1$ at that time.
	%
	%State transitions, however, require a little more thought. For now, the transitions between our two states is just a 2x2 matrix
	%
	%$$
	%\begin{bmatrix}
	%p_{00} & p_{01} \\ 
	%p_{10} & p_{11} 
	%\end{bmatrix}
	%$$
	%
	%For our cost function, define this again to be $R_s(a)=C*I(a=2)+L*I(a<2,s\neq a)$ where $s\in \{0,1\}$ represents the underlying distribution and $a\in \{0,1,2\}$ represents choosing $P_0$, $P_1$, or neither. So if $a=1$, we chose $P_1$. One way of dealing with the time-dependent structure here is by incorporating a discount factor $\gamma$ that weighs the most recent reward more.
	
	%In this setting, we cannot use a model-based approach since we don't have knowledge about our state transitions. But with a model-free approach such as deep variational reinforcement learning \url{https://arxiv.org/pdf/1806.02426.pdf}, we can infer the proportion of time data comes from each generating process. One potential route to take here is to use a generalized sequential probability ratio test for separate families of hypotheses \url{http://stat.columbia.edu/~jcliu/paper/GSPRT_SQA3.pdf} to better understand how the distributions are being combined. Another possible approach is to use a hierarchical Bayesian model to relate $P(z_t|P_0)$ and $P(z_t|P_1)$ and update the previously mentioned weighted mixture model.
	
	\section*{Current Status 2}
	
	For this, the first priority is studying how we can combine particle filters (outputs of VAE) can be used with SPRT. Other open problems that will be tackled after this are how to get guarantees on learning multiple source distributions using concentration-type inequalities.
	%
	%\section*{Appendix: End Of Life Analysis Background}
	%The goal of the preliminary analysis is to create a Bayesian probabilistic survival model, using incomplete data sets in a general way.  The goal of this section is to define the model and methodology used and to outline options and identify risk areas.
	%
	%Let T be the continuous nonnegative random variable representation the end of life time of a machine in some population.  Let f(t) be the pdf of T and let the distribution function be
	%\begin{equation}
	%F(t) = P( t \le t) = \int_0^t f(u) du
	%\end{equation}
	%The probability of a machine functioning until time t is given by the function
	%\begin{equation}
	%S(t) = 1 - F(t)=P(T>t)
	%\end{equation}
	%where S(0)=1 and $S(\infty)=\lim_{t \rightarrow \infty} S(t)=0$.  The hazard function h(t) is defined as the instantaneous rate of failure at time t
	%\begin{equation}
	%h(t)=\frac{f(t)}{S(t)}
	%\end{equation}
	%The interpretation is that $h(t)\Delta t$ is the approximate probability of failure in $(t, t+\Delta t)$.  It is easy to show that
	%\begin{equation}
	%h(t)=-\frac{d}{dt}log(S(t))
	%\end{equation}
	%Integrating both sides and exponentiating we get
	%\begin{equation}
	%S(t)=exp(-\int_0^t h(u) du)
	%\end{equation}
	%The hazard function H(t) and the survivor function S(t) are related by
	%\begin{equation}
	%S(t)=exp(-H(t))
	%\end{equation}
	%and the hazard function h(t) has the properties
	%\begin{equation}
	%h(t) \ge 0 \quad  \text{and} \quad  \int_0^\infty h(t)dt = \infty
	%\end{equation}
	%Finally the survival pdf is given by
	%\begin{equation}
	%f(t)=h(t)exp(-\int_0^t h(u) du)
	%\end{equation}
	%\subsection*{The Weibull distribution}
	%The Weibull distribution is one of the most commonly used distributions to model survival pdf, and is the distribution we will use to model survival pdf
	%\begin{equation}
	%f(t)=\begin{cases}
	%\alpha \gamma t^{\alpha -1}exp(-\gamma t^{\alpha}) \quad t>0, \alpha >0, \gamma>0\\
	%0 \quad \text{otherwise}
	%\end{cases}
	%\end{equation}
	%The Weibull distribution is denoted by $\mathcal{W}(\alpha, \gamma)$ where $\alpha$ and $\gamma$ are the parameters of the distribution.  For this distribution the hazard distribution h(t) is monotonically increasing when $\alpha >1$ and monotonically decreasing when $0 \alpha < 1$.  It follows that
	%the survivor function is 
	%\begin{equation} \label{eq:W}
	%S(t)=exp(-\gamma t^\alpha)
	%\end{equation}
	%and the hazard function is
	%\begin{equation}
	%h(t)=\gamma \alpha t^{\alpha -1}
	%\end{equation}
	%and the cumulative hazard function H(t) is given by
	%\begin{equation}
	%H(t)=\gamma t ^\alpha
	%\end{equation}
	%\subsection*{Proportional Hazard Model}
	%The hazard function depends on both time and a set of covariates.  The proportional hazards model separates these components by by specifying that the hazard at time t, for a machine whose covariate vector is $\mathbf{x}$ is given by the linear relatioship
	%\begin{equation}
	%h(t|\mathbf{x})=h_0(t) exp(\mathbf{x}' \mathbf{\beta})
	%\end{equation}
	%where $\mathbf{\beta}$ is a vector of regression coefficients.
	%\subsection*{Censoring}
	%We expect the end of life sensor data is right censored, that is, the survival times are only known for a portion of machines under study.  The likelihood function for righ censored data will be constructed as follows.  Suppose there are n machines and associated with the $i^{th}$ individual is a survival time $t_i$ and a fixed censoring time $c_i$.  The $t_o$ are assumed to be independent and identically distributed with density f(t) and survival function S(t).  The exact survival time for machine i, $t_i$, will be observed only if $t_i \le c_i$,  The data can be represented as the n pairs of random variables $(y_i, \nu_i)$ where
	%\begin{equation}
	%y_i=\min(t_i, c_i)
	%\end{equation}
	%and
	%\begin{equation}
	%\nu_i=\begin{cases}
	%1 \quad \text{if} \quad t_i \le c_i,\\
	%0 \quad \text{if} \quad t_i > c_i
	%\end{cases}
	%\end{equation}
	%Then the likelihood function for ($\mathbf{\beta}, h_0(.)$) for a set of right censored data is given by
	%\begin{equation}
	%\begin{split}
	%L(\mathbf{\beta}, h_0(t) | D) \propto \prod_{i=1}^n [h_o(y_i) exp(\eta_i)]^{\nu_i} (S_0(y_i)^{exp(\eta_i)})\\
	%\prod_{i=1}^n [h_o(y_i) exp(\eta_i)]^{\nu_i} exp \left \{ 
	%-\sum_{i=1}^n exp(\eta_i)H_0(y_i) \right \}
	%\end{split}
	%\end{equation}
	%where $D=(n,\mathbf{y}, X, \mathbf{\nu})$, $\mathbf{y}=(y_1, y_2,...,y_n)'$ and $\mathbf{\nu}=(\nu_1, \nu_2, ...,\nu_n)$, $\eta_i=\mathbf{x}_i' \mathbf{\beta}$ is the linear predictor of machine i, and
	%\begin{equation}
	%S_0(t)=exp(-\int_0^t h_0(u)du)=exp(-H_0(t))
	%\end{equation}
	%is the baseline survivor function.
	
\end{document}