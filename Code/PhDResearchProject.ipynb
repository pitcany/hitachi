{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import norm\n",
    "import pomegranate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data and diving them into unique unit numbers\n",
    "\n",
    "We need to divide data into unique numbers, because the state restes as the unit number changes, so we need to find Gaussian distribution for different unit numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('~/Documents/hitachi/CMAPSS/train_FD001.txt', sep=\" \", header=None)\n",
    "unique_unit_values = data[0].unique() #Number of units\n",
    "data_cycles = []\n",
    "for unit_num in unique_unit_values:\n",
    "    data_cycles.append(data[data[0] == unit_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing operational settings and normalize the data column wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    x = data.values\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    dataNew = pd.DataFrame(x_scaled)\n",
    "    return dataNew\n",
    "#Remove the operation settings\n",
    "dataT = data[data.columns[5:26]]\n",
    "dataT.columns = range(21)\n",
    "dataT = normalize(dataT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividing data for each unit\n",
    "\n",
    "I think this is why my transitional matrix previously was not working properly as in each unit the state resets and start from good condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataT_cycles = []\n",
    "for unit_num in unique_unit_values:\n",
    "    dataT_cycles.append(dataT[data[0] == unit_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying and removing non variable data columns\n",
    "\n",
    "Removing the columns where the data does not vary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "for dataT_cycle in dataT_cycles:\n",
    "    print(dataT_cycle.columns[dataT_cycle.std() == 0])\n",
    "\"\"\"\n",
    "Here we can see 0,4,9,15,17,18 but also 5 at many places so we drop column number 5 as well\n",
    "\"\"\"\n",
    "dataT.drop(data.columns[[0, 3, 4, 5, 9, 15, 17, 18]],axis=1,inplace=True)\n",
    "dataT.columns = range(13)\n",
    "dataT_cycles = []\n",
    "for unit_num in unique_unit_values:\n",
    "    dataT_cycles.append(dataT[data[0] == unit_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Right now only using the first data frame (i.e Machine 1) to train the VAE, but we can combine all the dataframes\n",
    "# and train the VAE jointly on the entire data for better performance \n",
    "\n",
    "x_train = dataT_cycles[0].values[:150]\n",
    "x_test = dataT_cycles[0].values[151:198]\n",
    "x_train.shape\n",
    "# x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational AutoEncoders to find Latent State Space Distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-e2e8ac5a5bbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "x_train = dataT_cycles[0].values[:100]\n",
    "x_test = dataT_cycles[0].values[101:198]\n",
    "x_train.shape\n",
    "x_test.shape\n",
    "original_dim = x_train[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "input_shape = (original_dim, )\n",
    "intermediate_dim = 9\n",
    "batch_size = 10\n",
    "latent_dim = 5\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# z = z_mean + sqrt(var)*eps\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    \n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 9)            126         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 5)            50          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 5)            50          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 5)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 226\n",
      "Trainable params: 226\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# VAE Model Encoder + Decoder \n",
    "\n",
    "# Building the Encoder\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 54        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 13)                130       \n",
      "=================================================================\n",
      "Total params: 184\n",
      "Trainable params: 184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build decoder model \n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    help_ = \"Load h5 model trained weights\"\n",
    "    parser.add_argument(\"-w\", \"--weights\", help=help_)\n",
    "    help_ = \"Use mse loss instead of binary cross entropy (default)\"\n",
    "    parser.add_argument(\"-m\", \"--mse\", help=help_, action='store_true')\n",
    "    \n",
    "    models = (encoder, decoder)\n",
    "    data = (x_test, None)\n",
    "    \n",
    "    # VAE loss = mse_loss or xent_loss + kl_loss\n",
    "    if args.mse:\n",
    "        reconstruction_loss = mse(inputs, outputs)\n",
    "    else:\n",
    "        reconstruction_loss = binary_crossentropy(inputs, outputs)\n",
    "        \n",
    "    reconstruction_loss *= original_dim\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis= -1)\n",
    "    kl_loss *= 0.5 \n",
    "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "    vae.add_loss(vae_loss)\n",
    "    vae.compile(optimizer='adam')\n",
    "    vae.summary()\n",
    "    \n",
    "    if args.weights:\n",
    "        vae.load_weights(args.weights)\n",
    "    else:\n",
    "        # Train the autoencoder\n",
    "        vae.fit(x_train, epochs=epochs, batch_size= batch_size, validation_data=(x_test, None))\n",
    "        vae.save_weights('vae_mlp_CMAPSS.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yannik/miniconda3/envs/ykp/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"vae_mlp\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 13)                0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 5), (None, 5), (N 226       \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 13)                184       \n",
      "=================================================================\n",
      "Total params: 410\n",
      "Trainable params: 410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/miniconda3/envs/ykp/lib/python3.7/site-packages/keras/engine/training_utils.py:819: UserWarning: Output decoder missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to decoder.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yannik/miniconda3/envs/ykp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 100 samples, validate on 91 samples\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 8.6136 - val_loss: 7.5069\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 0s 159us/step - loss: 8.1071 - val_loss: 6.7097\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 0s 176us/step - loss: 7.4209 - val_loss: 5.7314\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 0s 188us/step - loss: 6.5338 - val_loss: 4.2211\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 0s 188us/step - loss: 5.2482 - val_loss: 2.3457\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 0s 222us/step - loss: 3.4696 - val_loss: -0.6214\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 0s 234us/step - loss: 0.8200 - val_loss: -4.6524\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 0s 233us/step - loss: -3.1063 - val_loss: -10.8377\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 0s 199us/step - loss: -8.8391 - val_loss: -20.2851\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 0s 221us/step - loss: -17.8439 - val_loss: -35.3720\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 0s 212us/step - loss: -32.3196 - val_loss: -61.8668\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 0s 189us/step - loss: -57.6569 - val_loss: -110.5614\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 0s 192us/step - loss: -103.0687 - val_loss: -209.2532\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 0s 172us/step - loss: -193.8839 - val_loss: -428.7604\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 0s 166us/step - loss: -395.5606 - val_loss: -973.5886\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 0s 171us/step - loss: -876.5469 - val_loss: -2465.3963\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 0s 164us/step - loss: -2140.9943 - val_loss: -6978.1396\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 0s 191us/step - loss: -5912.4380 - val_loss: -21969.4612\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 0s 168us/step - loss: -17524.6221 - val_loss: -76693.2080\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 0s 192us/step - loss: -58330.1199 - val_loss: -299755.0117\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 0s 175us/step - loss: -211254.3961 - val_loss: -1294789.7390\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 0s 170us/step - loss: -824078.1781 - val_loss: -6254041.3846\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 0s 189us/step - loss: -3874470.8000 - val_loss: -33839958.4615\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 0s 179us/step - loss: -17673889.2000 - val_loss: -203298941.5385\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 0s 187us/step - loss: -93301684.8000 - val_loss: -1389514401.7582\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 0s 174us/step - loss: -575689105.6000 - val_loss: -10388783064.6154\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 0s 190us/step - loss: -3859205760.0000 - val_loss: -91073077878.1538\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 0s 195us/step - loss: -27723500134.4000 - val_loss: -887367626166.8572\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 0s 191us/step - loss: -211455042355.2000 - val_loss: -9740658291363.1641\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 0s 226us/step - loss: -1892797120512.0000 - val_loss: -123805296555919.4688\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 0s 212us/step - loss: -17698189370982.3984 - val_loss: -1668533313927055.5000\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 0s 233us/step - loss: -171281844115865.5938 - val_loss: -24902474047814240.0000\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 0s 221us/step - loss: -2271257136254157.0000 - val_loss: -430710419582321728.0000\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 0s 214us/step - loss: -30693065758448024.0000 - val_loss: -8158398050467080192.0000\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 0s 194us/step - loss: -391592129236828160.0000 - val_loss: -168394750595001450496.0000\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 0s 213us/step - loss: nan - val_loss: nan\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 0s 204us/step - loss: nan - val_loss: nan\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 0s 200us/step - loss: nan - val_loss: nan\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 0s 225us/step - loss: nan - val_loss: nan\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 0s 208us/step - loss: nan - val_loss: nan\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 0s 176us/step - loss: nan - val_loss: nan\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 0s 190us/step - loss: nan - val_loss: nan\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 0s 177us/step - loss: nan - val_loss: nan\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 0s 184us/step - loss: nan - val_loss: nan\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 0s 185us/step - loss: nan - val_loss: nan\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 0s 187us/step - loss: nan - val_loss: nan\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 0s 188us/step - loss: nan - val_loss: nan\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 0s 205us/step - loss: nan - val_loss: nan\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 0s 177us/step - loss: nan - val_loss: nan\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 0s 163us/step - loss: nan - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    mse = None\n",
    "    weights = None\n",
    "    \n",
    "args = Args()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the VAE has been trained, we can use the encoder to sample the latent space\n",
    "\n",
    "#predicting the latent space for first 13 observation values for machine 1\n",
    "test = np.asarray(x_train[0:13])  \n",
    "\n",
    "# indexing on 2 because the encoder predicts z_mean, z_log_var and sampled vector z (we are interested in z only)\n",
    "latent_space = encoder.predict(test)[2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each list is a 5 dimension latent state space for that observation value\n",
    "latent_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the raw observation from the learned latent space \n",
    "sample = decoder.predict(latent_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18373494, 0.40680183, 0.72624799, 0.24242424, 0.109755  ,\n",
       "       0.36904762, 0.63326226, 0.20588235, 0.1996078 , 0.36398615,\n",
       "       0.33333333, 0.71317829, 0.7246617 ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the with the real x_train value\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Right now they are not same as we trained the VAE on very less amount of data \n",
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using HMM to find out transitional matrices\n",
    "\n",
    "Here we first define transmatrix as [[0.5, 0.5, 0.0, 0.0],[0.0, 0.4, 0.6, 0.0],[0.0, 0.0, 0.3, 0.7],[0.0,0.0,0.0,1.0]] which means there is half chance for each state to go to next state and half to remain in the current state itself when fully healthy, 0.4 probability of remaining at current state when at 'above average' health, 0.3 probability when 'below average', and with certainty to stay at a failing state if already failing.\n",
    "\n",
    "Then we train for each unit for transmatrix as well as state means and we will take average of each unit transmatrices and states as the transmatrix and state\n",
    "\n",
    "*Note*: Here state '0' means the perfect health and '3' means weakest health "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.183735</td>\n",
       "      <td>0.406802</td>\n",
       "      <td>0.726248</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.109755</td>\n",
       "      <td>0.369048</td>\n",
       "      <td>0.633262</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.199608</td>\n",
       "      <td>0.363986</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.713178</td>\n",
       "      <td>0.724662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.283133</td>\n",
       "      <td>0.453019</td>\n",
       "      <td>0.628019</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.100242</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.765458</td>\n",
       "      <td>0.279412</td>\n",
       "      <td>0.162813</td>\n",
       "      <td>0.411312</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.731014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.343373</td>\n",
       "      <td>0.369523</td>\n",
       "      <td>0.710145</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.140043</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.795309</td>\n",
       "      <td>0.220588</td>\n",
       "      <td>0.171793</td>\n",
       "      <td>0.357445</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.621375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.343373</td>\n",
       "      <td>0.256159</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.124518</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.889126</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.174889</td>\n",
       "      <td>0.166603</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.573643</td>\n",
       "      <td>0.662386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.349398</td>\n",
       "      <td>0.257467</td>\n",
       "      <td>0.668277</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.149960</td>\n",
       "      <td>0.255952</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.174734</td>\n",
       "      <td>0.402078</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.589147</td>\n",
       "      <td>0.704502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.765060</td>\n",
       "      <td>0.683235</td>\n",
       "      <td>0.336554</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>0.072602</td>\n",
       "      <td>0.684524</td>\n",
       "      <td>0.234542</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.091599</td>\n",
       "      <td>0.753367</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.286822</td>\n",
       "      <td>0.089202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.894578</td>\n",
       "      <td>0.547853</td>\n",
       "      <td>0.136876</td>\n",
       "      <td>0.560606</td>\n",
       "      <td>0.102396</td>\n",
       "      <td>0.732143</td>\n",
       "      <td>0.189765</td>\n",
       "      <td>0.661765</td>\n",
       "      <td>0.090670</td>\n",
       "      <td>0.744132</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.263566</td>\n",
       "      <td>0.301712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.731928</td>\n",
       "      <td>0.614345</td>\n",
       "      <td>0.231884</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.084582</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.287846</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>0.065229</td>\n",
       "      <td>0.759523</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.271318</td>\n",
       "      <td>0.239299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.641566</td>\n",
       "      <td>0.682799</td>\n",
       "      <td>0.172303</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.094364</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>0.187633</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.075704</td>\n",
       "      <td>0.740669</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.240310</td>\n",
       "      <td>0.324910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.701807</td>\n",
       "      <td>0.662089</td>\n",
       "      <td>0.225443</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.051557</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.296375</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.056714</td>\n",
       "      <td>0.717199</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.263566</td>\n",
       "      <td>0.097625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.183735  0.406802  0.726248  0.242424  0.109755  0.369048  0.633262   \n",
       "1    0.283133  0.453019  0.628019  0.212121  0.100242  0.380952  0.765458   \n",
       "2    0.343373  0.369523  0.710145  0.272727  0.140043  0.250000  0.795309   \n",
       "3    0.343373  0.256159  0.740741  0.318182  0.124518  0.166667  0.889126   \n",
       "4    0.349398  0.257467  0.668277  0.242424  0.149960  0.255952  0.746269   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "187  0.765060  0.683235  0.336554  0.621212  0.072602  0.684524  0.234542   \n",
       "188  0.894578  0.547853  0.136876  0.560606  0.102396  0.732143  0.189765   \n",
       "189  0.731928  0.614345  0.231884  0.590909  0.084582  0.880952  0.287846   \n",
       "190  0.641566  0.682799  0.172303  0.575758  0.094364  0.773810  0.187633   \n",
       "191  0.701807  0.662089  0.225443  0.636364  0.051557  0.833333  0.296375   \n",
       "\n",
       "            7         8         9        10        11        12  \n",
       "0    0.205882  0.199608  0.363986  0.333333  0.713178  0.724662  \n",
       "1    0.279412  0.162813  0.411312  0.333333  0.666667  0.731014  \n",
       "2    0.220588  0.171793  0.357445  0.166667  0.627907  0.621375  \n",
       "3    0.294118  0.174889  0.166603  0.333333  0.573643  0.662386  \n",
       "4    0.235294  0.174734  0.402078  0.416667  0.589147  0.704502  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "187  0.514706  0.091599  0.753367  0.666667  0.286822  0.089202  \n",
       "188  0.661765  0.090670  0.744132  0.583333  0.263566  0.301712  \n",
       "189  0.691176  0.065229  0.759523  0.833333  0.271318  0.239299  \n",
       "190  0.617647  0.075704  0.740669  0.500000  0.240310  0.324910  \n",
       "191  0.647059  0.056714  0.717199  0.666667  0.263566  0.097625  \n",
       "\n",
       "[192 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataT_cycles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\",init_params=\"cm\", params=\"mtc\")\n",
    "lr.startprob_ = np.array([1.0, 0.0, 0.0, 0.0])\n",
    "transmats = []\n",
    "statemeans = []\n",
    "covars = []\n",
    "for i in range(100):\n",
    "    lr.transmat_ = np.array([[0.5, 0.5, 0.0, 0.0],[0.0, 0.4, 0.6, 0.0],[0.0, 0.0, 0.3, 0.7],[0.0,0.0,0.0,1.0]])\n",
    "    lr.fit(dataT_cycles[i])\n",
    "    transmats.append(lr.transmat_)\n",
    "    statemeans.append(lr.means_)\n",
    "    covars.append(lr.covars_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr = hmm.GMMHMM(n_components=4, n_mix=4, covariance_type=\"diag\",init_params=\"cm\", params=\"mt\")\n",
    "#lr.startprob_ = np.array([1.0, 0.0, 0.0, 0.0])\n",
    "#transmats = []\n",
    "#statemeans = []\n",
    "#for i in range(100):\n",
    "#    lr.transmat_ = np.array([[0.5, 0.5, 0.0, 0.0],[0.0, 0.5, 0.5, 0.0],[0.0, 0.0, 0.5, 0.5],[0.0,0.0,0.0,1.0]])\n",
    "#    lr.fit(dataT_cycles[i])\n",
    "#    transmat = lr.transmat_\n",
    "#    transmats.append(transmat)\n",
    "#    statemeans.append(lr.means_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transmat = np.array(transmats).mean(axis=0)\n",
    "statemean = np.array(statemeans).mean(axis=0)\n",
    "covar = np.array(covars).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61014794, 0.38985206, 0.        , 0.        ],\n",
       "       [0.        , 0.70767557, 0.29232443, 0.        ],\n",
       "       [0.        , 0.        , 0.81637529, 0.18362471],\n",
       "       [0.        , 0.        , 0.        , 1.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.610148</td>\n",
       "      <td>0.389852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707676</td>\n",
       "      <td>0.292324</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.816375</td>\n",
       "      <td>0.183625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3\n",
       "0  0.610148  0.389852  0.000000  0.000000\n",
       "1  0.000000  0.707676  0.292324  0.000000\n",
       "2  0.000000  0.000000  0.816375  0.183625\n",
       "3  0.000000  0.000000  0.000000  1.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(transmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.368642</td>\n",
       "      <td>0.360443</td>\n",
       "      <td>0.646729</td>\n",
       "      <td>0.251575</td>\n",
       "      <td>0.159847</td>\n",
       "      <td>0.310608</td>\n",
       "      <td>0.667523</td>\n",
       "      <td>0.271062</td>\n",
       "      <td>0.198058</td>\n",
       "      <td>0.375780</td>\n",
       "      <td>0.360988</td>\n",
       "      <td>0.596526</td>\n",
       "      <td>0.632670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.441376</td>\n",
       "      <td>0.436427</td>\n",
       "      <td>0.571924</td>\n",
       "      <td>0.292181</td>\n",
       "      <td>0.197164</td>\n",
       "      <td>0.406954</td>\n",
       "      <td>0.577873</td>\n",
       "      <td>0.312144</td>\n",
       "      <td>0.224662</td>\n",
       "      <td>0.454334</td>\n",
       "      <td>0.427735</td>\n",
       "      <td>0.524750</td>\n",
       "      <td>0.546944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.536103</td>\n",
       "      <td>0.506848</td>\n",
       "      <td>0.468405</td>\n",
       "      <td>0.360751</td>\n",
       "      <td>0.242245</td>\n",
       "      <td>0.527591</td>\n",
       "      <td>0.473883</td>\n",
       "      <td>0.377540</td>\n",
       "      <td>0.265704</td>\n",
       "      <td>0.544239</td>\n",
       "      <td>0.515299</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.449659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.676722</td>\n",
       "      <td>0.626082</td>\n",
       "      <td>0.331501</td>\n",
       "      <td>0.463127</td>\n",
       "      <td>0.309536</td>\n",
       "      <td>0.689428</td>\n",
       "      <td>0.314234</td>\n",
       "      <td>0.478832</td>\n",
       "      <td>0.319420</td>\n",
       "      <td>0.689047</td>\n",
       "      <td>0.636300</td>\n",
       "      <td>0.301774</td>\n",
       "      <td>0.305067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.368642  0.360443  0.646729  0.251575  0.159847  0.310608  0.667523   \n",
       "1  0.441376  0.436427  0.571924  0.292181  0.197164  0.406954  0.577873   \n",
       "2  0.536103  0.506848  0.468405  0.360751  0.242245  0.527591  0.473883   \n",
       "3  0.676722  0.626082  0.331501  0.463127  0.309536  0.689428  0.314234   \n",
       "\n",
       "          7         8         9        10        11        12  \n",
       "0  0.271062  0.198058  0.375780  0.360988  0.596526  0.632670  \n",
       "1  0.312144  0.224662  0.454334  0.427735  0.524750  0.546944  \n",
       "2  0.377540  0.265704  0.544239  0.515299  0.437500  0.449659  \n",
       "3  0.478832  0.319420  0.689047  0.636300  0.301774  0.305067  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(statemean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_prob = np.array([transmat, transmat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array([[100, 50, 0, -50],[-50, 0, 50, 100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>-50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-50</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1   2    3\n",
       "0  100  50   0  -50\n",
       "1  -50   0  50  100"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[9.31506823e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 8.86169403e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 6.89448238e-03, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.25110531e-03,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         4.02229235e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 6.74538921e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 6.93002407e-03, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.19656647e-03,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         3.95696710e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 7.99366317e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 7.98425060e-03, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 8.06436711e-03,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         8.60416624e-03]],\n",
       "\n",
       "       [[1.04104987e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 9.19411937e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 7.42036549e-03, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.76732805e-03,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         3.65594506e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 7.50265346e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 7.71002973e-03, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.75717576e-03,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         3.48953133e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 8.34252879e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 8.28900332e-03, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 8.10472413e-03,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         9.27372458e-03]],\n",
       "\n",
       "       [[1.50009345e+02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 1.50008571e+02, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 1.50006351e+02, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.50003774e+02,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.50002989e+02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 1.50007243e+02, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 1.50007201e+02, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.50003679e+02,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.50002786e+02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 1.50007720e+02, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 1.50007567e+02, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.50007866e+02,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.50008313e+02]],\n",
       "\n",
       "       [[5.50005307e+02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 5.50004339e+02, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 5.50003528e+02, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.50002362e+02,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         5.50001631e+02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 5.50004214e+02, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 5.50004011e+02, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.50002330e+02,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         5.50001483e+02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 5.50004401e+02, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 5.50004158e+02, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.50004673e+02,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         5.50004841e+02]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "covar=[covar[i].sum(axis=1) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.009315</td>\n",
       "      <td>0.008862</td>\n",
       "      <td>0.006894</td>\n",
       "      <td>0.005251</td>\n",
       "      <td>0.004022</td>\n",
       "      <td>0.006745</td>\n",
       "      <td>0.006930</td>\n",
       "      <td>0.005197</td>\n",
       "      <td>0.003957</td>\n",
       "      <td>0.007994</td>\n",
       "      <td>0.007984</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>0.008604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.010410</td>\n",
       "      <td>0.009194</td>\n",
       "      <td>0.007420</td>\n",
       "      <td>0.004767</td>\n",
       "      <td>0.003656</td>\n",
       "      <td>0.007503</td>\n",
       "      <td>0.007710</td>\n",
       "      <td>0.004757</td>\n",
       "      <td>0.003490</td>\n",
       "      <td>0.008343</td>\n",
       "      <td>0.008289</td>\n",
       "      <td>0.008105</td>\n",
       "      <td>0.009274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>150.009345</td>\n",
       "      <td>150.008571</td>\n",
       "      <td>150.006351</td>\n",
       "      <td>150.003774</td>\n",
       "      <td>150.002989</td>\n",
       "      <td>150.007243</td>\n",
       "      <td>150.007201</td>\n",
       "      <td>150.003679</td>\n",
       "      <td>150.002786</td>\n",
       "      <td>150.007720</td>\n",
       "      <td>150.007567</td>\n",
       "      <td>150.007866</td>\n",
       "      <td>150.008313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>550.005307</td>\n",
       "      <td>550.004339</td>\n",
       "      <td>550.003528</td>\n",
       "      <td>550.002362</td>\n",
       "      <td>550.001631</td>\n",
       "      <td>550.004214</td>\n",
       "      <td>550.004011</td>\n",
       "      <td>550.002330</td>\n",
       "      <td>550.001483</td>\n",
       "      <td>550.004401</td>\n",
       "      <td>550.004158</td>\n",
       "      <td>550.004673</td>\n",
       "      <td>550.004841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0           1           2           3           4           5  \\\n",
       "0    0.009315    0.008862    0.006894    0.005251    0.004022    0.006745   \n",
       "1    0.010410    0.009194    0.007420    0.004767    0.003656    0.007503   \n",
       "2  150.009345  150.008571  150.006351  150.003774  150.002989  150.007243   \n",
       "3  550.005307  550.004339  550.003528  550.002362  550.001631  550.004214   \n",
       "\n",
       "            6           7           8           9          10          11  \\\n",
       "0    0.006930    0.005197    0.003957    0.007994    0.007984    0.008064   \n",
       "1    0.007710    0.004757    0.003490    0.008343    0.008289    0.008105   \n",
       "2  150.007201  150.003679  150.002786  150.007720  150.007567  150.007866   \n",
       "3  550.004011  550.002330  550.001483  550.004401  550.004158  550.004673   \n",
       "\n",
       "           12  \n",
       "0    0.008604  \n",
       "1    0.009274  \n",
       "2  150.008313  \n",
       "3  550.004841  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(covar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_prob = np.array([norm.pdf(statemean,covar), norm.pdf(statemean,covar)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.37327764, 0.37573823, 0.3267665 , 0.38713115, 0.39415941,\n",
       "         0.38002938, 0.32109851, 0.38488808, 0.391417  , 0.37296664,\n",
       "         0.37434949, 0.33556263, 0.33045909],\n",
       "        [0.3605219 , 0.36319214, 0.34567815, 0.38097265, 0.39161874,\n",
       "         0.36461511, 0.34293784, 0.37833576, 0.38937535, 0.35789756,\n",
       "         0.36178362, 0.35346976, 0.34844858],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.37327764, 0.37573823, 0.3267665 , 0.38713115, 0.39415941,\n",
       "         0.38002938, 0.32109851, 0.38488808, 0.391417  , 0.37296664,\n",
       "         0.37434949, 0.33556263, 0.33045909],\n",
       "        [0.3605219 , 0.36319214, 0.34567815, 0.38097265, 0.39161874,\n",
       "         0.36461511, 0.34293784, 0.37833576, 0.38937535, 0.35789756,\n",
       "         0.36178362, 0.35346976, 0.34844858],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: no-repair, 1: repair\n",
    "actions = ('0', '1')\n",
    "# 0: failing, 1: low health, 2: good health, 3: perfect health\n",
    "states = ('0', '1', '2', '3')\n",
    "\n",
    "discount = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First we define an MDP. We also represent a policy\n",
    "as a dictionary of {state: action} pairs, and a utility function as a\n",
    "dictionary of {state: number} pairs. We then define the value_iteration\n",
    "and policy_iteration algorithms.\"\"\"\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class MDP:\n",
    "\n",
    "    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n",
    "    and reward function. We also keep track of a gamma value, for use by\n",
    "    algorithms. The transition model is represented somewhat differently from\n",
    "    the text. Instead of P(s' | s, a) being a probability number for each\n",
    "    state/state/action triplet, we instead have T(s, a) return a\n",
    "    list of (p, s') pairs. We also keep track of the possible states,\n",
    "    terminal states, and actions for each state.\"\"\"\n",
    "\n",
    "    def __init__(self, init, actlist, terminals, transitions=None, reward=None, states=None, discount=0.9):\n",
    "        if not (0 < discount <= 1):\n",
    "            raise ValueError(\"An MDP must have 0 < discount <= 1\")\n",
    "\n",
    "        # collect states from transitions table if not passed.\n",
    "        self.states = states or self.get_states_from_transitions(transitions)\n",
    "            \n",
    "        self.init = init\n",
    "        \n",
    "        if isinstance(actlist, list):\n",
    "            # if actlist is a list, all states have the same actions\n",
    "            self.actlist = actlist\n",
    "\n",
    "        elif isinstance(actlist, dict):\n",
    "            # if actlist is a dict, different actions for each state\n",
    "            self.actlist = actlist\n",
    "        \n",
    "        self.terminals = terminals\n",
    "        self.transitions = transitions or {}\n",
    "        if not self.transitions:\n",
    "            print(\"Warning: Transition table is empty.\")\n",
    "            \n",
    "        self.discount = discount\n",
    "        \n",
    "        # maybe I should change this\n",
    "        # self.gamma = gamma\n",
    "\n",
    "        self.reward = reward or {s: 0 for s in self.states}\n",
    "\n",
    "        # self.check_consistency()\n",
    "\n",
    "    def R(self, state):\n",
    "        \"\"\"Return a numeric reward for this state.\"\"\"\n",
    "\n",
    "        return self.reward[state]\n",
    "\n",
    "    def T(self, state, action):\n",
    "        \"\"\"Transition model. From a state and an action, return a list\n",
    "        of (probability, result-state) pairs.\"\"\"\n",
    "\n",
    "        if not self.transitions:\n",
    "            raise ValueError(\"Transition model is missing\")\n",
    "        else:\n",
    "            return self.transitions[state][action]\n",
    "\n",
    "    def actions(self, state):\n",
    "        \"\"\"Return a list of actions that can be performed in this state. By default, a\n",
    "        fixed list of actions, except for terminal states. Override this\n",
    "        method if you need to specialize by state.\"\"\"\n",
    "\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.actlist\n",
    "\n",
    "    def get_states_from_transitions(self, transitions):\n",
    "        if isinstance(transitions, dict):\n",
    "            s1 = set(transitions.keys())\n",
    "            s2 = set(tr[1] for actions in transitions.values()\n",
    "                     for effects in actions.values()\n",
    "                     for tr in effects)\n",
    "            return s1.union(s2)\n",
    "        else:\n",
    "            print('Could not retrieve states from transitions')\n",
    "            return None\n",
    "\n",
    "    def check_consistency(self):\n",
    "\n",
    "        # check that all states in transitions are valid\n",
    "        assert set(self.states) == self.get_states_from_transitions(self.transitions)\n",
    "\n",
    "        # check that init is a valid state\n",
    "        assert self.init in self.states\n",
    "\n",
    "        # check reward for each state\n",
    "        assert set(self.reward.keys()) == set(self.states)\n",
    "\n",
    "        # check that all terminals are valid states\n",
    "        assert all(t in self.states for t in self.terminals)\n",
    "\n",
    "        # check that probability distributions for all actions sum to 1\n",
    "        for s1, actions in self.transitions.items():\n",
    "            for a in actions.keys():\n",
    "                s = 0\n",
    "                for o in actions[a]:\n",
    "                    s += o[0]\n",
    "                assert abs(s - 1) < 0.001\n",
    "\n",
    "class POMDP(MDP):\n",
    "\n",
    "    \"\"\"A Partially Observable Markov Decision Process, defined by\n",
    "    a transition model P(s'|s,a), actions A(s), a reward function R(s),\n",
    "    and a sensor model P(e|s). We also keep track of a gamma value,\n",
    "    for use by algorithms. The transition and the sensor models\n",
    "    are defined as matrices. We also keep track of the possible states\n",
    "    and actions for each state.\"\"\"\n",
    "\n",
    "    def __init__(self, actions, transitions=None, evidences=None, rewards=None, states=None, discount=0.95):\n",
    "        \"\"\"Initialize variables of the pomdp\"\"\"\n",
    "\n",
    "        if not (0 < discount <= 1):\n",
    "            raise ValueError('A POMDP must have 0 < discount <= 1')\n",
    "\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "\n",
    "        # transition model cannot be undefined\n",
    "        self.t_prob = transitions\n",
    "        if not self.t_prob.any():\n",
    "            print('Warning: Transition model is undefined')\n",
    "        \n",
    "        # sensor model cannot be undefined\n",
    "        self.e_prob = evidences\n",
    "        if not self.e_prob.any():\n",
    "            print('Warning: Sensor model is undefined')\n",
    "        \n",
    "        self.discount = discount\n",
    "        # may have to change this\n",
    "        # self.gamma = gamma\n",
    "        self.rewards = rewards\n",
    "        \n",
    "class Matrix:\n",
    "    \"\"\"Matrix operations class\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def add(A, B):\n",
    "        \"\"\"Add two matrices A and B\"\"\"\n",
    "\n",
    "        res = []\n",
    "        for i in range(len(A)):\n",
    "            row = []\n",
    "            for j in range(len(A[0])):\n",
    "                row.append(A[i][j] + B[i][j])\n",
    "            res.append(row)\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def scalar_multiply(a, B):\n",
    "        \"\"\"Multiply scalar a to matrix B\"\"\"\n",
    "\n",
    "        for i in range(len(B)):\n",
    "            for j in range(len(B[0])):\n",
    "                B[i][j] = a * B[i][j]\n",
    "        return B\n",
    "\n",
    "    @staticmethod\n",
    "    def multiply(A, B):\n",
    "        \"\"\"Multiply two matrices A and B element-wise\"\"\"\n",
    "\n",
    "        matrix = []\n",
    "        for i in range(len(B)):\n",
    "            row = []\n",
    "            for j in range(len(B[0])):\n",
    "                row.append(B[i][j] * A[j][i])\n",
    "            matrix.append(row)\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def matmul(A, B):\n",
    "        \"\"\"Inner-product of two matrices\"\"\"\n",
    "\n",
    "        return [[sum(ele_a*ele_b for ele_a, ele_b in zip(row_a, col_b)) for col_b in list(zip(*B))] for row_a in A]\n",
    "\n",
    "    @staticmethod\n",
    "    def transpose(A):\n",
    "        \"\"\"Transpose a matrix\"\"\"\n",
    "        \n",
    "        return [list(i) for i in zip(*A)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving POMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linprog\n",
    "import numpy as np\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaVector:\n",
    "    \"\"\"\n",
    "    Simple wrapper for an alpha vector, used for representing the value function for a POMDP as a piecewise-linear,\n",
    "    convex function\n",
    "    \"\"\"\n",
    "    def __init__(self, a, v):\n",
    "        self.action = a\n",
    "        self.v = v\n",
    "\n",
    "    def copy(self):\n",
    "        return AlphaVector(self.action, self.v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIteration:\n",
    "    def __init__(self, pomdp):\n",
    "        \"\"\"\n",
    "        Initialize the POMDP exact value iteration solver\n",
    "        :param agent:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.model = pomdp\n",
    "        self.gamma = set()\n",
    "\n",
    "    def value_iteration(self, horizon):\n",
    "        \"\"\"\n",
    "        Solve the POMDP by computing all alpha vectors\n",
    "        :param t: transition probability matrix\n",
    "        :param o: observation probability matrix\n",
    "        :param r: immediate rewards matrix\n",
    "        :param horizon: integer valued scalar represented the number of planning steps\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        t = self.model.t_prob\n",
    "        o = self.model.e_prob\n",
    "        r = self.model.reward\n",
    "        # the above three are used later--important\n",
    "        discount = self.model.discount\n",
    "        actions = len(self.model.actions)  # |A| actions\n",
    "        states = len(self.model.states)  # |S| states\n",
    "        # I might need to change this\n",
    "        observations = len(self.model.get_all_observations())  # |Z| observations\n",
    "        first = True\n",
    "\n",
    "        # initialize gamma with a 0 alpha-vector\n",
    "        dummy = AlphaVector(a=-1, v=np.zeros(states))\n",
    "        self.gamma.add(dummy)\n",
    "\n",
    "        # start with 1 step planning horizon, up to horizon-length planning horizon\n",
    "        for k in range(horizon):\n",
    "            print('[Value Iteration] planning horizon {}...'.format(k))\n",
    "            # new set of alpha vectors to add to set gamma\n",
    "            gamma_k = set()\n",
    "            # Compute the new coefficients for the new alpha-vectors\n",
    "            v_new = np.zeros(shape=(len(self.gamma), actions, observations, states))\n",
    "            idx = 0\n",
    "            for v in self.gamma:\n",
    "                for u in range(actions):\n",
    "                    for z in range(observations):\n",
    "                        for j in range(states):\n",
    "                            for i in range(states):\n",
    "                                # v_i_k * p(z | x_i, u) * p(x_i | u, x_j)\n",
    "                                v_new[idx][u][z][i] += v.v[i] * o[u][i][z] * t[u][j][i]\n",
    "                idx += 1\n",
    "            # add (|A| * |V|^|Z|) alpha-vectors to gamma, |V| is |gamma_k|\n",
    "            for u in range(actions):\n",
    "                c = self.compute_indices(idx, observations)\n",
    "                for indices in c:  # n elements in c is |V|^|Z|\n",
    "                    for z in range(observations):\n",
    "                        temp = np.zeros(states)\n",
    "                        for i in range(states):\n",
    "                            temp[i] = discount * (r[u][i] + v_new[indices[z]][u][z][i])\n",
    "                        gamma_k.add(AlphaVector(a=u, v=temp))\n",
    "            self.gamma.update(gamma_k)\n",
    "            if first:\n",
    "                # remove the dummy alpha vector\n",
    "                self.gamma.remove(dummy)\n",
    "                first = False\n",
    "            self.prune(states)\n",
    "            #  plot_gamma(title='V(b) for horizon T = ' + str(k + 1), self.gamma)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_indices(k, m):\n",
    "        \"\"\"\n",
    "        Compute all orderings of m elements with values between [0, k-1]\n",
    "        :param k: Number of alpha-vectors\n",
    "        :param m: Number of observations\n",
    "        :return: list of lists, where each list contains m elements, and each element is in [0, k-1].\n",
    "        Total should be k^m elements\n",
    "        \"\"\"\n",
    "        x = list(range(k))\n",
    "        return [p for p in product(x, repeat=m)]\n",
    "\n",
    "    def prune(self, n_states):\n",
    "        \"\"\"\n",
    "        Remove dominated alpha-vectors using Lark's filtering algorithm\n",
    "        :param n_states\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # parameters for linear program\n",
    "        delta = 0.0000000001\n",
    "        # equality constraints on the belief states\n",
    "        A_eq = np.array([np.append(np.ones(n_states), [0.])])\n",
    "        b_eq = np.array([1.])\n",
    "\n",
    "        # dirty set\n",
    "        F = self.gamma.copy()\n",
    "        # clean set\n",
    "        Q = set()\n",
    "\n",
    "        for i in range(n_states):\n",
    "            max_i = -np.inf\n",
    "            best = None\n",
    "            for av in F:\n",
    "                if av.v[i] > max_i:\n",
    "                    max_i = av.v[i]\n",
    "                    best = av\n",
    "            Q.update({best})\n",
    "            F.remove(best)\n",
    "        while F:\n",
    "            av_i = F.pop()  # get a reference to av_i\n",
    "            F.add(av_i)  # don't want to remove it yet from F\n",
    "            dominated = False\n",
    "            for av_j in Q:\n",
    "                c = np.append(np.zeros(n_states), [1.])\n",
    "                A_ub = np.array([np.append(-(av_i.v - av_j.v), [-1.])])\n",
    "                b_ub = np.array([-delta])\n",
    "\n",
    "                res = linprog(c, A_eq=A_eq, b_eq=b_eq, A_ub=A_ub, b_ub=b_ub, bounds=(0, None))\n",
    "                if res.x[n_states] > 0.0:\n",
    "                    # this one is dominated\n",
    "                    dominated = True\n",
    "                    F.remove(av_i)\n",
    "                    break\n",
    "\n",
    "            if not dominated:\n",
    "                max_k = -np.inf\n",
    "                best = None\n",
    "                for av_k in F:\n",
    "                    b = res.x[0:2]\n",
    "                    v = np.dot(av_k.v, b)\n",
    "                    if v > max_k:\n",
    "                        max_k = v\n",
    "                        best = av_k\n",
    "                F.remove(best)\n",
    "                if not self.check_duplicate(Q, best):\n",
    "                    Q.update({best})\n",
    "        self.gamma = Q\n",
    "\n",
    "    @staticmethod\n",
    "    def check_duplicate(a, av):\n",
    "        \"\"\"\n",
    "        Check whether alpha vector av is already in set a\n",
    "\n",
    "        :param a:\n",
    "        :param av:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for av_i in a:\n",
    "            if np.allclose(av_i.v, av.v):\n",
    "                return True\n",
    "            if av_i.v[0] == av.v[0] and av_i.v[1] > av.v[1]:\n",
    "                return True\n",
    "            if av_i.v[1] == av.v[1] and av_i.v[0] > av.v[0]:\n",
    "                return True\n",
    "\n",
    "    @staticmethod\n",
    "    def select_action(belief, vector_set):\n",
    "        \"\"\"\n",
    "        Compute optimal action given a belief distribution\n",
    "        :param belief: dim(belief) == dim(AlphaVector)\n",
    "        :param vector_set\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        max_v = -np.inf\n",
    "        best = None\n",
    "        for av in vector_set:\n",
    "            v = np.dot(av.v, belief)\n",
    "\n",
    "            if v > max_v:\n",
    "                max_v = v\n",
    "                best = av\n",
    "\n",
    "        if best is None:\n",
    "            raise ValueError('Vector set should not be empty')\n",
    "\n",
    "        return best.action, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'states' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1fad23fdbdfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'states' is not defined"
     ]
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pomdp = POMDP(actions, t_prob, e_prob, rewards, states, discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pomdp_solve = ValueIteration(pomdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'0': [array([ 219.62145794,   87.04320423,  -58.95578452, -185.0924472 ])]})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utility = pomdp_value_iteration(pomdp, epsilon=3)\n",
    "utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>204.525160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>88.982036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-34.290243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-171.265281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0  204.525160\n",
       "1   88.982036\n",
       "2  -34.290243\n",
       "3 -171.265281"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.array([204.52516018,   88.9820356 ,  -34.29024281, -171.26528113]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADQRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/miniconda3/envs/custom_hmm/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/yannik/miniconda3/envs/custom_hmm/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/yannik/miniconda3/envs/custom_hmm/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/yannik/miniconda3/envs/custom_hmm/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/yannik/miniconda3/envs/custom_hmm/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/yannik/miniconda3/envs/custom_hmm/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/yannik/miniconda3/envs/custom_hmm/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/yannik/miniconda3/envs/custom_hmm/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/yannik/miniconda3/envs/custom_hmm/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/yannik/miniconda3/envs/custom_hmm/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/yannik/miniconda3/envs/custom_hmm/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/yannik/miniconda3/envs/custom_hmm/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These lines establish the feed-forward part of the network used to choose actions\n",
    "inputs1 = tf.placeholder(shape=[1,13],dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_uniform([13,2],0,0.01))\n",
    "Qout = tf.matmul(inputs1,W)\n",
    "predict = tf.argmax(Qout,1)\n",
    "\n",
    "#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "nextQ = tf.placeholder(shape=[1,2],dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(obs):\n",
    "    state = 0\n",
    "    diff = 16\n",
    "    for i in range(len(statemean)):\n",
    "        stateDiff = obs - statemean[i]\n",
    "        stateDiffVal = np.sqrt(np.mean(stateDiff**2))\n",
    "        if stateDiffVal < diff:\n",
    "            diff = stateDiffVal\n",
    "            state = i\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the next step after an action is done\n",
    "\n",
    "def getStepDetails(i,j,action):\n",
    "    unitData = dataT_cycles[i]\n",
    "    d = False\n",
    "    if action == 1:\n",
    "        newJ = 0\n",
    "    else:\n",
    "        newJ = j+1\n",
    "    obsNext = unitData.values[newJ]\n",
    "    if newJ >= len(unitData) - 1:\n",
    "        d = True\n",
    "    s1 = get_state(obsNext)\n",
    "    r1 = rewards[action][s1]\n",
    "    return r1,newJ,s1,obsNext,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set learning parameters\n",
    "init = tf.global_variables_initializer()\n",
    "y = discount\n",
    "e = 0.1\n",
    "num_episodes = len(dataT_cycles)\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "D = np.empty([0,5]) # Replay memory\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation for new unit\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        k = 0\n",
    "        unitData = dataT_cycles[i]\n",
    "        #The Q-Network\n",
    "        while j < len(unitData):\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            a,allQ = sess.run([predict,Qout],feed_dict={inputs1:unitData.values[j].reshape(1,13)})\n",
    "            if np.random.rand(1) < e:\n",
    "                a[0] = np.random.randint(0,2)\n",
    "            #Get new state and reward from environment\n",
    "            r,j,s1,o1,d = getStepDetails(i,j,a[0])\n",
    "            D = np.vstack([D, [a[0],unitData.values[j-1].reshape(1,13),r,o1,s1]])\n",
    "            if len(D) > 20:\n",
    "                lastInd = np.random.randint(15,len(D))\n",
    "                randomSample = D[lastInd-15:lastInd]\n",
    "                finalO = D[lastInd,3].reshape(1,13)\n",
    "                Reward = np.sum(D[lastInd-15:lastInd,2])\n",
    "            else:\n",
    "                finalO = o1.reshape(1,13)\n",
    "                Reward = r\n",
    "            # We take batch size of 15 (j in algorithm)\n",
    "            #Obtain the Q' values by feeding the new state through our network\n",
    "            Q1 = sess.run(Qout,feed_dict={inputs1:finalO})\n",
    "            #Obtain maxQ' and set our target value for chosen action.\n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            targetQ[0,a[0]] = Reward + y*maxQ1\n",
    "            #Train our network using target and predicted Q values\n",
    "            _,W1 = sess.run([updateModel,W],feed_dict={inputs1:unitData.values[j-1].reshape(1,13),nextQ:targetQ})\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            k += 1\n",
    "            if d == True or k >= 1000:\n",
    "                #Reduce chance of random action as we train the model.\n",
    "                e = 1./((i/50) + 10)\n",
    "                break\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dataT_cycles[5].values[160].reshape(-1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77108434, 0.48484848, 0.40257649, 0.51515152, 0.03688414,\n",
       "       0.61904762, 0.36886994, 0.55882353, 0.03333677, 0.65101962,\n",
       "       0.58333333, 0.30232558, 0.2903894 ])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataT_cycles[5].values[160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2364.409  , 5880.967  ],\n",
       "       [1871.21   , 5603.3594 ],\n",
       "       [3616.782  , 4988.657  ],\n",
       "       [4809.125  ,  873.2698 ],\n",
       "       [3126.0398 ,   76.49012],\n",
       "       [3813.273  , 3731.947  ],\n",
       "       [5509.8306 , 1830.7938 ],\n",
       "       [4643.098  ,  543.59045],\n",
       "       [8576.893  ,  -95.22542],\n",
       "       [2755.4534 , 2314.9817 ],\n",
       "       [2022.7578 , 2662.1064 ],\n",
       "       [4247.6187 , 1322.4498 ],\n",
       "       [2997.3079 , 1125.1122 ]], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19181.12600722, 16785.20466186]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a,W1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate that my modified HMM approach does well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "startprob = np.array([1.0, 0.0, 0.0, 0.0])\n",
    "# The transition matrix\n",
    "transmat = np.array([[0.5, 0.5, 0.        , 0.        ],\n",
    "       [0.        , 0.7, 0.3, 0.        ],\n",
    "       [0.        , 0.        , 0.7, 0.3],\n",
    "       [0.        , 0.        , 0.        , 1.        ]])\n",
    "# The means of each component\n",
    "means = np.array([[0.0],\n",
    "                  [2.0],\n",
    "                  [4.0],\n",
    "                  [6.0]])\n",
    "# The covariance of each component\n",
    "covars = np.array([[0.5],[0.5],[0.5],[0.5]])\n",
    "#covars = .5 * np.tile(np.identity(1), (4, 1, 1))\n",
    "\n",
    "# Build an HMM instance and set parameters\n",
    "model = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\")\n",
    "\n",
    "# Instead of fitting it from the data, we directly set the estimated\n",
    "# parameters, the means and covariance of the components\n",
    "model.startprob_ = startprob\n",
    "model.transmat_ = transmat\n",
    "model.means_ = means\n",
    "model.covars_ = covars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "X, Z = model.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47728385],\n",
       "       [1.06168843],\n",
       "       [1.14539785],\n",
       "       ...,\n",
       "       [5.84424743],\n",
       "       [7.08687377],\n",
       "       [5.60390752]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 3, 3, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianHMM(algorithm='viterbi', covariance_type='diag', covars_prior=0.01,\n",
       "            covars_weight=1, init_params='cm', means_prior=0, means_weight=0,\n",
       "            min_covar=0.001, n_components=4, n_iter=10, params='mtc',\n",
       "            random_state=None, startprob_prior=1.0, tol=0.01,\n",
       "            transmat_prior=1.0, verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\",init_params=\"cm\", params=\"mtc\")\n",
    "lr.startprob_ = np.array([1.0, 0.0, 0.0, 0.0])\n",
    "lr.transmat_ = np.array([[0.5, 0.5, 0.0, 0.0],[0.0, 0.5, 0.5, 0.0],[0.0, 0.0, 0.5, 0.5],[0.0,0.0,0.0,1.0]])\n",
    "lr.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.98338808e-01, 1.01661192e-01, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 9.99665045e-01, 3.34955472e-04, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 8.83387738e-01, 1.16612262e-01],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.transmat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.898339</td>\n",
       "      <td>0.101661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999665</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.883388</td>\n",
       "      <td>0.116612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3\n",
       "0  0.898339  0.101661  0.000000  0.000000\n",
       "1  0.000000  0.999665  0.000335  0.000000\n",
       "2  0.000000  0.000000  0.883388  0.116612\n",
       "3  0.000000  0.000000  0.000000  1.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lr.transmat_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2.51972294],\n",
       "        [6.01111988],\n",
       "        [5.5002918 ],\n",
       "        [5.98793509]]), array([[[1.76249943]],\n",
       " \n",
       "        [[0.52250327]],\n",
       " \n",
       "        [[0.35316779]],\n",
       " \n",
       "        [[0.4864394 ]]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.means_, lr.covars_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 3, 3, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\",init_params=\"cm\", params=\"mtc\")\n",
    "lr.startprob_ = np.array([1.0, 0.0, 0.0, 0.0])\n",
    "transmats = []\n",
    "statemeans = []\n",
    "covars = []\n",
    "for i in range(15):\n",
    "    X, Z = model.sample(1000)\n",
    "    lr.transmat_ = np.array([[0.5, 0.5, 0.0, 0.0],[0.0, 0.5, 0.5, 0.0],[0.0, 0.0, 0.5, 0.5],[0.0,0.0,0.0,1.0]])\n",
    "    lr.fit(X)\n",
    "    statemeans.append(lr.means_)\n",
    "    transmats.append(lr.transmat_)\n",
    "    covars.append(lr.covars_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transmat = np.array(transmats).mean(axis=0)\n",
    "statemean = np.array(statemeans).mean(axis=0)\n",
    "covar = np.array(covars).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.633537]),\n",
       " array([66.94567181]),\n",
       " array([200.51668965]),\n",
       " array([400.30160193])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[covar[i].sum(axis=1) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94301815],\n",
       "       [3.03061227],\n",
       "       [4.23161938],\n",
       "       [6.21571146]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statemean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33804046, 0.66195954, 0.        , 0.        ],\n",
       "       [0.        , 0.32367405, 0.67632595, 0.        ],\n",
       "       [0.        , 0.        , 0.72214069, 0.27785931],\n",
       "       [0.        , 0.        , 0.        , 1.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.33804</td>\n",
       "      <td>0.661960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.323674</td>\n",
       "      <td>0.676326</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.722141</td>\n",
       "      <td>0.277859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3\n",
       "0  0.33804  0.661960  0.000000  0.000000\n",
       "1  0.00000  0.323674  0.676326  0.000000\n",
       "2  0.00000  0.000000  0.722141  0.277859\n",
       "3  0.00000  0.000000  0.000000  1.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(transmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.means_=statemean; lr.covars_=covar; lr.transmat_=transmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "covar=[covar[i].sum(axis=1) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5],\n",
       "       [0.5],\n",
       "       [0.5],\n",
       "       [0.5]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[0.5],[0.5],[0.5],[0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
