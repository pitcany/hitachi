{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import norm\n",
    "import pomegranate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data and diving them into unique unit numbers\n",
    "\n",
    "We need to divide data into unique numbers, because the state restes as the unit number changes, so we need to find Gaussian distribution for different unit numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('~/Documents/hitachi/CMAPSS/train_FD001.txt', sep=\" \", header=None)\n",
    "unique_unit_values = data[0].unique() #Number of units\n",
    "data_cycles = []\n",
    "for unit_num in unique_unit_values:\n",
    "    data_cycles.append(data[data[0] == unit_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing operational settings and normalize the data column wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    x = data.values\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    dataNew = pd.DataFrame(x_scaled)\n",
    "    return dataNew\n",
    "#Remove the operation settings\n",
    "dataT = data[data.columns[5:26]]\n",
    "dataT.columns = range(21)\n",
    "dataT = normalize(dataT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividing data for each unit\n",
    "\n",
    "I think this is why my transitional matrix previously was not working properly as in each unit the state resets and start from good condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataT_cycles = []\n",
    "for unit_num in unique_unit_values:\n",
    "    dataT_cycles.append(dataT[data[0] == unit_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying and removing non variable data columns\n",
    "\n",
    "Removing the columns where the data does not vary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "for dataT_cycle in dataT_cycles:\n",
    "    print(dataT_cycle.columns[dataT_cycle.std() == 0])\n",
    "\"\"\"\n",
    "Here we can see 0,4,9,15,17,18 but also 5 at many places so we drop column number 5 as well\n",
    "\"\"\"\n",
    "dataT.drop(data.columns[[0, 3, 4, 5, 9, 15, 17, 18]],axis=1,inplace=True)\n",
    "dataT.columns = range(13)\n",
    "dataT_cycles = []\n",
    "for unit_num in unique_unit_values:\n",
    "    dataT_cycles.append(dataT[data[0] == unit_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 13)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Right now only using the first data frame (i.e Machine 1) to train the VAE, but we can combine all the dataframes\n",
    "# and train the VAE jointly on the entire data for better performance \n",
    "\n",
    "x_train = dataT_cycles[0].values[:150]\n",
    "x_test = dataT_cycles[0].values[151:198]\n",
    "x_train.shape\n",
    "# x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational AutoEncoders to find Latent State Space Distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "x_train = dataT_cycles[0].values[:100]\n",
    "x_test = dataT_cycles[0].values[101:198]\n",
    "x_train.shape\n",
    "x_test.shape\n",
    "original_dim = x_train[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "input_shape = (original_dim, )\n",
    "intermediate_dim = 9\n",
    "batch_size = 10\n",
    "latent_dim = 5\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# z = z_mean + sqrt(var)*eps\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    \n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yannik/miniconda3/envs/ykp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yannik/miniconda3/envs/ykp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yannik/miniconda3/envs/ykp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yannik/miniconda3/envs/ykp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 9)            126         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 5)            50          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 5)            50          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 5)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 226\n",
      "Trainable params: 226\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# VAE Model Encoder + Decoder \n",
    "\n",
    "# Building the Encoder\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 54        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 13)                130       \n",
      "=================================================================\n",
      "Total params: 184\n",
      "Trainable params: 184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build decoder model \n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    help_ = \"Load h5 model trained weights\"\n",
    "    parser.add_argument(\"-w\", \"--weights\", help=help_)\n",
    "    help_ = \"Use mse loss instead of binary cross entropy (default)\"\n",
    "    parser.add_argument(\"-m\", \"--mse\", help=help_, action='store_true')\n",
    "    \n",
    "    models = (encoder, decoder)\n",
    "    data = (x_test, None)\n",
    "    \n",
    "    # VAE loss = mse_loss or xent_loss + kl_loss\n",
    "    if args.mse:\n",
    "        reconstruction_loss = mse(inputs, outputs)\n",
    "    else:\n",
    "        reconstruction_loss = binary_crossentropy(inputs, outputs)\n",
    "        \n",
    "    reconstruction_loss *= original_dim\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis= -1)\n",
    "    kl_loss *= 0.5 \n",
    "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "    vae.add_loss(vae_loss)\n",
    "    vae.compile(optimizer='adam')\n",
    "    vae.summary()\n",
    "    \n",
    "    if args.weights:\n",
    "        vae.load_weights(args.weights)\n",
    "    else:\n",
    "        # Train the autoencoder\n",
    "        vae.fit(x_train, epochs=epochs, batch_size= batch_size, validation_data=(x_test, None))\n",
    "        vae.save_weights('vae_mlp_CMAPSS.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yannik/miniconda3/envs/ykp/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/yannik/miniconda3/envs/ykp/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/yannik/miniconda3/envs/ykp/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"vae_mlp\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 13)                0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 5), (None, 5), (N 226       \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 13)                184       \n",
      "=================================================================\n",
      "Total params: 410\n",
      "Trainable params: 410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 100 samples, validate on 91 samples\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 8.6769 - val_loss: 8.8645\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 0s 148us/step - loss: 8.3519 - val_loss: 8.5144\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 0s 147us/step - loss: 7.8255 - val_loss: 8.2903\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 0s 135us/step - loss: 7.2818 - val_loss: 7.7478\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 0s 160us/step - loss: 6.7071 - val_loss: 7.3226\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 0s 157us/step - loss: 5.9024 - val_loss: 6.6842\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 0s 149us/step - loss: 4.8250 - val_loss: 5.8048\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 0s 146us/step - loss: 3.6265 - val_loss: 4.7482\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 0s 147us/step - loss: 2.1559 - val_loss: 3.4966\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 0s 143us/step - loss: 0.2559 - val_loss: 1.8037\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 0s 141us/step - loss: -2.0393 - val_loss: -0.5226\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 0s 129us/step - loss: -5.5086 - val_loss: -3.6549\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 0s 133us/step - loss: -9.9743 - val_loss: -8.1698\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 0s 134us/step - loss: -16.3911 - val_loss: -14.6761\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 0s 140us/step - loss: -26.1404 - val_loss: -24.7845\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 0s 144us/step - loss: -40.7592 - val_loss: -40.3913\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 0s 131us/step - loss: -64.0481 - val_loss: -66.5411\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 0s 133us/step - loss: -103.4882 - val_loss: -111.1918\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 0s 125us/step - loss: -173.0324 - val_loss: -194.4501\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 0s 130us/step - loss: -303.7161 - val_loss: -354.4499\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 0s 131us/step - loss: -559.3179 - val_loss: -687.4695\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 0s 122us/step - loss: -1090.3372 - val_loss: -1421.7147\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 0s 152us/step - loss: -2317.1455 - val_loss: -3152.3568\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 0s 155us/step - loss: -5173.7008 - val_loss: -7505.0789\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 0s 170us/step - loss: -12652.9899 - val_loss: -19147.9315\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 0s 149us/step - loss: -32355.5510 - val_loss: -52455.2810\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 0s 140us/step - loss: -89452.4062 - val_loss: -154226.5113\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 0s 137us/step - loss: -267485.3063 - val_loss: -484234.3293\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 0s 131us/step - loss: -812258.0594 - val_loss: -1617520.3530\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 0s 129us/step - loss: -2696916.6250 - val_loss: -5834498.8681\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 0s 144us/step - loss: -9529330.1750 - val_loss: -22602066.1758\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 0s 138us/step - loss: -36401668.8000 - val_loss: -93728027.6923\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 0s 125us/step - loss: -151809246.4000 - val_loss: -421101754.0220\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 0s 132us/step - loss: -636083996.8000 - val_loss: -2024083935.6484\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 0s 109us/step - loss: -2981812544.0000 - val_loss: -10615089860.9231\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 0s 112us/step - loss: -14448699084.8000 - val_loss: -59401903227.7802\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 0s 105us/step - loss: -82958974771.2000 - val_loss: -349718182495.6484\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 0s 125us/step - loss: -504404056473.6000 - val_loss: -2209139918240.3516\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 0s 132us/step - loss: -2455431472742.3999 - val_loss: -14302201537862.3320\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 0s 128us/step - loss: -16546123795660.8008 - val_loss: -102467904053338.0156\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 0s 127us/step - loss: -110129182670848.0000 - val_loss: -790240565199568.1250\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 0s 113us/step - loss: -806637331388825.6250 - val_loss: -6085804254989481.0000\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 0s 124us/step - loss: -5998949728623002.0000 - val_loss: -50539582233993848.0000\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 0s 123us/step - loss: -41903960307322064.0000 - val_loss: -456907070801061760.0000\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 0s 110us/step - loss: -297370485746407808.0000 - val_loss: -4642760663743772672.0000\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 0s 120us/step - loss: nan - val_loss: nan\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 0s 131us/step - loss: nan - val_loss: nan\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 0s 120us/step - loss: nan - val_loss: nan\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 0s 122us/step - loss: nan - val_loss: nan\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 0s 117us/step - loss: nan - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    mse = None\n",
    "    weights = None\n",
    "    \n",
    "args = Args()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the VAE has been trained, we can use the encoder to sample the latent space\n",
    "\n",
    "#predicting the latent space for first 13 observation values for machine 1\n",
    "test = np.asarray(x_train[0:13])  \n",
    "\n",
    "# indexing on 2 because the encoder predicts z_mean, z_log_var and sampled vector z (we are interested in z only)\n",
    "latent_space = encoder.predict(test)[2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan],\n",
       "       [nan, nan, nan, nan, nan]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each list is a 5 dimension latent state space for that observation value\n",
    "latent_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the raw observation from the learned latent space \n",
    "sample = decoder.predict(latent_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18373494, 0.40680183, 0.72624799, 0.24242424, 0.109755  ,\n",
       "       0.36904762, 0.63326226, 0.20588235, 0.1996078 , 0.36398615,\n",
       "       0.33333333, 0.71317829, 0.7246617 ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the with the real x_train value\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Right now they are not same as we trained the VAE on very less amount of data \n",
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using HMM to find out transitional matrices\n",
    "\n",
    "Here we first define transmatrix as [[0.5, 0.5, 0.0, 0.0],[0.0, 0.5, 0.5, 0.0],[0.0, 0.0, 0.5, 0.5],[0.0,0.0,0.0,1.0]] which means there is half chance for each state to go to next state and half to remain in the current state itself.\n",
    "\n",
    "Then we train for each unit for transmatrix as well as state means and we will take average of each unit transmatrices and states as the transmatrix and state\n",
    "\n",
    "*Note*: Here state '0' means the perfect health and '3' means weakest health "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.183735</td>\n",
       "      <td>0.406802</td>\n",
       "      <td>0.726248</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.109755</td>\n",
       "      <td>0.369048</td>\n",
       "      <td>0.633262</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.199608</td>\n",
       "      <td>0.363986</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.713178</td>\n",
       "      <td>0.724662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.283133</td>\n",
       "      <td>0.453019</td>\n",
       "      <td>0.628019</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.100242</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.765458</td>\n",
       "      <td>0.279412</td>\n",
       "      <td>0.162813</td>\n",
       "      <td>0.411312</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.731014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.343373</td>\n",
       "      <td>0.369523</td>\n",
       "      <td>0.710145</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.140043</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.795309</td>\n",
       "      <td>0.220588</td>\n",
       "      <td>0.171793</td>\n",
       "      <td>0.357445</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.621375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.343373</td>\n",
       "      <td>0.256159</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.124518</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.889126</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.174889</td>\n",
       "      <td>0.166603</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.573643</td>\n",
       "      <td>0.662386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.349398</td>\n",
       "      <td>0.257467</td>\n",
       "      <td>0.668277</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.149960</td>\n",
       "      <td>0.255952</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.174734</td>\n",
       "      <td>0.402078</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.589147</td>\n",
       "      <td>0.704502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.765060</td>\n",
       "      <td>0.683235</td>\n",
       "      <td>0.336554</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>0.072602</td>\n",
       "      <td>0.684524</td>\n",
       "      <td>0.234542</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.091599</td>\n",
       "      <td>0.753367</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.286822</td>\n",
       "      <td>0.089202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.894578</td>\n",
       "      <td>0.547853</td>\n",
       "      <td>0.136876</td>\n",
       "      <td>0.560606</td>\n",
       "      <td>0.102396</td>\n",
       "      <td>0.732143</td>\n",
       "      <td>0.189765</td>\n",
       "      <td>0.661765</td>\n",
       "      <td>0.090670</td>\n",
       "      <td>0.744132</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.263566</td>\n",
       "      <td>0.301712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.731928</td>\n",
       "      <td>0.614345</td>\n",
       "      <td>0.231884</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.084582</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.287846</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>0.065229</td>\n",
       "      <td>0.759523</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.271318</td>\n",
       "      <td>0.239299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.641566</td>\n",
       "      <td>0.682799</td>\n",
       "      <td>0.172303</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.094364</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>0.187633</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.075704</td>\n",
       "      <td>0.740669</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.240310</td>\n",
       "      <td>0.324910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.701807</td>\n",
       "      <td>0.662089</td>\n",
       "      <td>0.225443</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.051557</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.296375</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.056714</td>\n",
       "      <td>0.717199</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.263566</td>\n",
       "      <td>0.097625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.183735  0.406802  0.726248  0.242424  0.109755  0.369048  0.633262   \n",
       "1    0.283133  0.453019  0.628019  0.212121  0.100242  0.380952  0.765458   \n",
       "2    0.343373  0.369523  0.710145  0.272727  0.140043  0.250000  0.795309   \n",
       "3    0.343373  0.256159  0.740741  0.318182  0.124518  0.166667  0.889126   \n",
       "4    0.349398  0.257467  0.668277  0.242424  0.149960  0.255952  0.746269   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "187  0.765060  0.683235  0.336554  0.621212  0.072602  0.684524  0.234542   \n",
       "188  0.894578  0.547853  0.136876  0.560606  0.102396  0.732143  0.189765   \n",
       "189  0.731928  0.614345  0.231884  0.590909  0.084582  0.880952  0.287846   \n",
       "190  0.641566  0.682799  0.172303  0.575758  0.094364  0.773810  0.187633   \n",
       "191  0.701807  0.662089  0.225443  0.636364  0.051557  0.833333  0.296375   \n",
       "\n",
       "            7         8         9        10        11        12  \n",
       "0    0.205882  0.199608  0.363986  0.333333  0.713178  0.724662  \n",
       "1    0.279412  0.162813  0.411312  0.333333  0.666667  0.731014  \n",
       "2    0.220588  0.171793  0.357445  0.166667  0.627907  0.621375  \n",
       "3    0.294118  0.174889  0.166603  0.333333  0.573643  0.662386  \n",
       "4    0.235294  0.174734  0.402078  0.416667  0.589147  0.704502  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "187  0.514706  0.091599  0.753367  0.666667  0.286822  0.089202  \n",
       "188  0.661765  0.090670  0.744132  0.583333  0.263566  0.301712  \n",
       "189  0.691176  0.065229  0.759523  0.833333  0.271318  0.239299  \n",
       "190  0.617647  0.075704  0.740669  0.500000  0.240310  0.324910  \n",
       "191  0.647059  0.056714  0.717199  0.666667  0.263566  0.097625  \n",
       "\n",
       "[192 rows x 13 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataT_cycles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\",init_params=\"cm\", params=\"mtc\")\n",
    "lr.startprob_ = np.array([1.0, 0.0, 0.0, 0.0])\n",
    "transmats = []\n",
    "statemeans = []\n",
    "covars = []\n",
    "for i in range(100):\n",
    "    lr.transmat_ = np.array([[0.5, 0.5, 0.0, 0.0],[0.0, 0.5, 0.5, 0.0],[0.0, 0.0, 0.5, 0.5],[0.0,0.0,0.0,1.0]])\n",
    "    lr.fit(dataT_cycles[i])\n",
    "    transmats.append(lr.transmat_)\n",
    "    statemeans.append(lr.means_)\n",
    "    covars.append(lr.covars_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/miniconda3/envs/ykp/lib/python3.7/site-packages/hmmlearn/hmm.py:929: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  new_cov = new_cov_numer / new_cov_denom\n",
      "/home/yannik/miniconda3/envs/ykp/lib/python3.7/site-packages/hmmlearn/hmm.py:870: RuntimeWarning: invalid value encountered in true_divide\n",
      "  new_weights = new_weights_numer / new_weights_denom\n",
      "/home/yannik/miniconda3/envs/ykp/lib/python3.7/site-packages/hmmlearn/hmm.py:879: RuntimeWarning: invalid value encountered in true_divide\n",
      "  new_means = new_means_numer / new_means_denom\n",
      "/home/yannik/miniconda3/envs/ykp/lib/python3.7/site-packages/hmmlearn/hmm.py:929: RuntimeWarning: invalid value encountered in true_divide\n",
      "  new_cov = new_cov_numer / new_cov_denom\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "mixture weights must sum up to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-88007512b356>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransmat_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataT_cycles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtransmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransmat_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtransmats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ykp/lib/python3.7/site-packages/hmmlearn/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, lengths)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvergenceMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ykp/lib/python3.7/site-packages/hmmlearn/hmm.py\u001b[0m in \u001b[0;36m_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    724\u001b[0m         if not np.allclose(np.sum(self.weights_, axis=1),\n\u001b[1;32m    725\u001b[0m                            np.ones(self.n_components)):\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mixture weights must sum up to 1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;31m# Checking means' shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mixture weights must sum up to 1"
     ]
    }
   ],
   "source": [
    "lr = hmm.GMMHMM(n_components=4, n_mix=4, covariance_type=\"diag\",init_params=\"cm\", params=\"mt\")\n",
    "lr.startprob_ = np.array([1.0, 0.0, 0.0, 0.0])\n",
    "transmats = []\n",
    "statemeans = []\n",
    "for i in range(100):\n",
    "    lr.transmat_ = np.array([[0.5, 0.5, 0.0, 0.0],[0.0, 0.5, 0.5, 0.0],[0.0, 0.0, 0.5, 0.5],[0.0,0.0,0.0,1.0]])\n",
    "    lr.fit(dataT_cycles[i])\n",
    "    transmat = lr.transmat_\n",
    "    transmats.append(transmat)\n",
    "    statemeans.append(lr.means_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transmat = np.array(transmats).mean(axis=0)\n",
    "statemean = np.array(statemeans).mean(axis=0)\n",
    "covar = np.array(covars).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50564876, 0.49435124, 0.        , 0.        ],\n",
       "       [0.        , 0.65384813, 0.34615187, 0.        ],\n",
       "       [0.        , 0.        , 0.96245835, 0.03754165],\n",
       "       [0.        , 0.        , 0.        , 0.9       ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.505649</td>\n",
       "      <td>0.494351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.653848</td>\n",
       "      <td>0.346152</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.962458</td>\n",
       "      <td>0.037542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3\n",
       "0  0.505649  0.494351  0.000000  0.000000\n",
       "1  0.000000  0.653848  0.346152  0.000000\n",
       "2  0.000000  0.000000  0.962458  0.037542\n",
       "3  0.000000  0.000000  0.000000  0.900000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(transmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.361359</td>\n",
       "      <td>0.346030</td>\n",
       "      <td>0.655493</td>\n",
       "      <td>0.245415</td>\n",
       "      <td>0.159495</td>\n",
       "      <td>0.313884</td>\n",
       "      <td>0.673148</td>\n",
       "      <td>0.263521</td>\n",
       "      <td>0.196907</td>\n",
       "      <td>0.375045</td>\n",
       "      <td>0.360181</td>\n",
       "      <td>0.598837</td>\n",
       "      <td>0.631248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.434035</td>\n",
       "      <td>0.425362</td>\n",
       "      <td>0.572821</td>\n",
       "      <td>0.287795</td>\n",
       "      <td>0.190909</td>\n",
       "      <td>0.403642</td>\n",
       "      <td>0.579643</td>\n",
       "      <td>0.313027</td>\n",
       "      <td>0.223028</td>\n",
       "      <td>0.448312</td>\n",
       "      <td>0.435004</td>\n",
       "      <td>0.535866</td>\n",
       "      <td>0.559106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.554349</td>\n",
       "      <td>0.517152</td>\n",
       "      <td>0.452618</td>\n",
       "      <td>0.370233</td>\n",
       "      <td>0.254850</td>\n",
       "      <td>0.554979</td>\n",
       "      <td>0.449653</td>\n",
       "      <td>0.389209</td>\n",
       "      <td>0.275640</td>\n",
       "      <td>0.574109</td>\n",
       "      <td>0.533126</td>\n",
       "      <td>0.411767</td>\n",
       "      <td>0.426787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.689436</td>\n",
       "      <td>0.629144</td>\n",
       "      <td>0.311113</td>\n",
       "      <td>0.469851</td>\n",
       "      <td>0.321038</td>\n",
       "      <td>0.713590</td>\n",
       "      <td>0.285331</td>\n",
       "      <td>0.487081</td>\n",
       "      <td>0.330574</td>\n",
       "      <td>0.713734</td>\n",
       "      <td>0.658332</td>\n",
       "      <td>0.274883</td>\n",
       "      <td>0.281913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.361359  0.346030  0.655493  0.245415  0.159495  0.313884  0.673148   \n",
       "1  0.434035  0.425362  0.572821  0.287795  0.190909  0.403642  0.579643   \n",
       "2  0.554349  0.517152  0.452618  0.370233  0.254850  0.554979  0.449653   \n",
       "3  0.689436  0.629144  0.311113  0.469851  0.321038  0.713590  0.285331   \n",
       "\n",
       "          7         8         9        10        11        12  \n",
       "0  0.263521  0.196907  0.375045  0.360181  0.598837  0.631248  \n",
       "1  0.313027  0.223028  0.448312  0.435004  0.535866  0.559106  \n",
       "2  0.389209  0.275640  0.574109  0.533126  0.411767  0.426787  \n",
       "3  0.487081  0.330574  0.713734  0.658332  0.274883  0.281913  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(statemean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_prob = np.array([transmat, transmat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array([[100, 50, 0, -50],[-50, 0, 50, 100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>-50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-50</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1   2    3\n",
       "0  100  50   0  -50\n",
       "1  -50   0  50  100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_prob = np.array([norm.pdf(statemean), norm.pdf(statemean)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.37372732, 0.37575921, 0.32181639, 0.3871075 , 0.39390016,\n",
       "         0.37976592, 0.31806404, 0.38532811, 0.3912828 , 0.37184888,\n",
       "         0.37388624, 0.33345696, 0.32687552],\n",
       "        [0.36308008, 0.36443575, 0.33857817, 0.38275836, 0.39173818,\n",
       "         0.36773161, 0.33724974, 0.37986797, 0.3891427 , 0.36080048,\n",
       "         0.36292724, 0.34558563, 0.34121652],\n",
       "        [0.34212125, 0.34900758, 0.36010121, 0.37251613, 0.38619504,\n",
       "         0.34200173, 0.36058317, 0.3698417 , 0.38407122, 0.33832807,\n",
       "         0.34609219, 0.36651544, 0.36421457],\n",
       "        [0.31455406, 0.32730932, 0.38009499, 0.35725039, 0.37890442,\n",
       "         0.30926892, 0.3830287 , 0.35431735, 0.3777291 , 0.30923712,\n",
       "         0.3212167 , 0.38415131, 0.38340012]],\n",
       "\n",
       "       [[0.37372732, 0.37575921, 0.32181639, 0.3871075 , 0.39390016,\n",
       "         0.37976592, 0.31806404, 0.38532811, 0.3912828 , 0.37184888,\n",
       "         0.37388624, 0.33345696, 0.32687552],\n",
       "        [0.36308008, 0.36443575, 0.33857817, 0.38275836, 0.39173818,\n",
       "         0.36773161, 0.33724974, 0.37986797, 0.3891427 , 0.36080048,\n",
       "         0.36292724, 0.34558563, 0.34121652],\n",
       "        [0.34212125, 0.34900758, 0.36010121, 0.37251613, 0.38619504,\n",
       "         0.34200173, 0.36058317, 0.3698417 , 0.38407122, 0.33832807,\n",
       "         0.34609219, 0.36651544, 0.36421457],\n",
       "        [0.31455406, 0.32730932, 0.38009499, 0.35725039, 0.37890442,\n",
       "         0.30926892, 0.3830287 , 0.35431735, 0.3777291 , 0.30923712,\n",
       "         0.3212167 , 0.38415131, 0.38340012]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.373727</td>\n",
       "      <td>0.375759</td>\n",
       "      <td>0.321816</td>\n",
       "      <td>0.387107</td>\n",
       "      <td>0.393900</td>\n",
       "      <td>0.379766</td>\n",
       "      <td>0.318064</td>\n",
       "      <td>0.385328</td>\n",
       "      <td>0.391283</td>\n",
       "      <td>0.371849</td>\n",
       "      <td>0.373886</td>\n",
       "      <td>0.333457</td>\n",
       "      <td>0.326876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.363080</td>\n",
       "      <td>0.364436</td>\n",
       "      <td>0.338578</td>\n",
       "      <td>0.382758</td>\n",
       "      <td>0.391738</td>\n",
       "      <td>0.367732</td>\n",
       "      <td>0.337250</td>\n",
       "      <td>0.379868</td>\n",
       "      <td>0.389143</td>\n",
       "      <td>0.360800</td>\n",
       "      <td>0.362927</td>\n",
       "      <td>0.345586</td>\n",
       "      <td>0.341217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.342121</td>\n",
       "      <td>0.349008</td>\n",
       "      <td>0.360101</td>\n",
       "      <td>0.372516</td>\n",
       "      <td>0.386195</td>\n",
       "      <td>0.342002</td>\n",
       "      <td>0.360583</td>\n",
       "      <td>0.369842</td>\n",
       "      <td>0.384071</td>\n",
       "      <td>0.338328</td>\n",
       "      <td>0.346092</td>\n",
       "      <td>0.366515</td>\n",
       "      <td>0.364215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.314554</td>\n",
       "      <td>0.327309</td>\n",
       "      <td>0.380095</td>\n",
       "      <td>0.357250</td>\n",
       "      <td>0.378904</td>\n",
       "      <td>0.309269</td>\n",
       "      <td>0.383029</td>\n",
       "      <td>0.354317</td>\n",
       "      <td>0.377729</td>\n",
       "      <td>0.309237</td>\n",
       "      <td>0.321217</td>\n",
       "      <td>0.384151</td>\n",
       "      <td>0.383400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.373727  0.375759  0.321816  0.387107  0.393900  0.379766  0.318064   \n",
       "1  0.363080  0.364436  0.338578  0.382758  0.391738  0.367732  0.337250   \n",
       "2  0.342121  0.349008  0.360101  0.372516  0.386195  0.342002  0.360583   \n",
       "3  0.314554  0.327309  0.380095  0.357250  0.378904  0.309269  0.383029   \n",
       "\n",
       "          7         8         9        10        11        12  \n",
       "0  0.385328  0.391283  0.371849  0.373886  0.333457  0.326876  \n",
       "1  0.379868  0.389143  0.360800  0.362927  0.345586  0.341217  \n",
       "2  0.369842  0.384071  0.338328  0.346092  0.366515  0.364215  \n",
       "3  0.354317  0.377729  0.309237  0.321217  0.384151  0.383400  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(e_prob[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[9.39222928e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 9.03650285e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 7.52301985e-03, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.15939399e-03,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         5.25336287e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 7.25147732e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 7.47566571e-03, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 6.12489466e-03,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         5.20408763e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 8.26741270e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 8.35239941e-03, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 8.29788669e-03,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         8.80366517e-03]],\n",
       "\n",
       "       [[1.01861792e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 9.28156607e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 7.72246955e-03, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.43289258e-03,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         4.82057749e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 7.89712224e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 7.97945367e-03, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.40106043e-03,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         4.65883204e-03, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 8.88081108e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 8.44466611e-03, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 8.65611542e-03,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         9.33824209e-03]],\n",
       "\n",
       "       [[1.40009544e+02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 1.40008539e+02, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 1.40006443e+02, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.40003269e+02,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.40002355e+02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 1.40007082e+02, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 1.40007193e+02, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.40003214e+02,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.40002089e+02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 1.40007751e+02, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 1.40007457e+02, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.40007880e+02,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         1.40008841e+02]],\n",
       "\n",
       "       [[5.20006315e+02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 5.20005382e+02, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 5.20004187e+02, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.20003155e+02,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         5.20001732e+02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 5.20005093e+02, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 5.20005046e+02, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.20003082e+02,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         5.20001612e+02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 5.20005065e+02, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 5.20004769e+02, 0.00000000e+00,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 5.20004997e+02,\n",
       "         0.00000000e+00],\n",
       "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         5.20005509e+02]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "covar=[covar[i].sum(axis=1) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.009392</td>\n",
       "      <td>0.009037</td>\n",
       "      <td>0.007523</td>\n",
       "      <td>0.006159</td>\n",
       "      <td>0.005253</td>\n",
       "      <td>0.007251</td>\n",
       "      <td>0.007476</td>\n",
       "      <td>0.006125</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>0.008267</td>\n",
       "      <td>0.008352</td>\n",
       "      <td>0.008298</td>\n",
       "      <td>0.008804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.010186</td>\n",
       "      <td>0.009282</td>\n",
       "      <td>0.007722</td>\n",
       "      <td>0.005433</td>\n",
       "      <td>0.004821</td>\n",
       "      <td>0.007897</td>\n",
       "      <td>0.007979</td>\n",
       "      <td>0.005401</td>\n",
       "      <td>0.004659</td>\n",
       "      <td>0.008881</td>\n",
       "      <td>0.008445</td>\n",
       "      <td>0.008656</td>\n",
       "      <td>0.009338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>140.009544</td>\n",
       "      <td>140.008539</td>\n",
       "      <td>140.006443</td>\n",
       "      <td>140.003269</td>\n",
       "      <td>140.002355</td>\n",
       "      <td>140.007082</td>\n",
       "      <td>140.007193</td>\n",
       "      <td>140.003214</td>\n",
       "      <td>140.002089</td>\n",
       "      <td>140.007751</td>\n",
       "      <td>140.007457</td>\n",
       "      <td>140.007880</td>\n",
       "      <td>140.008841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>520.006315</td>\n",
       "      <td>520.005382</td>\n",
       "      <td>520.004187</td>\n",
       "      <td>520.003155</td>\n",
       "      <td>520.001732</td>\n",
       "      <td>520.005093</td>\n",
       "      <td>520.005046</td>\n",
       "      <td>520.003082</td>\n",
       "      <td>520.001612</td>\n",
       "      <td>520.005065</td>\n",
       "      <td>520.004769</td>\n",
       "      <td>520.004997</td>\n",
       "      <td>520.005509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0           1           2           3           4           5  \\\n",
       "0    0.009392    0.009037    0.007523    0.006159    0.005253    0.007251   \n",
       "1    0.010186    0.009282    0.007722    0.005433    0.004821    0.007897   \n",
       "2  140.009544  140.008539  140.006443  140.003269  140.002355  140.007082   \n",
       "3  520.006315  520.005382  520.004187  520.003155  520.001732  520.005093   \n",
       "\n",
       "            6           7           8           9          10          11  \\\n",
       "0    0.007476    0.006125    0.005204    0.008267    0.008352    0.008298   \n",
       "1    0.007979    0.005401    0.004659    0.008881    0.008445    0.008656   \n",
       "2  140.007193  140.003214  140.002089  140.007751  140.007457  140.007880   \n",
       "3  520.005046  520.003082  520.001612  520.005065  520.004769  520.004997   \n",
       "\n",
       "           12  \n",
       "0    0.008804  \n",
       "1    0.009338  \n",
       "2  140.008841  \n",
       "3  520.005509  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(covar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: no-repair, 1: repair\n",
    "actions = ('0', '1')\n",
    "# 0: failing, 1: low health, 2: good health, 3: perfect health\n",
    "states = ('0', '1', '2', '3')\n",
    "\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First we define an MDP. We also represent a policy\n",
    "as a dictionary of {state: action} pairs, and a utility function as a\n",
    "dictionary of {state: number} pairs. We then define the value_iteration\n",
    "and policy_iteration algorithms.\"\"\"\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class MDP:\n",
    "\n",
    "    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n",
    "    and reward function. We also keep track of a gamma value, for use by\n",
    "    algorithms. The transition model is represented somewhat differently from\n",
    "    the text. Instead of P(s' | s, a) being a probability number for each\n",
    "    state/state/action triplet, we instead have T(s, a) return a\n",
    "    list of (p, s') pairs. We also keep track of the possible states,\n",
    "    terminal states, and actions for each state.\"\"\"\n",
    "\n",
    "    def __init__(self, init, actlist, terminals, transitions=None, reward=None, states=None, gamma=0.9):\n",
    "        if not (0 < gamma <= 1):\n",
    "            raise ValueError(\"An MDP must have 0 < gamma <= 1\")\n",
    "\n",
    "        # collect states from transitions table if not passed.\n",
    "        self.states = states or self.get_states_from_transitions(transitions)\n",
    "            \n",
    "        self.init = init\n",
    "        \n",
    "        if isinstance(actlist, list):\n",
    "            # if actlist is a list, all states have the same actions\n",
    "            self.actlist = actlist\n",
    "\n",
    "        elif isinstance(actlist, dict):\n",
    "            # if actlist is a dict, different actions for each state\n",
    "            self.actlist = actlist\n",
    "        \n",
    "        self.terminals = terminals\n",
    "        self.transitions = transitions or {}\n",
    "        if not self.transitions:\n",
    "            print(\"Warning: Transition table is empty.\")\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.reward = reward or {s: 0 for s in self.states}\n",
    "\n",
    "        # self.check_consistency()\n",
    "\n",
    "    def R(self, state):\n",
    "        \"\"\"Return a numeric reward for this state.\"\"\"\n",
    "\n",
    "        return self.reward[state]\n",
    "\n",
    "    def T(self, state, action):\n",
    "        \"\"\"Transition model. From a state and an action, return a list\n",
    "        of (probability, result-state) pairs.\"\"\"\n",
    "\n",
    "        if not self.transitions:\n",
    "            raise ValueError(\"Transition model is missing\")\n",
    "        else:\n",
    "            return self.transitions[state][action]\n",
    "\n",
    "    def actions(self, state):\n",
    "        \"\"\"Return a list of actions that can be performed in this state. By default, a\n",
    "        fixed list of actions, except for terminal states. Override this\n",
    "        method if you need to specialize by state.\"\"\"\n",
    "\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.actlist\n",
    "\n",
    "    def get_states_from_transitions(self, transitions):\n",
    "        if isinstance(transitions, dict):\n",
    "            s1 = set(transitions.keys())\n",
    "            s2 = set(tr[1] for actions in transitions.values()\n",
    "                     for effects in actions.values()\n",
    "                     for tr in effects)\n",
    "            return s1.union(s2)\n",
    "        else:\n",
    "            print('Could not retrieve states from transitions')\n",
    "            return None\n",
    "\n",
    "    def check_consistency(self):\n",
    "\n",
    "        # check that all states in transitions are valid\n",
    "        assert set(self.states) == self.get_states_from_transitions(self.transitions)\n",
    "\n",
    "        # check that init is a valid state\n",
    "        assert self.init in self.states\n",
    "\n",
    "        # check reward for each state\n",
    "        assert set(self.reward.keys()) == set(self.states)\n",
    "\n",
    "        # check that all terminals are valid states\n",
    "        assert all(t in self.states for t in self.terminals)\n",
    "\n",
    "        # check that probability distributions for all actions sum to 1\n",
    "        for s1, actions in self.transitions.items():\n",
    "            for a in actions.keys():\n",
    "                s = 0\n",
    "                for o in actions[a]:\n",
    "                    s += o[0]\n",
    "                assert abs(s - 1) < 0.001\n",
    "\n",
    "class POMDP(MDP):\n",
    "\n",
    "    \"\"\"A Partially Observable Markov Decision Process, defined by\n",
    "    a transition model P(s'|s,a), actions A(s), a reward function R(s),\n",
    "    and a sensor model P(e|s). We also keep track of a gamma value,\n",
    "    for use by algorithms. The transition and the sensor models\n",
    "    are defined as matrices. We also keep track of the possible states\n",
    "    and actions for each state.\"\"\"\n",
    "\n",
    "    def __init__(self, actions, transitions=None, evidences=None, rewards=None, states=None, gamma=0.95):\n",
    "        \"\"\"Initialize variables of the pomdp\"\"\"\n",
    "\n",
    "        if not (0 < gamma <= 1):\n",
    "            raise ValueError('A POMDP must have 0 < gamma <= 1')\n",
    "\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "\n",
    "        # transition model cannot be undefined\n",
    "        self.t_prob = transitions\n",
    "        if not self.t_prob.any():\n",
    "            print('Warning: Transition model is undefined')\n",
    "        \n",
    "        # sensor model cannot be undefined\n",
    "        self.e_prob = evidences\n",
    "        if not self.e_prob.any():\n",
    "            print('Warning: Sensor model is undefined')\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.rewards = rewards\n",
    "\n",
    "    def remove_dominated_plans(self, input_values):\n",
    "        \"\"\"\n",
    "        Remove dominated plans.\n",
    "        This method finds all the lines contributing to the\n",
    "        upper surface and removes those which don't.\n",
    "        \"\"\"\n",
    "\n",
    "        values = [val for action in input_values for val in input_values[action]]\n",
    "        values.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        best = [values[0]]\n",
    "        y1_max = max(val[1] for val in values)\n",
    "        tgt = values[0]\n",
    "        prev_b = 0\n",
    "        prev_ix = 0\n",
    "        while tgt[1] != y1_max:\n",
    "            min_b = 1\n",
    "            min_ix = 0\n",
    "            for i in range(prev_ix + 1, len(values)):\n",
    "                if values[i][0] - tgt[0] + tgt[1] - values[i][1] != 0:\n",
    "                    trans_b = (values[i][0] - tgt[0]) / (values[i][0] - tgt[0] + tgt[1] - values[i][1])\n",
    "                    if 0 <= trans_b <= 1 and trans_b > prev_b and trans_b < min_b:\n",
    "                        min_b = trans_b\n",
    "                        min_ix = i\n",
    "            prev_b = min_b\n",
    "            prev_ix = min_ix\n",
    "            tgt = values[min_ix]\n",
    "            best.append(tgt)\n",
    "\n",
    "        return self.generate_mapping(best, input_values)\n",
    "\n",
    "    def remove_dominated_plans_fast(self, input_values):\n",
    "        \"\"\"\n",
    "        Remove dominated plans using approximations.\n",
    "        Resamples the upper boundary at intervals of 100 and\n",
    "        finds the maximum values at these points.\n",
    "        \"\"\"\n",
    "\n",
    "        values = [val for action in input_values for val in input_values[action]]\n",
    "        values.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        best = []\n",
    "        sr = 100\n",
    "        for i in range(sr + 1):\n",
    "            x = i / float(sr)\n",
    "            maximum = (values[0][1] - values[0][0]) * x + values[0][0]\n",
    "            tgt = values[0]\n",
    "            for value in values:\n",
    "                val = (value[1] - value[0]) * x + value[0]\n",
    "                if val > maximum:\n",
    "                    maximum = val\n",
    "                    tgt = value\n",
    "\n",
    "            if all(any(tgt != v) for v in best):\n",
    "                best.append(np.array(tgt))\n",
    "\n",
    "        return self.generate_mapping(best, input_values)\n",
    "\n",
    "    def generate_mapping(self, best, input_values):\n",
    "        \"\"\"Generate mappings after removing dominated plans\"\"\"\n",
    "\n",
    "        mapping = defaultdict(list)\n",
    "        for value in best:\n",
    "            for action in input_values:\n",
    "                if any(all(value == v) for v in input_values[action]):\n",
    "                    mapping[action].append(value)\n",
    "\n",
    "        return mapping\n",
    "\n",
    "    def max_difference(self, U1, U2):\n",
    "        \"\"\"Find maximum difference between two utility mappings\"\"\"\n",
    "\n",
    "        for k, v in U1.items():\n",
    "            sum1 = 0\n",
    "            for element in U1[k]:\n",
    "                sum1 += sum(element)\n",
    "            sum2 = 0\n",
    "            for element in U2[k]:\n",
    "                sum2 += sum(element)\n",
    "        return abs(sum1 - sum2)\n",
    "\n",
    "        \n",
    "class Matrix:\n",
    "    \"\"\"Matrix operations class\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def add(A, B):\n",
    "        \"\"\"Add two matrices A and B\"\"\"\n",
    "\n",
    "        res = []\n",
    "        for i in range(len(A)):\n",
    "            row = []\n",
    "            for j in range(len(A[0])):\n",
    "                row.append(A[i][j] + B[i][j])\n",
    "            res.append(row)\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def scalar_multiply(a, B):\n",
    "        \"\"\"Multiply scalar a to matrix B\"\"\"\n",
    "\n",
    "        for i in range(len(B)):\n",
    "            for j in range(len(B[0])):\n",
    "                B[i][j] = a * B[i][j]\n",
    "        return B\n",
    "\n",
    "    @staticmethod\n",
    "    def multiply(A, B):\n",
    "        \"\"\"Multiply two matrices A and B element-wise\"\"\"\n",
    "\n",
    "        matrix = []\n",
    "        for i in range(len(B)):\n",
    "            row = []\n",
    "            for j in range(len(B[0])):\n",
    "                row.append(B[i][j] * A[j][i])\n",
    "            matrix.append(row)\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def matmul(A, B):\n",
    "        \"\"\"Inner-product of two matrices\"\"\"\n",
    "\n",
    "        return [[sum(ele_a*ele_b for ele_a, ele_b in zip(row_a, col_b)) for col_b in list(zip(*B))] for row_a in A]\n",
    "\n",
    "    @staticmethod\n",
    "    def transpose(A):\n",
    "        \"\"\"Transpose a matrix\"\"\"\n",
    "        \n",
    "        return [list(i) for i in zip(*A)]\n",
    "\n",
    "\n",
    "def pomdp_value_iteration(pomdp, epsilon=0.1):\n",
    "    \"\"\"Solving a POMDP by value iteration.\"\"\"\n",
    "\n",
    "    U = {'':[[0]* len(pomdp.states)]}\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        prev_U = U\n",
    "        values = [val for action in U for val in U[action]]\n",
    "        value_matxs = []\n",
    "        for i in values:\n",
    "            for j in values:\n",
    "                value_matxs.append([i, j])\n",
    "\n",
    "        U1 = defaultdict(list)\n",
    "        for action in pomdp.actions:\n",
    "            for u in value_matxs:\n",
    "                u1 = Matrix.matmul(Matrix.matmul(pomdp.t_prob[int(action)], Matrix.multiply(pomdp.e_prob[int(action)], Matrix.transpose(u))), [[1], [1]])\n",
    "                u1 = Matrix.add(Matrix.scalar_multiply(pomdp.gamma, Matrix.transpose(u1)), [pomdp.rewards[int(action)]])\n",
    "                U1[action].append(u1[0])\n",
    "\n",
    "        U = pomdp.remove_dominated_plans_fast(U1)\n",
    "        # replace with U = pomdp.remove_dominated_plans(U1) for accurate calculations\n",
    "        \n",
    "        if count > 10:\n",
    "            if pomdp.max_difference(U, prev_U) < epsilon * (1 - pomdp.gamma) / pomdp.gamma:\n",
    "                return U\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0', '1', '2', '3')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pomdp = POMDP(actions, t_prob, e_prob, rewards, states, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'0': [array([ 204.52516018,   88.9820356 ,  -34.29024281, -171.26528113])]})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utility = pomdp_value_iteration(pomdp, epsilon=3)\n",
    "utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>204.525160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>88.982036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-34.290243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-171.265281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0  204.525160\n",
       "1   88.982036\n",
       "2  -34.290243\n",
       "3 -171.265281"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.array([204.52516018,   88.9820356 ,  -34.29024281, -171.26528113]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADQRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These lines establish the feed-forward part of the network used to choose actions\n",
    "inputs1 = tf.placeholder(shape=[1,13],dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_uniform([13,2],0,0.01))\n",
    "Qout = tf.matmul(inputs1,W)\n",
    "predict = tf.argmax(Qout,1)\n",
    "\n",
    "#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "nextQ = tf.placeholder(shape=[1,2],dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(obs):\n",
    "    state = 0\n",
    "    diff = 16\n",
    "    for i in range(len(statemean)):\n",
    "        stateDiff = obs - statemean[i]\n",
    "        stateDiffVal = np.sqrt(np.mean(stateDiff**2))\n",
    "        if stateDiffVal < diff:\n",
    "            diff = stateDiffVal\n",
    "            state = i\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the next step after an action is done\n",
    "\n",
    "def getStepDetails(i,j,action):\n",
    "    unitData = dataT_cycles[i]\n",
    "    d = False\n",
    "    if action == 1:\n",
    "        newJ = 0\n",
    "    else:\n",
    "        newJ = j+1\n",
    "    obsNext = unitData.values[newJ]\n",
    "    if newJ >= len(unitData) - 1:\n",
    "        d = True\n",
    "    s1 = get_state(obsNext)\n",
    "    r1 = rewards[action][s1]\n",
    "    return r1,newJ,s1,obsNext,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set learning parameters\n",
    "init = tf.global_variables_initializer()\n",
    "y = gamma\n",
    "e = 0.1\n",
    "num_episodes = len(dataT_cycles)\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "D = np.empty([0,5]) # Replay memory\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation for new unit\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        k = 0\n",
    "        unitData = dataT_cycles[i]\n",
    "        #The Q-Network\n",
    "        while j < len(unitData):\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            a,allQ = sess.run([predict,Qout],feed_dict={inputs1:unitData.values[j].reshape(1,13)})\n",
    "            if np.random.rand(1) < e:\n",
    "                a[0] = np.random.randint(0,2)\n",
    "            #Get new state and reward from environment\n",
    "            r,j,s1,o1,d = getStepDetails(i,j,a[0])\n",
    "            D = np.vstack([D, [a[0],unitData.values[j-1].reshape(1,13),r,o1,s1]])\n",
    "            if len(D) > 20:\n",
    "                lastInd = np.random.randint(15,len(D))\n",
    "                randomSample = D[lastInd-15:lastInd]\n",
    "                finalO = D[lastInd,3].reshape(1,13)\n",
    "                Reward = np.sum(D[lastInd-15:lastInd,2])\n",
    "            else:\n",
    "                finalO = o1.reshape(1,13)\n",
    "                Reward = r\n",
    "            # We take batch size of 15 (j in algorithm)\n",
    "            #Obtain the Q' values by feeding the new state through our network\n",
    "            Q1 = sess.run(Qout,feed_dict={inputs1:finalO})\n",
    "            #Obtain maxQ' and set our target value for chosen action.\n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            targetQ[0,a[0]] = Reward + y*maxQ1\n",
    "            #Train our network using target and predicted Q values\n",
    "            _,W1 = sess.run([updateModel,W],feed_dict={inputs1:unitData.values[j-1].reshape(1,13),nextQ:targetQ})\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            k += 1\n",
    "            if d == True or k >= 1000:\n",
    "                #Reduce chance of random action as we train the model.\n",
    "                e = 1./((i/50) + 10)\n",
    "                break\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dataT_cycles[5].values[160].reshape(-1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77108434, 0.48484848, 0.40257649, 0.51515152, 0.03688414,\n",
       "        0.61904762, 0.36886994, 0.55882353, 0.03333677, 0.65101962,\n",
       "        0.58333333, 0.30232558, 0.2903894 ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataT_cycles[5].values[160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(a,W1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
