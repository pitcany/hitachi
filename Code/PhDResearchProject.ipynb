{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import norm\n",
    "import pomegranate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data and diving them into unique unit numbers\n",
    "\n",
    "We need to divide data into unique numbers, because the state restes as the unit number changes, so we need to find Gaussian distribution for different unit numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('~/Documents/hitachi/CMAPSS/train_FD001.txt', sep=\" \", header=None)\n",
    "unique_unit_values = data[0].unique() #Number of units\n",
    "data_cycles = []\n",
    "for unit_num in unique_unit_values:\n",
    "    data_cycles.append(data[data[0] == unit_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing operational settings and normalize the data column wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    x = data.values\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    dataNew = pd.DataFrame(x_scaled)\n",
    "    return dataNew\n",
    "#Remove the operation settings\n",
    "dataT = data[data.columns[5:26]]\n",
    "dataT.columns = range(21)\n",
    "dataT = normalize(dataT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividing data for each unit\n",
    "\n",
    "I think this is why my transitional matrix previously was not working properly as in each unit the state resets and start from good condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataT_cycles = []\n",
    "for unit_num in unique_unit_values:\n",
    "    dataT_cycles.append(dataT[data[0] == unit_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying and removing non variable data columns\n",
    "\n",
    "Removing the columns where the data does not vary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 5, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n",
      "Int64Index([0, 4, 9, 15, 17, 18], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "for dataT_cycle in dataT_cycles:\n",
    "    print(dataT_cycle.columns[dataT_cycle.std() == 0])\n",
    "\"\"\"\n",
    "Here we can see 0,4,9,15,17,18 but also 5 at many places so we drop column number 5 as well\n",
    "\"\"\"\n",
    "dataT.drop(data.columns[[0, 3, 4, 5, 9, 15, 17, 18]],axis=1,inplace=True)\n",
    "dataT.columns = range(13)\n",
    "dataT_cycles = []\n",
    "for unit_num in unique_unit_values:\n",
    "    dataT_cycles.append(dataT[data[0] == unit_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 13)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Right now only using the first data frame (i.e Machine 1) to train the VAE, but we can combine all the dataframes\n",
    "# and train the VAE jointly on the entire data for better performance \n",
    "\n",
    "x_train = dataT_cycles[0].values[:150]\n",
    "x_test = dataT_cycles[0].values[151:198]\n",
    "x_train.shape\n",
    "# x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational AutoEncoders to find Latent State Space Distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "x_train = dataT_cycles[0].values[:100]\n",
    "x_test = dataT_cycles[0].values[101:198]\n",
    "x_train.shape\n",
    "x_test.shape\n",
    "original_dim = x_train[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "input_shape = (original_dim, )\n",
    "intermediate_dim = 9\n",
    "batch_size = 10\n",
    "latent_dim = 5\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample eps = N(0,I)\n",
    "# z = z_mean + sqrt(var)*eps\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    \n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 13)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 9)            126         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 5)            50          dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 5)            50          dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 5)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 226\n",
      "Trainable params: 226\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# VAE Model Encoder + Decoder \n",
    "\n",
    "# Building the Encoder\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 9)                 54        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 13)                130       \n",
      "=================================================================\n",
      "Total params: 184\n",
      "Trainable params: 184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build decoder model \n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    help_ = \"Load h5 model trained weights\"\n",
    "    parser.add_argument(\"-w\", \"--weights\", help=help_)\n",
    "    help_ = \"Use mse loss instead of binary cross entropy (default)\"\n",
    "    parser.add_argument(\"-m\", \"--mse\", help=help_, action='store_true')\n",
    "    \n",
    "    models = (encoder, decoder)\n",
    "    data = (x_test, None)\n",
    "    \n",
    "    # VAE loss = mse_loss or xent_loss + kl_loss\n",
    "    if args.mse:\n",
    "        reconstruction_loss = mse(inputs, outputs)\n",
    "    else:\n",
    "        reconstruction_loss = binary_crossentropy(inputs, outputs)\n",
    "        \n",
    "    reconstruction_loss *= original_dim\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis= -1)\n",
    "    kl_loss *= 0.5 \n",
    "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "    vae.add_loss(vae_loss)\n",
    "    vae.compile(optimizer='adam')\n",
    "    vae.summary()\n",
    "    \n",
    "    if args.weights:\n",
    "        vae.load_weights(args.weights)\n",
    "    else:\n",
    "        # Train the autoencoder\n",
    "        vae.fit(x_train, epochs=epochs, batch_size= batch_size, validation_data=(x_test, None))\n",
    "        vae.save_weights('vae_mlp_CMAPSS.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae_mlp\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 13)                0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 5), (None, 5), (N 226       \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 13)                184       \n",
      "=================================================================\n",
      "Total params: 410\n",
      "Trainable params: 410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 100 samples, validate on 91 samples\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 0s 4ms/step - loss: 9.3237 - val_loss: 9.0488\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 0s 140us/step - loss: 9.1351 - val_loss: 8.9685\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 0s 132us/step - loss: 8.7703 - val_loss: 8.8599\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 0s 158us/step - loss: 8.4681 - val_loss: 8.6881\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 0s 152us/step - loss: 8.0217 - val_loss: 8.3543\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 0s 130us/step - loss: 7.5298 - val_loss: 7.9555\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 0s 154us/step - loss: 6.9453 - val_loss: 7.4310\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 0s 151us/step - loss: 6.1665 - val_loss: 6.6121\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 0s 157us/step - loss: 5.1903 - val_loss: 5.7473\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 0s 156us/step - loss: 3.9731 - val_loss: 4.4611\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 0s 135us/step - loss: 2.4871 - val_loss: 2.9702\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 0s 151us/step - loss: 0.6601 - val_loss: 1.0742\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 0s 155us/step - loss: -1.5492 - val_loss: -1.2917\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 0s 165us/step - loss: -4.2151 - val_loss: -4.1132\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 0s 200us/step - loss: -7.3532 - val_loss: -7.4566\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 0s 138us/step - loss: -11.0672 - val_loss: -11.4704\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 0s 141us/step - loss: -15.3882 - val_loss: -16.2186\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 0s 125us/step - loss: -20.4541 - val_loss: -21.7731\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 0s 163us/step - loss: -26.2994 - val_loss: -28.2361\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 0s 117us/step - loss: -33.0558 - val_loss: -35.8064\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 0s 116us/step - loss: -40.8111 - val_loss: -44.5319\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 0s 153us/step - loss: -49.7432 - val_loss: -54.5473\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 0s 119us/step - loss: -59.8478 - val_loss: -65.9639\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 0s 140us/step - loss: -71.3664 - val_loss: -78.9972\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 0s 146us/step - loss: -84.3942 - val_loss: -93.6540\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 0s 150us/step - loss: -99.0032 - val_loss: -110.1178\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 0s 153us/step - loss: -115.3944 - val_loss: -128.6723\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 0s 134us/step - loss: -133.6656 - val_loss: -149.3538\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 0s 174us/step - loss: -154.0564 - val_loss: -172.3889\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 0s 155us/step - loss: -176.6753 - val_loss: -197.8542\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 0s 167us/step - loss: -201.6378 - val_loss: -226.0516\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 0s 141us/step - loss: -229.2158 - val_loss: -257.0855\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 0s 139us/step - loss: -259.3665 - val_loss: -291.2995\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 0s 145us/step - loss: -292.6659 - val_loss: -328.7502\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 0s 145us/step - loss: -328.8985 - val_loss: -369.6788\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 0s 141us/step - loss: -368.3565 - val_loss: -414.4307\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 0s 154us/step - loss: -411.4577 - val_loss: -463.0646\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 0s 129us/step - loss: -458.2200 - val_loss: -515.7527\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 0s 142us/step - loss: -508.8015 - val_loss: -572.9309\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 0s 145us/step - loss: -563.6350 - val_loss: -634.6259\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 0s 132us/step - loss: -622.5677 - val_loss: -701.2246\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 0s 128us/step - loss: -686.1298 - val_loss: -773.0123\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 0s 154us/step - loss: -754.5213 - val_loss: -850.0337\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 0s 135us/step - loss: -827.5266 - val_loss: -932.8984\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 0s 115us/step - loss: -906.2504 - val_loss: -1021.3450\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 0s 145us/step - loss: -990.1081 - val_loss: -1116.1556\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 0s 140us/step - loss: -1079.5034 - val_loss: -1217.3660\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 0s 138us/step - loss: -1175.3159 - val_loss: -1325.0027\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 0s 154us/step - loss: -1276.9609 - val_loss: -1439.7038\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 0s 126us/step - loss: -1385.2192 - val_loss: -1561.4682\n"
     ]
    }
   ],
   "source": [
    "class Args:\n",
    "    mse = None\n",
    "    weights = None\n",
    "    \n",
    "args = Args()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the VAE has been trained, we can use the encoder to sample the latent space\n",
    "\n",
    "#predicting the latent space for first 13 observation values for machine 1\n",
    "test = np.asarray(x_train[0:13])  \n",
    "\n",
    "# indexing on 2 because the encoder predicts z_mean, z_log_var and sampled vector z (we are interested in z only)\n",
    "latent_space = encoder.predict(test)[2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 27.251003, -20.169312,  22.938713, -29.368395,  19.887693],\n",
       "       [ 28.004028, -20.717808,  23.52812 , -30.190313,  20.401749],\n",
       "       [ 26.674105, -19.824026,  22.48051 , -28.758377,  19.549667],\n",
       "       [ 26.771925, -19.89632 ,  22.58438 , -28.8585  ,  19.634975],\n",
       "       [ 27.21871 , -20.129292,  22.902697, -29.331566,  19.845766],\n",
       "       [ 25.162264, -18.688475,  21.239565, -27.113237,  18.45327 ],\n",
       "       [ 27.825731, -20.64691 ,  23.433723, -29.999489,  20.355967],\n",
       "       [ 25.292551, -18.82446 ,  21.323738, -27.268307,  18.56439 ],\n",
       "       [ 25.6682  , -18.988995,  21.635544, -27.64892 ,  18.744232],\n",
       "       [ 26.223043, -19.250334,  22.048712, -28.224167,  18.99528 ],\n",
       "       [ 26.14768 , -19.279438,  22.037632, -28.152092,  19.042107],\n",
       "       [ 25.701624, -19.079872,  21.717144, -27.687557,  18.856134],\n",
       "       [ 26.871473, -19.978725,  22.61129 , -28.98185 ,  19.678198]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each list is a 5 dimension latent state space for that observation value\n",
    "latent_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the raw observation from the learned latent space \n",
    "sample = decoder.predict(latent_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18373494, 0.40680183, 0.72624799, 0.24242424, 0.109755  ,\n",
       "       0.36904762, 0.63326226, 0.20588235, 0.1996078 , 0.36398615,\n",
       "       0.33333333, 0.71317829, 0.7246617 ])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the with the real x_train value\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.31233034, 0.32204977, 0.68397444, 0.24470285, 0.13417616,\n",
       "       0.27928537, 0.69099057, 0.25210077, 0.17723036, 0.3619346 ,\n",
       "       0.32728148, 0.63931286, 0.6711073 ], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Right now they are not same as we trained the VAE on very less amount of data \n",
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using HMM to find out transitional matrices\n",
    "\n",
    "Here we first define transmatrix as [[0.5, 0.5, 0.0, 0.0],[0.0, 0.5, 0.5, 0.0],[0.0, 0.0, 0.5, 0.5],[0.0,0.0,0.0,1.0]] which means there is half chance for each state to go to next state and half to remain in the current state itself.\n",
    "\n",
    "Then we train for each unit for transmatrix as well as state means and we will take average of each unit transmatrices and states as the transmatrix and state\n",
    "\n",
    "*Note*: Here state '0' means the perfect health and '3' means weakest health "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = hmm.GaussianHMM(n_components=4, covariance_type=\"diag\",init_params=\"cm\", params=\"mtc\")\n",
    "lr.startprob_ = np.array([1.0, 0.0, 0.0, 0.0])\n",
    "transmats = []\n",
    "statemeans = []\n",
    "for i in range(100):\n",
    "    lr.transmat_ = np.array([[0.5, 0.5, 0.0, 0.0],[0.0, 0.5, 0.5, 0.0],[0.0, 0.0, 0.5, 0.5],[0.0,0.0,0.0,1.0]])\n",
    "    lr.fit(dataT_cycles[i])\n",
    "    transmat = lr.transmat_\n",
    "    transmats.append(transmat)\n",
    "    statemeans.append(lr.means_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yannik/miniconda3/envs/ykp/lib/python3.7/site-packages/hmmlearn/hmm.py:929: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  new_cov = new_cov_numer / new_cov_denom\n",
      "/home/yannik/miniconda3/envs/ykp/lib/python3.7/site-packages/hmmlearn/hmm.py:929: RuntimeWarning: invalid value encountered in true_divide\n",
      "  new_cov = new_cov_numer / new_cov_denom\n",
      "/home/yannik/miniconda3/envs/ykp/lib/python3.7/site-packages/hmmlearn/stats.py:47: RuntimeWarning: divide by zero encountered in log\n",
      "  + np.dot(X ** 2, (1.0 / covars).T))\n",
      "/home/yannik/miniconda3/envs/ykp/lib/python3.7/site-packages/hmmlearn/stats.py:47: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  + np.dot(X ** 2, (1.0 / covars).T))\n",
      "/home/yannik/miniconda3/envs/ykp/lib/python3.7/site-packages/hmmlearn/stats.py:47: RuntimeWarning: invalid value encountered in add\n",
      "  + np.dot(X ** 2, (1.0 / covars).T))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "mixture weights must sum up to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-88007512b356>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransmat_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataT_cycles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtransmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransmat_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtransmats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ykp/lib/python3.7/site-packages/hmmlearn/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, lengths)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvergenceMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ykp/lib/python3.7/site-packages/hmmlearn/hmm.py\u001b[0m in \u001b[0;36m_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    724\u001b[0m         if not np.allclose(np.sum(self.weights_, axis=1),\n\u001b[1;32m    725\u001b[0m                            np.ones(self.n_components)):\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mixture weights must sum up to 1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;31m# Checking means' shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mixture weights must sum up to 1"
     ]
    }
   ],
   "source": [
    "lr = hmm.GMMHMM(n_components=4, n_mix=4, covariance_type=\"diag\",init_params=\"cm\", params=\"mt\")\n",
    "lr.startprob_ = np.array([1.0, 0.0, 0.0, 0.0])\n",
    "transmats = []\n",
    "statemeans = []\n",
    "for i in range(100):\n",
    "    lr.transmat_ = np.array([[0.5, 0.5, 0.0, 0.0],[0.0, 0.5, 0.5, 0.0],[0.0, 0.0, 0.5, 0.5],[0.0,0.0,0.0,1.0]])\n",
    "    lr.fit(dataT_cycles[i])\n",
    "    transmat = lr.transmat_\n",
    "    transmats.append(transmat)\n",
    "    statemeans.append(lr.means_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transmat = np.array(transmats).mean(axis=0)\n",
    "statemean = np.array(statemeans).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.51835732, 0.48164268, 0.        , 0.        ],\n",
       "       [0.        , 0.7002123 , 0.2997877 , 0.        ],\n",
       "       [0.        , 0.        , 0.87145676, 0.12854324],\n",
       "       [0.        , 0.        , 0.        , 0.98      ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.366748</td>\n",
       "      <td>0.360500</td>\n",
       "      <td>0.648286</td>\n",
       "      <td>0.248550</td>\n",
       "      <td>0.159554</td>\n",
       "      <td>0.315721</td>\n",
       "      <td>0.675591</td>\n",
       "      <td>0.266175</td>\n",
       "      <td>0.197184</td>\n",
       "      <td>0.374635</td>\n",
       "      <td>0.351366</td>\n",
       "      <td>0.605654</td>\n",
       "      <td>0.633334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.436753</td>\n",
       "      <td>0.415611</td>\n",
       "      <td>0.571987</td>\n",
       "      <td>0.290103</td>\n",
       "      <td>0.197851</td>\n",
       "      <td>0.399325</td>\n",
       "      <td>0.588434</td>\n",
       "      <td>0.308757</td>\n",
       "      <td>0.228241</td>\n",
       "      <td>0.445890</td>\n",
       "      <td>0.427247</td>\n",
       "      <td>0.527394</td>\n",
       "      <td>0.550402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.536725</td>\n",
       "      <td>0.506111</td>\n",
       "      <td>0.471711</td>\n",
       "      <td>0.360246</td>\n",
       "      <td>0.247191</td>\n",
       "      <td>0.525837</td>\n",
       "      <td>0.477683</td>\n",
       "      <td>0.375745</td>\n",
       "      <td>0.268967</td>\n",
       "      <td>0.550086</td>\n",
       "      <td>0.517278</td>\n",
       "      <td>0.435316</td>\n",
       "      <td>0.449778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.679415</td>\n",
       "      <td>0.624325</td>\n",
       "      <td>0.330025</td>\n",
       "      <td>0.472313</td>\n",
       "      <td>0.315224</td>\n",
       "      <td>0.689620</td>\n",
       "      <td>0.312354</td>\n",
       "      <td>0.488682</td>\n",
       "      <td>0.325557</td>\n",
       "      <td>0.688270</td>\n",
       "      <td>0.639782</td>\n",
       "      <td>0.296632</td>\n",
       "      <td>0.302288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.366748  0.360500  0.648286  0.248550  0.159554  0.315721  0.675591   \n",
       "1  0.436753  0.415611  0.571987  0.290103  0.197851  0.399325  0.588434   \n",
       "2  0.536725  0.506111  0.471711  0.360246  0.247191  0.525837  0.477683   \n",
       "3  0.679415  0.624325  0.330025  0.472313  0.315224  0.689620  0.312354   \n",
       "\n",
       "          7         8         9        10        11        12  \n",
       "0  0.266175  0.197184  0.374635  0.351366  0.605654  0.633334  \n",
       "1  0.308757  0.228241  0.445890  0.427247  0.527394  0.550402  \n",
       "2  0.375745  0.268967  0.550086  0.517278  0.435316  0.449778  \n",
       "3  0.488682  0.325557  0.688270  0.639782  0.296632  0.302288  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(statemean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_prob = np.array([transmat, transmat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array([[100, 50, 0, -50],[-50, 0, 50, 100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_prob = np.array([norm.pdf(statemean), norm.pdf(statemean)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.3729949 , 0.37384322, 0.32333181, 0.38680786, 0.39389644,\n",
       "         0.37954642, 0.31754041, 0.38505735, 0.39126148, 0.3719059 ,\n",
       "         0.37506058, 0.3320907 , 0.3264447 ],\n",
       "        [0.36265064, 0.36593308, 0.33873974, 0.38250316, 0.3912099 ,\n",
       "         0.36836958, 0.33552258, 0.38037257, 0.38868521, 0.36119137,\n",
       "         0.364143  , 0.34714575, 0.34286804],\n",
       "        [0.34542646, 0.3509846 , 0.35693762, 0.37387749, 0.3869382 ,\n",
       "         0.34743041, 0.35592727, 0.37175108, 0.38476971, 0.34292754,\n",
       "         0.34898477, 0.362878  , 0.36056297],\n",
       "        [0.31671883, 0.32829941, 0.37779757, 0.35683622, 0.3796059 ,\n",
       "         0.31451413, 0.37994797, 0.35404071, 0.37835124, 0.31480677,\n",
       "         0.32510764, 0.38177117, 0.38112516]],\n",
       "\n",
       "       [[0.3729949 , 0.37384322, 0.32333181, 0.38680786, 0.39389644,\n",
       "         0.37954642, 0.31754041, 0.38505735, 0.39126148, 0.3719059 ,\n",
       "         0.37506058, 0.3320907 , 0.3264447 ],\n",
       "        [0.36265064, 0.36593308, 0.33873974, 0.38250316, 0.3912099 ,\n",
       "         0.36836958, 0.33552258, 0.38037257, 0.38868521, 0.36119137,\n",
       "         0.364143  , 0.34714575, 0.34286804],\n",
       "        [0.34542646, 0.3509846 , 0.35693762, 0.37387749, 0.3869382 ,\n",
       "         0.34743041, 0.35592727, 0.37175108, 0.38476971, 0.34292754,\n",
       "         0.34898477, 0.362878  , 0.36056297],\n",
       "        [0.31671883, 0.32829941, 0.37779757, 0.35683622, 0.3796059 ,\n",
       "         0.31451413, 0.37994797, 0.35404071, 0.37835124, 0.31480677,\n",
       "         0.32510764, 0.38177117, 0.38112516]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.372995</td>\n",
       "      <td>0.373843</td>\n",
       "      <td>0.323332</td>\n",
       "      <td>0.386808</td>\n",
       "      <td>0.393896</td>\n",
       "      <td>0.379546</td>\n",
       "      <td>0.317540</td>\n",
       "      <td>0.385057</td>\n",
       "      <td>0.391261</td>\n",
       "      <td>0.371906</td>\n",
       "      <td>0.375061</td>\n",
       "      <td>0.332091</td>\n",
       "      <td>0.326445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.362651</td>\n",
       "      <td>0.365933</td>\n",
       "      <td>0.338740</td>\n",
       "      <td>0.382503</td>\n",
       "      <td>0.391210</td>\n",
       "      <td>0.368370</td>\n",
       "      <td>0.335523</td>\n",
       "      <td>0.380373</td>\n",
       "      <td>0.388685</td>\n",
       "      <td>0.361191</td>\n",
       "      <td>0.364143</td>\n",
       "      <td>0.347146</td>\n",
       "      <td>0.342868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.345426</td>\n",
       "      <td>0.350985</td>\n",
       "      <td>0.356938</td>\n",
       "      <td>0.373877</td>\n",
       "      <td>0.386938</td>\n",
       "      <td>0.347430</td>\n",
       "      <td>0.355927</td>\n",
       "      <td>0.371751</td>\n",
       "      <td>0.384770</td>\n",
       "      <td>0.342928</td>\n",
       "      <td>0.348985</td>\n",
       "      <td>0.362878</td>\n",
       "      <td>0.360563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.316719</td>\n",
       "      <td>0.328299</td>\n",
       "      <td>0.377798</td>\n",
       "      <td>0.356836</td>\n",
       "      <td>0.379606</td>\n",
       "      <td>0.314514</td>\n",
       "      <td>0.379948</td>\n",
       "      <td>0.354041</td>\n",
       "      <td>0.378351</td>\n",
       "      <td>0.314807</td>\n",
       "      <td>0.325108</td>\n",
       "      <td>0.381771</td>\n",
       "      <td>0.381125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.372995  0.373843  0.323332  0.386808  0.393896  0.379546  0.317540   \n",
       "1  0.362651  0.365933  0.338740  0.382503  0.391210  0.368370  0.335523   \n",
       "2  0.345426  0.350985  0.356938  0.373877  0.386938  0.347430  0.355927   \n",
       "3  0.316719  0.328299  0.377798  0.356836  0.379606  0.314514  0.379948   \n",
       "\n",
       "          7         8         9        10        11        12  \n",
       "0  0.385057  0.391261  0.371906  0.375061  0.332091  0.326445  \n",
       "1  0.380373  0.388685  0.361191  0.364143  0.347146  0.342868  \n",
       "2  0.371751  0.384770  0.342928  0.348985  0.362878  0.360563  \n",
       "3  0.354041  0.378351  0.314807  0.325108  0.381771  0.381125  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(e_prob[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: no-repair, 1: repair\n",
    "actions = ('0', '1')\n",
    "# 0: failing, 1: low health, 2: good health, 3: perfect health\n",
    "states = ('0', '1', '2', '3')\n",
    "\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First we define an MDP. We also represent a policy\n",
    "as a dictionary of {state: action} pairs, and a utility function as a\n",
    "dictionary of {state: number} pairs. We then define the value_iteration\n",
    "and policy_iteration algorithms.\"\"\"\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class MDP:\n",
    "\n",
    "    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n",
    "    and reward function. We also keep track of a gamma value, for use by\n",
    "    algorithms. The transition model is represented somewhat differently from\n",
    "    the text. Instead of P(s' | s, a) being a probability number for each\n",
    "    state/state/action triplet, we instead have T(s, a) return a\n",
    "    list of (p, s') pairs. We also keep track of the possible states,\n",
    "    terminal states, and actions for each state.\"\"\"\n",
    "\n",
    "    def __init__(self, init, actlist, terminals, transitions=None, reward=None, states=None, gamma=0.9):\n",
    "        if not (0 < gamma <= 1):\n",
    "            raise ValueError(\"An MDP must have 0 < gamma <= 1\")\n",
    "\n",
    "        # collect states from transitions table if not passed.\n",
    "        self.states = states or self.get_states_from_transitions(transitions)\n",
    "            \n",
    "        self.init = init\n",
    "        \n",
    "        if isinstance(actlist, list):\n",
    "            # if actlist is a list, all states have the same actions\n",
    "            self.actlist = actlist\n",
    "\n",
    "        elif isinstance(actlist, dict):\n",
    "            # if actlist is a dict, different actions for each state\n",
    "            self.actlist = actlist\n",
    "        \n",
    "        self.terminals = terminals\n",
    "        self.transitions = transitions or {}\n",
    "        if not self.transitions:\n",
    "            print(\"Warning: Transition table is empty.\")\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.reward = reward or {s: 0 for s in self.states}\n",
    "\n",
    "        # self.check_consistency()\n",
    "\n",
    "    def R(self, state):\n",
    "        \"\"\"Return a numeric reward for this state.\"\"\"\n",
    "\n",
    "        return self.reward[state]\n",
    "\n",
    "    def T(self, state, action):\n",
    "        \"\"\"Transition model. From a state and an action, return a list\n",
    "        of (probability, result-state) pairs.\"\"\"\n",
    "\n",
    "        if not self.transitions:\n",
    "            raise ValueError(\"Transition model is missing\")\n",
    "        else:\n",
    "            return self.transitions[state][action]\n",
    "\n",
    "    def actions(self, state):\n",
    "        \"\"\"Return a list of actions that can be performed in this state. By default, a\n",
    "        fixed list of actions, except for terminal states. Override this\n",
    "        method if you need to specialize by state.\"\"\"\n",
    "\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.actlist\n",
    "\n",
    "    def get_states_from_transitions(self, transitions):\n",
    "        if isinstance(transitions, dict):\n",
    "            s1 = set(transitions.keys())\n",
    "            s2 = set(tr[1] for actions in transitions.values()\n",
    "                     for effects in actions.values()\n",
    "                     for tr in effects)\n",
    "            return s1.union(s2)\n",
    "        else:\n",
    "            print('Could not retrieve states from transitions')\n",
    "            return None\n",
    "\n",
    "    def check_consistency(self):\n",
    "\n",
    "        # check that all states in transitions are valid\n",
    "        assert set(self.states) == self.get_states_from_transitions(self.transitions)\n",
    "\n",
    "        # check that init is a valid state\n",
    "        assert self.init in self.states\n",
    "\n",
    "        # check reward for each state\n",
    "        assert set(self.reward.keys()) == set(self.states)\n",
    "\n",
    "        # check that all terminals are valid states\n",
    "        assert all(t in self.states for t in self.terminals)\n",
    "\n",
    "        # check that probability distributions for all actions sum to 1\n",
    "        for s1, actions in self.transitions.items():\n",
    "            for a in actions.keys():\n",
    "                s = 0\n",
    "                for o in actions[a]:\n",
    "                    s += o[0]\n",
    "                assert abs(s - 1) < 0.001\n",
    "\n",
    "class POMDP(MDP):\n",
    "\n",
    "    \"\"\"A Partially Observable Markov Decision Process, defined by\n",
    "    a transition model P(s'|s,a), actions A(s), a reward function R(s),\n",
    "    and a sensor model P(e|s). We also keep track of a gamma value,\n",
    "    for use by algorithms. The transition and the sensor models\n",
    "    are defined as matrices. We also keep track of the possible states\n",
    "    and actions for each state.\"\"\"\n",
    "\n",
    "    def __init__(self, actions, transitions=None, evidences=None, rewards=None, states=None, gamma=0.95):\n",
    "        \"\"\"Initialize variables of the pomdp\"\"\"\n",
    "\n",
    "        if not (0 < gamma <= 1):\n",
    "            raise ValueError('A POMDP must have 0 < gamma <= 1')\n",
    "\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "\n",
    "        # transition model cannot be undefined\n",
    "        self.t_prob = transitions\n",
    "        if not self.t_prob.any():\n",
    "            print('Warning: Transition model is undefined')\n",
    "        \n",
    "        # sensor model cannot be undefined\n",
    "        self.e_prob = evidences\n",
    "        if not self.e_prob.any():\n",
    "            print('Warning: Sensor model is undefined')\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.rewards = rewards\n",
    "\n",
    "    def remove_dominated_plans(self, input_values):\n",
    "        \"\"\"\n",
    "        Remove dominated plans.\n",
    "        This method finds all the lines contributing to the\n",
    "        upper surface and removes those which don't.\n",
    "        \"\"\"\n",
    "\n",
    "        values = [val for action in input_values for val in input_values[action]]\n",
    "        values.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        best = [values[0]]\n",
    "        y1_max = max(val[1] for val in values)\n",
    "        tgt = values[0]\n",
    "        prev_b = 0\n",
    "        prev_ix = 0\n",
    "        while tgt[1] != y1_max:\n",
    "            min_b = 1\n",
    "            min_ix = 0\n",
    "            for i in range(prev_ix + 1, len(values)):\n",
    "                if values[i][0] - tgt[0] + tgt[1] - values[i][1] != 0:\n",
    "                    trans_b = (values[i][0] - tgt[0]) / (values[i][0] - tgt[0] + tgt[1] - values[i][1])\n",
    "                    if 0 <= trans_b <= 1 and trans_b > prev_b and trans_b < min_b:\n",
    "                        min_b = trans_b\n",
    "                        min_ix = i\n",
    "            prev_b = min_b\n",
    "            prev_ix = min_ix\n",
    "            tgt = values[min_ix]\n",
    "            best.append(tgt)\n",
    "\n",
    "        return self.generate_mapping(best, input_values)\n",
    "\n",
    "    def remove_dominated_plans_fast(self, input_values):\n",
    "        \"\"\"\n",
    "        Remove dominated plans using approximations.\n",
    "        Resamples the upper boundary at intervals of 100 and\n",
    "        finds the maximum values at these points.\n",
    "        \"\"\"\n",
    "\n",
    "        values = [val for action in input_values for val in input_values[action]]\n",
    "        values.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        best = []\n",
    "        sr = 100\n",
    "        for i in range(sr + 1):\n",
    "            x = i / float(sr)\n",
    "            maximum = (values[0][1] - values[0][0]) * x + values[0][0]\n",
    "            tgt = values[0]\n",
    "            for value in values:\n",
    "                val = (value[1] - value[0]) * x + value[0]\n",
    "                if val > maximum:\n",
    "                    maximum = val\n",
    "                    tgt = value\n",
    "\n",
    "            if all(any(tgt != v) for v in best):\n",
    "                best.append(np.array(tgt))\n",
    "\n",
    "        return self.generate_mapping(best, input_values)\n",
    "\n",
    "    def generate_mapping(self, best, input_values):\n",
    "        \"\"\"Generate mappings after removing dominated plans\"\"\"\n",
    "\n",
    "        mapping = defaultdict(list)\n",
    "        for value in best:\n",
    "            for action in input_values:\n",
    "                if any(all(value == v) for v in input_values[action]):\n",
    "                    mapping[action].append(value)\n",
    "\n",
    "        return mapping\n",
    "\n",
    "    def max_difference(self, U1, U2):\n",
    "        \"\"\"Find maximum difference between two utility mappings\"\"\"\n",
    "\n",
    "        for k, v in U1.items():\n",
    "            sum1 = 0\n",
    "            for element in U1[k]:\n",
    "                sum1 += sum(element)\n",
    "            sum2 = 0\n",
    "            for element in U2[k]:\n",
    "                sum2 += sum(element)\n",
    "        return abs(sum1 - sum2)\n",
    "\n",
    "        \n",
    "class Matrix:\n",
    "    \"\"\"Matrix operations class\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def add(A, B):\n",
    "        \"\"\"Add two matrices A and B\"\"\"\n",
    "\n",
    "        res = []\n",
    "        for i in range(len(A)):\n",
    "            row = []\n",
    "            for j in range(len(A[0])):\n",
    "                row.append(A[i][j] + B[i][j])\n",
    "            res.append(row)\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def scalar_multiply(a, B):\n",
    "        \"\"\"Multiply scalar a to matrix B\"\"\"\n",
    "\n",
    "        for i in range(len(B)):\n",
    "            for j in range(len(B[0])):\n",
    "                B[i][j] = a * B[i][j]\n",
    "        return B\n",
    "\n",
    "    @staticmethod\n",
    "    def multiply(A, B):\n",
    "        \"\"\"Multiply two matrices A and B element-wise\"\"\"\n",
    "\n",
    "        matrix = []\n",
    "        for i in range(len(B)):\n",
    "            row = []\n",
    "            for j in range(len(B[0])):\n",
    "                row.append(B[i][j] * A[j][i])\n",
    "            matrix.append(row)\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def matmul(A, B):\n",
    "        \"\"\"Inner-product of two matrices\"\"\"\n",
    "\n",
    "        return [[sum(ele_a*ele_b for ele_a, ele_b in zip(row_a, col_b)) for col_b in list(zip(*B))] for row_a in A]\n",
    "\n",
    "    @staticmethod\n",
    "    def transpose(A):\n",
    "        \"\"\"Transpose a matrix\"\"\"\n",
    "        \n",
    "        return [list(i) for i in zip(*A)]\n",
    "\n",
    "\n",
    "def pomdp_value_iteration(pomdp, epsilon=0.1):\n",
    "    \"\"\"Solving a POMDP by value iteration.\"\"\"\n",
    "\n",
    "    U = {'':[[0]* len(pomdp.states)]}\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        prev_U = U\n",
    "        values = [val for action in U for val in U[action]]\n",
    "        value_matxs = []\n",
    "        for i in values:\n",
    "            for j in values:\n",
    "                value_matxs.append([i, j])\n",
    "\n",
    "        U1 = defaultdict(list)\n",
    "        for action in pomdp.actions:\n",
    "            for u in value_matxs:\n",
    "                u1 = Matrix.matmul(Matrix.matmul(pomdp.t_prob[int(action)], Matrix.multiply(pomdp.e_prob[int(action)], Matrix.transpose(u))), [[1], [1]])\n",
    "                u1 = Matrix.add(Matrix.scalar_multiply(pomdp.gamma, Matrix.transpose(u1)), [pomdp.rewards[int(action)]])\n",
    "                U1[action].append(u1[0])\n",
    "\n",
    "        U = pomdp.remove_dominated_plans_fast(U1)\n",
    "        # replace with U = pomdp.remove_dominated_plans(U1) for accurate calculations\n",
    "        \n",
    "        if count > 10:\n",
    "            if pomdp.max_difference(U, prev_U) < epsilon * (1 - pomdp.gamma) / pomdp.gamma:\n",
    "                return U\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0', '1', '2', '3')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pomdp = POMDP(actions, t_prob, e_prob, rewards, states, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'0': [array([ 209.88702446,   90.76156902,  -34.01230831, -185.57933297])]})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utility = pomdp_value_iteration(pomdp, epsilon=3)\n",
    "utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADQRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These lines establish the feed-forward part of the network used to choose actions\n",
    "inputs1 = tf.placeholder(shape=[1,13],dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_uniform([13,2],0,0.01))\n",
    "Qout = tf.matmul(inputs1,W)\n",
    "predict = tf.argmax(Qout,1)\n",
    "\n",
    "#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "nextQ = tf.placeholder(shape=[1,2],dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(obs):\n",
    "    state = 0\n",
    "    diff = 16\n",
    "    for i in range(len(statemean)):\n",
    "        stateDiff = obs - statemean[i]\n",
    "        stateDiffVal = np.sqrt(np.mean(stateDiff**2))\n",
    "        if stateDiffVal < diff:\n",
    "            diff = stateDiffVal\n",
    "            state = i\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the next step after an action is done\n",
    "\n",
    "def getStepDetails(i,j,action):\n",
    "    unitData = dataT_cycles[i]\n",
    "    d = False\n",
    "    if action == 1:\n",
    "        newJ = 0\n",
    "    else:\n",
    "        newJ = j+1\n",
    "    obsNext = unitData.values[newJ]\n",
    "    if newJ >= len(unitData) - 1:\n",
    "        d = True\n",
    "    s1 = get_state(obsNext)\n",
    "    r1 = rewards[action][s1]\n",
    "    return r1,newJ,s1,obsNext,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "# Set learning parameters\n",
    "init = tf.global_variables_initializer()\n",
    "y = gamma\n",
    "e = 0.1\n",
    "num_episodes = len(dataT_cycles)\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "D = np.empty([0,5]) # Replay memory\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation for new unit\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        k = 0\n",
    "        unitData = dataT_cycles[i]\n",
    "        #The Q-Network\n",
    "        while j < len(unitData):\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            a,allQ = sess.run([predict,Qout],feed_dict={inputs1:unitData.values[j].reshape(1,13)})\n",
    "            if np.random.rand(1) < e:\n",
    "                a[0] = np.random.randint(0,2)\n",
    "            #Get new state and reward from environment\n",
    "            r,j,s1,o1,d = getStepDetails(i,j,a[0])\n",
    "            D = np.vstack([D, [a[0],unitData.values[j-1].reshape(1,13),r,o1,s1]])\n",
    "            if len(D) > 20:\n",
    "                lastInd = np.random.randint(15,len(D))\n",
    "                randomSample = D[lastInd-15:lastInd]\n",
    "                finalO = D[lastInd,3].reshape(1,13)\n",
    "                Reward = np.sum(D[lastInd-15:lastInd,2])\n",
    "            else:\n",
    "                finalO = o1.reshape(1,13)\n",
    "                Reward = r\n",
    "            # We take batch size of 15 (j in algorithm)\n",
    "            #Obtain the Q' values by feeding the new state through our network\n",
    "            Q1 = sess.run(Qout,feed_dict={inputs1:finalO})\n",
    "            #Obtain maxQ' and set our target value for chosen action.\n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            targetQ[0,a[0]] = Reward + y*maxQ1\n",
    "            #Train our network using target and predicted Q values\n",
    "            _,W1 = sess.run([updateModel,W],feed_dict={inputs1:unitData.values[j-1].reshape(1,13),nextQ:targetQ})\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            k += 1\n",
    "            if d == True or k >= 1000:\n",
    "                #Reduce chance of random action as we train the model.\n",
    "                e = 1./((i/50) + 10)\n",
    "                break\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dataT_cycles[5].values[160].reshape(-1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2127.6558 ,  6986.8765 ],\n",
       "       [ 2248.1982 ,  5232.41   ],\n",
       "       [ 3317.4592 ,  3077.835  ],\n",
       "       [ 4824.3174 ,   -45.76535],\n",
       "       [ 3286.811  ,  2219.9832 ],\n",
       "       [ 2962.1672 ,  4778.8364 ],\n",
       "       [ 5146.6387 ,  1414.4443 ],\n",
       "       [ 4606.688  , -1017.36865],\n",
       "       [ 8425.082  ,  1503.2651 ],\n",
       "       [ 2555.8647 ,  2504.4    ],\n",
       "       [ 1596.5271 ,  3793.9429 ],\n",
       "       [ 3868.3757 ,   737.1053 ],\n",
       "       [ 3159.9434 ,  1868.2078 ]], dtype=float32)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17942.35635803, 16792.32532451]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a,W1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
